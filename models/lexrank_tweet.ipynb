{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "1. Built-in Textrank with gensim\n",
    "2. Built-in Lexrank\n",
    "3. Lexrank with tfidf & LSH\n",
    "4. Lexrank with sentence embedding & LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "from fast_pagerank import pagerank\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from fast_lexrank import Lexrank\n",
    "import numpy as np\n",
    "from lsh import LSH\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import emoji, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  RT @MENTION : Emergency Rally Against Trump's ...\n",
      "1  824941519857610752  RT @MENTION : Theresa May has not apologized t...\n",
      "2  824941616314122240  RT @MENTION : Trump's Immigration Ban Excludes...\n",
      "3  824942056741167105  RT @MENTION : Trump's immigration order expand...\n",
      "4  824942966875774976  ALERT : Senator John McCain Threatens Action O...\n",
      "(123385, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/nguyen/data/processed_travel_ban.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet  \\\n",
      "0  824941360449015808  emergency rally against trump's muslim travel ...   \n",
      "1  824941519857610752  theresa may has not apologized to trump for in...   \n",
      "2  824941616314122240  trump's immigration ban excludes countries wit...   \n",
      "3  824942056741167105  trump's immigration order expands the definiti...   \n",
      "4  824942966875774976  alert : senator john mccain threatens action o...   \n",
      "\n",
      "                                              Tweet1  uniWPercent  \n",
      "0  emergency rally trumps muslim travel ban nyc 1...           10  \n",
      "1  theresa may apologized trump insulting fails t...           11  \n",
      "2  trumps immigration ban excludes countries busi...            9  \n",
      "3  trumps immigration order expands definition cr...            6  \n",
      "4  alert senator john mccain threatens action pre...            8  \n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI))\n",
    "# remove stopwords, punctuation\n",
    "stopWords = stopwords.words('english')\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stopWords))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '',  string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—|→', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())\n",
    "\n",
    "# remove tweets #unique words less than haft of length\n",
    "data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['uniWPercent']!=0]\n",
    "# # remove tweets with lengths < 3, duplicates\n",
    "while data['uniWPercent'].min() <=2:\n",
    "    data = data[data['uniWPercent'] >2]\n",
    "    data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "# # # remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "# data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(lemmatize_stemming(y) for y in x.split(\" \") if y.strip()!= \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                            825521541756645377\n",
       "Tweet          if you or someone you know has been affected b...\n",
       "Tweet1         someone know affected new refugeeimmigrationtr...\n",
       "uniWPercent                                                   10\n",
       "Name: 23995, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[21284]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remained_index = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['Tweet'].str.contains(\"president trump fires acting attorne\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emergency rally trumps muslim travel ban nyc 125 5 pm', 'theresa may apologized trump insulting fails today trump send back b', 'trumps immigration ban excludes countries business ties via democracyfor', 'trumps immigration order expands definition criminal', 'alert senator john mccain threatens action president trump', 'kiva still distracted trump gets peoples business', 'ty bailing gmb today piers morgan drank trump kool aid vocal opponent', 'trump sign eo temporary ban suspending visas syria six african countries buildthewall', 'moral obligation stop hitler moral obligation stop trump', 'people getting radicalized trump always hate freedom']\n"
     ]
    }
   ],
   "source": [
    "print(list(data.iloc[0:10]['Tweet1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lexrank + lsh + tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105175, 48876)\n"
     ]
    }
   ],
   "source": [
    "#extract tfidf vector\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfData = tfidf.fit_transform(data['Tweet1'])\n",
    "print(tfidfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105175,)\n"
     ]
    }
   ],
   "source": [
    "lsh_tfidf = LSH(tfidfData)\n",
    "lsh_tfidf.train(num_bits = 8)\n",
    "lex_tfidf = Lexrank(tfidfData, lsh_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#buckets: 256\n",
      ".......Buck: 0, vec: (4405, 48876)\n",
      ".......Buck: 100, vec: (3664, 48876)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lex_tfidf.build_graph(search_radius = 1, cosine_sim = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n"
     ]
    }
   ],
   "source": [
    "lex.train(lexrank_iter = 100, damping_factor = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentences....\n",
      "Sent scores: 105175\n",
      "selected one: 62658, 0.00020242707405364153\n",
      "Sent 80396 is similar to a 62658: 0.7907479705005656\n",
      "selected one: 13109, 0.00018880050779761126\n",
      "selected one: 32294, 0.00017162311007124417\n",
      "selected one: 91027, 0.00015780050458681738\n",
      "selected one: 16346, 0.00013832732862091305\n",
      "Sent 62378 is similar to a 32294: 0.800885372372297\n",
      "selected one: 55536, 0.00012749418907202905\n",
      "selected one: 22089, 0.0001244623936573965\n",
      "Sent 10126 is similar to a 13109: 0.8216643750207849\n",
      "Sent 33510 is similar to a 22089: 0.720695735370632\n",
      "selected one: 103476, 0.00011871329635948848\n",
      "selected one: 39127, 0.00011697775526405306\n",
      "selected one: 41746, 0.00011553935404484163\n",
      "selected one: 15499, 0.00011492288316361148\n",
      "Sent 85108 is similar to a 16346: 0.5930587680453396\n",
      "Sent 61849 is similar to a 62658: 0.672487377445378\n",
      "Sent 65930 is similar to a 55536: 0.932304152068153\n",
      "Sent 47447 is similar to a 62658: 0.7400724194755225\n",
      "selected one: 71398, 0.00010798483103909025\n",
      "selected one: 79435, 0.00010697164164957916\n",
      "Sent 96240 is similar to a 55536: 0.6358305898099601\n",
      "selected one: 8767, 0.0001059602037582633\n",
      "Sent 49450 is similar to a 62658: 0.5669629511024311\n",
      "selected one: 24331, 0.00010534054644177753\n",
      "selected one: 2146, 0.00010525056852056679\n",
      "Sent 38847 is similar to a 24331: 0.7590877447707252\n",
      "Sent 34650 is similar to a 91027: 0.6894409016416914\n",
      "Sent 68537 is similar to a 32294: 0.8707652667270216\n",
      "Sent 20319 is similar to a 103476: 0.9474568955716832\n",
      "Sent 1958 is similar to a 2146: 0.9604497146086457\n",
      "selected one: 18434, 9.781553837955189e-05\n",
      "Sent 57196 is similar to a 8767: 0.5316958723589162\n",
      "selected one: 30976, 9.574184662715573e-05\n",
      "Sent 81705 is similar to a 71398: 0.7044466554968243\n",
      "Sent 57895 is similar to a 79435: 0.938479203928398\n",
      "Sent 1154 is similar to a 8767: 0.6064325804289423\n",
      "selected one: 61197, 9.462894645832923e-05\n",
      "selected one: 29492, 9.19887782750149e-05\n"
     ]
    }
   ],
   "source": [
    "sentIds = lex.extract_summary(n_sents = 20, cosine_thres=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id Index lexrank\n",
      "0 356 0.00020242707405364153\n",
      "1 342 0.00018880050779761126\n",
      "2 215 0.00017162311007124417\n",
      "3 159 0.00015780050458681738\n",
      "4 259 0.00013832732862091305\n",
      "5 149 0.00012749418907202905\n",
      "6 258 0.0001244623936573965\n",
      "7 102 0.00011871329635948848\n",
      "8 228 0.00011697775526405306\n",
      "9 95 0.00011553935404484163\n",
      "10 149 0.00011492288316361148\n",
      "11 149 0.00010798483103909025\n",
      "12 147 0.00010697164164957916\n",
      "13 475 0.0001059602037582633\n",
      "14 158 0.00010534054644177753\n",
      "15 250 0.00010525056852056679\n",
      "16 129 9.781553837955189e-05\n",
      "17 103 9.574184662715573e-05\n",
      "18 105 9.462894645832923e-05\n",
      "19 78 9.19887782750149e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Id\", \"#adjacentEdges\", \"lexrank\")\n",
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, len(lex.graph[idx]), lex.scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  on executive order immigration \n",
      "1 green card holders \n",
      "2 ban all muslim countries \n",
      "3 what do people in the middle east think about terrorism ? for more on the middle east visit http …\n",
      "4 our statement on president trump’s executive order on immigration : \n",
      "5  make america great . again ?\n",
      "6  from saudi arabia . #trump's ban doesn't include saudi arabia .\n",
      "7 keep america safe president trump keep the ban on muslim refugees , keep them out and keep us safe . \n",
      "8 my statement on president trump's executive order on refugees : \n",
      "9  it is a religion ban trump\n",
      "10 protest at the airport !!! #muslimban \n",
      "11 the president of the united states fires the attorney general of the united states .\n",
      "12  are you from any of the 7 banned countries ? what are you on about ?\n",
      "13 trump's state visit to the uk :\n",
      "14 it's not a muslim ban . it's a ban from muslim countries where trump doesn't do business . \n",
      "15 trump signs executive order on ‘ extreme vetting ’ \n",
      "16 jfk protest . #muslimban #nobannowall \n",
      "17 it's a temporary ban from these countries . \n",
      "18  ..... a white nationalist that did it when there was a muslim too\n",
      "19  yes yes and yes ! our america , not trumps !\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, data.iloc[idx]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saved_models/tfidf_model.pkl', \"wb\") as f:\n",
    "    pickle.dump(lex, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####\n",
    "\n",
    "* <b>Sub-events captured:</b>\n",
    "    1. green card orders\n",
    "    2. protest at jfk airport\n",
    "    3. attorney general get fired\n",
    "    4. ban 7 countries, muslim countries\n",
    "    5. trump's ban doesn't include saudi arabia .\n",
    "    6. trump visit uk\n",
    "    7. executive order on ‘ extreme vetting ’ \n",
    "* <b>Lack</b>:\n",
    "    1. starbuck hires 10K refugees\n",
    "    2. trump’s deportation orders \n",
    "    3. washington state will sue to stop trump's immigration\n",
    "    4. canada will accept the refugees \n",
    "    5. quebec city mosque shooting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lexrank with sentEmbeddings and LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123385\n"
     ]
    }
   ],
   "source": [
    "#load embeddings\n",
    "with open('/home/nguyen/data/travel_ban_sentence_transformers_embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0].shape\n",
    "count = 0\n",
    "list = []\n",
    "for i in range(1000):\n",
    "    cos_sim = cosine_similarity(embeddings[0].reshape(1, -1), embeddings[i].reshape(1, -1))\n",
    "    print(cos_sim)\n",
    "    \n",
    "    if cos_sim >0.65:\n",
    "        count+=1\n",
    "        list.append(i)\n",
    "print(\"Number of sentences that are similar with the first sen:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123385, 768)\n"
     ]
    }
   ],
   "source": [
    "sentenceEmbs = np.array(embeddings)\n",
    "print(sentenceEmbs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceEmbs = sentenceEmbs[remained_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceEmbs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105175,)\n"
     ]
    }
   ],
   "source": [
    "lsh = LSH(sentenceEmbs)\n",
    "lsh.train(num_bits=14)\n",
    "table = lsh.model['table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key, value in table.items():\n",
    "    print(key, len(value))\n",
    "    if len(value) > 150:\n",
    "        count+=1\n",
    "print(len(table))\n",
    "print(\"#buckets with >150 sens: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = Lexrank(sentenceEmbs, lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lex.build_graph(search_radius = 1, percent=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n"
     ]
    }
   ],
   "source": [
    "lex.train(lexrank_iter = 100, damping_factor = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet1</th>\n",
       "      <th>uniWPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>825064765261176832</td>\n",
       "      <td>company sent out a notice about trump's muslim...</td>\n",
       "      <td>company sent notice trumps muslim ban green ca...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>825137645504172032</td>\n",
       "      <td>visas being denied immediately . chaos at airp...</td>\n",
       "      <td>visas denied immediately chaos airports air mu...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>825141021906382848</td>\n",
       "      <td>ban applies if you have a visa , green card , ...</td>\n",
       "      <td>ban applies visa green card even dual citizen ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>825147334342303745</td>\n",
       "      <td>current concern : what about people with green...</td>\n",
       "      <td>current concern people green cards currently a...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2755</th>\n",
       "      <td>825152564652081157</td>\n",
       "      <td>many elderly come to green card interview with...</td>\n",
       "      <td>many elderly come green card interview suit ti...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103920</th>\n",
       "      <td>827623785662644224</td>\n",
       "      <td>my uncle from syria needs a green card any tak...</td>\n",
       "      <td>uncle syria needs green card takers</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104019</th>\n",
       "      <td>827626771994533889</td>\n",
       "      <td>why is tomi defending the rights of milo ( a ...</td>\n",
       "      <td>tomi defending rights milo non american rights...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104760</th>\n",
       "      <td>827658820658851840</td>\n",
       "      <td>green card coming soon !</td>\n",
       "      <td>green card coming soon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104876</th>\n",
       "      <td>827664285862129668</td>\n",
       "      <td>alert : dos issues stmnt clarifying ban dn app...</td>\n",
       "      <td>alert dos issues stmnt clarifying ban dn apply...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104916</th>\n",
       "      <td>827665892280528897</td>\n",
       "      <td>the first female fields medalist , , came to t...</td>\n",
       "      <td>first female fields medalist came us green car...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1446 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Id                                              Tweet  \\\n",
       "1216    825064765261176832  company sent out a notice about trump's muslim...   \n",
       "2323    825137645504172032  visas being denied immediately . chaos at airp...   \n",
       "2419    825141021906382848  ban applies if you have a visa , green card , ...   \n",
       "2609    825147334342303745  current concern : what about people with green...   \n",
       "2755    825152564652081157  many elderly come to green card interview with...   \n",
       "...                    ...                                                ...   \n",
       "103920  827623785662644224  my uncle from syria needs a green card any tak...   \n",
       "104019  827626771994533889   why is tomi defending the rights of milo ( a ...   \n",
       "104760  827658820658851840                           green card coming soon !   \n",
       "104876  827664285862129668  alert : dos issues stmnt clarifying ban dn app...   \n",
       "104916  827665892280528897  the first female fields medalist , , came to t...   \n",
       "\n",
       "                                                   Tweet1  uniWPercent  \n",
       "1216    company sent notice trumps muslim ban green ca...           10  \n",
       "2323    visas denied immediately chaos airports air mu...           13  \n",
       "2419    ban applies visa green card even dual citizen ...            9  \n",
       "2609    current concern people green cards currently a...           13  \n",
       "2755    many elderly come green card interview suit ti...           12  \n",
       "...                                                   ...          ...  \n",
       "103920                uncle syria needs green card takers            6  \n",
       "104019  tomi defending rights milo non american rights...           12  \n",
       "104760                             green card coming soon            4  \n",
       "104876  alert dos issues stmnt clarifying ban dn apply...           18  \n",
       "104916  first female fields medalist came us green car...           13  \n",
       "\n",
       "[1446 rows x 4 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Tweet'].str.contains('green card')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                            825128489326088196\n",
       "Tweet          trump signs executive order on ‘ extreme vetti...\n",
       "Tweet1               trump signs executive order extreme vetting\n",
       "uniWPercent                                                    6\n",
       "Name: 2146, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[2146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsh.model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentences....\n",
      "Sent scores: 105175\n",
      "selected one: 42773, 0.00010440142035777392\n",
      "selected one: 70826, 0.00010220440798520725\n",
      "selected one: 72679, 9.952317010832082e-05\n",
      "Sent 67230 is similar to a 70826: 0.9897810275772859\n",
      "selected one: 23584, 9.790741698338028e-05\n",
      "Sent 66616 is similar to a 70826: 0.9818513941126814\n",
      "Sent 65594 is similar to a 23584: 0.8982108068636003\n",
      "selected one: 15323, 9.108063279018107e-05\n",
      "selected one: 31600, 8.952282977429021e-05\n",
      "selected one: 2754, 8.94612728374494e-05\n",
      "selected one: 38737, 8.916163568577142e-05\n",
      "Sent 96348 is similar to a 70826: 0.9162692941825311\n",
      "selected one: 15006, 8.773176537896183e-05\n",
      "selected one: 34043, 8.760762320177292e-05\n",
      "selected one: 59451, 8.726155740041375e-05\n",
      "selected one: 9399, 8.668920580458683e-05\n",
      "selected one: 49222, 8.600378997886487e-05\n",
      "Sent 76628 is similar to a 23584: 0.8911665337519232\n",
      "Sent 101509 is similar to a 42773: 0.8944038170610407\n",
      "Sent 9573 is similar to a 70826: 0.9093985959365581\n",
      "Sent 6261 is similar to a 15006: 0.9037798983991393\n",
      "Sent 48724 is similar to a 23584: 0.9069246704791699\n",
      "selected one: 81347, 8.270572045910686e-05\n",
      "selected one: 26825, 8.257767461424093e-05\n",
      "Sent 41747 is similar to a 23584: 0.8943403010921198\n",
      "Sent 36918 is similar to a 42773: 0.9902525069804194\n",
      "selected one: 1620, 8.029157895416175e-05\n",
      "Sent 71278 is similar to a 9399: 0.8988515688813016\n",
      "Sent 54112 is similar to a 23584: 0.9093940131596737\n",
      "Sent 86089 is similar to a 9399: 0.9058245197019031\n",
      "selected one: 98256, 7.941415400518185e-05\n",
      "Sent 31008 is similar to a 23584: 0.8924956797162281\n",
      "Sent 69825 is similar to a 23584: 0.9073396555842914\n",
      "selected one: 67422, 7.90822650947837e-05\n",
      "Sent 9155 is similar to a 70826: 0.9027310403251115\n",
      "Sent 61550 is similar to a 23584: 0.8965985933377446\n",
      "selected one: 42236, 7.85775492009802e-05\n",
      "selected one: 579, 7.831156338267633e-05\n"
     ]
    }
   ],
   "source": [
    "# 14 bins\n",
    "sentIds = lex.extract_summary(n_sents = 20, cosine_thres = 0.89, max_sent=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. \" this policy is going to get americans killed : \" sen. chris murphy on trump's refugee ban • #antitrumpmvmt …\n",
      "1. us muslim leaders sue trump over ' fear-mongering ' travel ban via …\n",
      "2. pragmactivist : trump fired ag #sallyyates for enforcing laws and #resist ing a #muslimban . will he fire seanspi … \n",
      "3.  remove trump ! commit/lock trump away ! aclu exec director anthony romero comes out aclu just argued won block trump's muslim ban\n",
      "4. breaking - #cair planning lawsuit against trump over #muslimban i guess ur lawyers don't know a president is immune ? https :/ …\n",
      "5. if muslims cause further bloodshed to americans it will be on the hands of the federal judge that blocked the trump enforcement order ! #maga\n",
      "6. trump blasted by top democrat for imposing a religious test on fleeing refugees via …\n",
      "7. thr : ' we can't keep pretending ' conway defends trump immigration ban , rips media ' a new one ' for bias …\n",
      "8. tech finds its voice , opposing trump’s muslim ban : \" so un-american , it pains us all . \" …\n",
      "9. _com : iraqi mps urge punitive measures against us in return for trump’s ‘ muslim ban ’ #news\n",
      "10. breaking - state department dissent memo published , condemning #trump’s #muslimban as “ counterproductive ” this is …\n",
      "11. breaking : the aclu has officially filed a lawsuit against president trump over his refugee ban . \n",
      "12. started w/ trump's muslim ban proves that his administration is full of idiots | gq #inners\n",
      "13. ag talking about suit against trump travel ban , calls eo \" harmful , discriminatory and unconstitutional . \" #mapoli\n",
      "14. uber ceo said we have to support trump . lyft ceo said his immigration ban is against our values . #muslinbanprotest #boycott …\n",
      "15. breaking : trump signs executive action for `new vetting measures ' to keep `radical islamic terrorists ' out of us https :/ …\n",
      "16. fascist , racist , bigoted shits : support for trump travel ban in line with anti-muslim attitudes in america …\n",
      "17. barack obama breaks silence on trump presidency to condemn migration ban #muslimban #obama htt …\n",
      "18. #deleteuber trend erupts as lyft backs aclu amid trump ban fury - usa today \n",
      "19. impeach donald trump now for violating the us constitution . sign the petition #theresistance …\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(sentIds):\n",
    "    print(\"{}. {}\".format(i, data.iloc[idx]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id Index lexrank\n",
      "0 3382 0.00010440142035777392\n",
      "1 3538 0.00010220440798520725\n",
      "2 3442 9.952317010832082e-05\n",
      "3 3391 9.790741698338028e-05\n",
      "4 3227 9.367025901136787e-05\n",
      "5 3189 9.108063279018107e-05\n",
      "6 3096 8.952282977429021e-05\n",
      "7 3014 8.94612728374494e-05\n",
      "8 3141 8.916163568577142e-05\n",
      "9 2817 8.773176537896183e-05\n",
      "10 2833 8.760762320177292e-05\n",
      "11 2853 8.726155740041375e-05\n",
      "12 3063 8.668920580458683e-05\n",
      "13 2805 8.600378997886487e-05\n",
      "14 2703 8.509400810913653e-05\n",
      "15 2810 8.270572045910686e-05\n",
      "16 2757 8.257767461424093e-05\n",
      "17 2617 8.183067572493015e-05\n",
      "18 2689 8.029157895416175e-05\n",
      "19 2798 8.02714257585991e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Id\", \"Index\", \"lexrank\")\n",
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, len(lex.graph[idx]), lex.scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1205"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lex.graph[43711])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of LSH with tfidf and lex rank with sent embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105175, 48876)\n"
     ]
    }
   ],
   "source": [
    "#extract tfidf vector\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfData = tfidf.fit_transform(data['Tweet1'])\n",
    "print(tfidfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105175,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LexRank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f7ad8468895f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlsh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidfData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_bits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLexRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentenceEmbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LexRank' is not defined"
     ]
    }
   ],
   "source": [
    "lsh = LSH(tfidfData)\n",
    "lsh.train(num_bits = 8)\n",
    "lex = LexRank(sentenceEmbs, lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in table.items():\n",
    "    print(key, len(value))\n",
    "print(len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lex.build_graph(search_radius = 1, percent=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.train(lexrank_iter = 100, damping_factor = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentIds = lex.extract_sentences(n_sents = 20, cosThres = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, len(lex.graph[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105176, 48876)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfCosine = cosine_similarity(tfidfData[0], tfidfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: [69002 89160 45071 97336  8934 47837 23965 97958 79194     0]\n",
      "Cosine values: [0.29885418 0.31449294 0.31054197 0.32932623 0.3548277  0.37053829\n",
      " 0.35177915 0.32968794 0.33406313 1.        ]\n",
      "Tweet 0: emergency rally trumps muslim travel ban nyc 125 5 pm\n",
      "69002\t0.29885417674875303\twatch live protesters gather rally hate nyc response trumps refugee ban\n",
      "89160\t0.314492944844012\tgoogle workers rally trumps travel ban\n",
      "45071\t0.3105419662113536\tmany american flags todays nyc rally trumps muslim ban america resist htt\n",
      "97336\t0.3293262319915567\tdefend immigrants trump rally nyc 2 pm today near nyu nobannowall\n",
      "8934\t0.35482769835646444\tprotesters rally trumps muslim immigration ban\n",
      "47837\t0.3705382876374108\tsenator chuck schumer slams trumps travel ban nyc rally\n",
      "23965\t0.35177914692132783\tblocks call trumps unconstitutional muslim ban comes emergency hearing nyc\n",
      "97958\t0.32968793637635174\tjudge extends emergency stay blocking trumps travel ban via newyork nyc\n",
      "79194\t0.334063133607824\tknow fridays eo muslim ban temp travel restriction countries w 125 global musli\n",
      "0\t1.0\temergency rally trumps muslim travel ban nyc 125 5 pm\n"
     ]
    }
   ],
   "source": [
    "# extract 10 sentence with highest similarity\n",
    "indices = np.argpartition(tfidfCosine[0], -10)[-10:]\n",
    "elements = tfidfCosine[0][indices]\n",
    "print(\"Indices: {}\".format(indices))\n",
    "print(\"Cosine values: {}\".format(elements))\n",
    "print(\"Tweet 0:\", str(data.iloc[0]['Tweet1']))\n",
    "for i in indices:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, tfidfCosine[0][i], str(data.iloc[i]['Tweet1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices: [ 1279 44958 40096 52607 37999 52571 25520 97778 90082     0]\n",
      "Cosine values: [0.8552602  0.85747457 0.8578343  0.87147146 0.87335134 0.8906126\n",
      " 0.897992   0.8847035  0.8772365  1.        ]\n",
      "Tweet 0: emergency rally trumps muslim travel ban nyc 125 5 pm\n",
      "1279\t0.8552601933479309\tprotest today trump dapl downtown 5pm 611 woodward\n",
      "44958\t0.8574745655059814\tseattle protest muslimban tomorrow westlake park 5pm\n",
      "40096\t0.8578342795372009\temergency demo york tomorrow 5pm st helens square protest muslimban uk complicity mu\n",
      "52607\t0.8714714646339417\tmorning half 5 today protest trumps muslim ban hopefully see ️️\n",
      "37999\t0.8733513355255127\tcambridge friends rally 5pm tomorrow gsm trumps muslimban uk govts complicity whos coming ht\n",
      "52571\t0.8906126022338867\tseattle join us 5pm westlake park protest president trumps immigration refugee ban\n",
      "25520\t0.8979920148849487\tprotest muslimban tomorrow dallas city hall 5 pm\n",
      "97778\t0.884703516960144\twest hollywood holding antitrump rally today 5pm pst ill defend great president\n",
      "90082\t0.8772364854812622\ttoday 5pm est take questions president trumps immigration ban response new york\n",
      "0\t1.0\temergency rally trumps muslim travel ban nyc 125 5 pm\n"
     ]
    }
   ],
   "source": [
    "embCosine = cosine_similarity(sentenceEmbs[0].reshape(1, -1), sentenceEmbs)\n",
    "# extract 10 sentence with highest similarity\n",
    "indices = np.argpartition(embCosine[0], -10)[-10:]\n",
    "elements = embCosine[0][indices]\n",
    "print(\"Indices: {}\".format(indices))\n",
    "print(\"Cosine values: {}\".format(elements))\n",
    "print(\"Tweet 0:\", str(data.iloc[0]['Tweet1']))\n",
    "for i in indices:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, embCosine[0][i], str(data.iloc[i]['Tweet1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279\t[[0.85526013]]\tprotest today trump dapl downtown 5pm 611 woodward\n",
      "44958\t[[0.85747457]]\tseattle protest muslimban tomorrow westlake park 5pm\n",
      "40096\t[[0.8578342]]\temergency demo york tomorrow 5pm st helens square protest muslimban uk complicity mu\n",
      "52607\t[[0.8714714]]\tmorning half 5 today protest trumps muslim ban hopefully see ️️\n",
      "37999\t[[0.87335134]]\tcambridge friends rally 5pm tomorrow gsm trumps muslimban uk govts complicity whos coming ht\n",
      "52571\t[[0.8906126]]\tseattle join us 5pm westlake park protest president trumps immigration refugee ban\n",
      "25520\t[[0.897992]]\tprotest muslimban tomorrow dallas city hall 5 pm\n",
      "97778\t[[0.8847035]]\twest hollywood holding antitrump rally today 5pm pst ill defend great president\n",
      "90082\t[[0.87723655]]\ttoday 5pm est take questions president trumps immigration ban response new york\n",
      "0\t[[1.]]\temergency rally trumps muslim travel ban nyc 125 5 pm\n"
     ]
    }
   ],
   "source": [
    "for i in indices:\n",
    "    print(\"{}\\t{}\\t{}\".format(i, cosine_similarity(sentenceEmbs[0].reshape(1, -1), sentenceEmbs[i].reshape(1, -1)), str(data.iloc[i]['Tweet1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 768)\n"
     ]
    }
   ],
   "source": [
    "sents = sentenceEmbs[indices]\n",
    "print(sents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "num = np.dot(sents, sents.T)\n",
    "            \n",
    "if scipy.sparse.issparse(sents):\n",
    "    magnitude = norm(sents.toarray(), axis = 1)\n",
    "else:\n",
    "    magnitude = norm(sents, axis = 1)\n",
    "\n",
    "den = np.dot(magnitude.reshape(-1, 1), magnitude.T.reshape(1, -1))\n",
    "\n",
    "\n",
    "cosine_matrix = np.array(num/den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8552605 , 0.85747445, 0.85783404, 0.8714711 , 0.8733514 ,\n",
       "       0.8906124 , 0.89799184, 0.88470346, 0.87723655, 0.9999999 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_matrix[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lexrank with tfidf and bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123385,)\n"
     ]
    }
   ],
   "source": [
    "lsh = LSH(embeddingData)\n",
    "lsh.train(num_bits = 32)\n",
    "lex = LexRank(embeddingData, lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lex.build_graph(search_radius = 1, percent = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   0, 346, ...,   1,   0,  87], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lex.matrix.getnnz(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n"
     ]
    }
   ],
   "source": [
    "lex.train(lexrank_iter = 100, damping_factor = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentences.....\n",
      "0 ,  id:  105661 :  when will we get a prime minister who stands up against trump for british values top q by today\n",
      "1 ,  id:  108290 :  trumps america is a rogue state the remaining free world nations to impose sanctions before its too late\n",
      "2 ,  id:  24813 :  at jfk where protesters have totally shut down roads to terminal 4 a cheer erupts as news breaks that the\n",
      "3 ,  id:  75672 :  farsi speakers needed at sfo see below\n",
      "4 ,  id:  79361 :  honestly how many people does the uk government deport every single day why would they speak up against the muslimban\n",
      "5 ,  id:  118736 :  48 of trump voters think airport protesters across the country last weekend were paid to do so by george soros\n",
      "6 ,  id:  63237 :  why did miami submit to trumps executive order culture\n",
      "7 ,  id:  63259 :  over 1 million sign u k petition to ban trump from state visit\n",
      "8 ,  id:  119399 :  48 ppl killed by white terrorists in us while 26 were killed by radical islamists since 911\n",
      "9 ,  id:  55284 :  i understand a fight out between über and lyft but wouldnot it be more effective to support a total ban of all trump prod\n",
      "10 ,  id:  5238 :  hamas terrorist spox disapproves of president trump vetting pause\n",
      "11 ,  id:  37680 :  sunchat a cuomo if were deporting immigrants start with me are you an illegal alien are you a me refugee\n",
      "12 ,  id:  106199 :  is a common sense conservative president trump competence safety unity us jobs jobs jobs\n",
      "13 ,  id:  98424 :  if you disapprove of trumps refugee travel ban youre in the minority\n",
      "14 ,  id:  106760 :  aksibelataipan412 31 of americans say travel ban makes them safer poll\n"
     ]
    }
   ],
   "source": [
    "sentIds2 = lex.extract_sentences(n_sents = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3464\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(lex.graph[105661]))\n",
    "print(len(lex.graph[106760]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00012323896237877713\n",
      "1.8887369727844597e-06\n"
     ]
    }
   ],
   "source": [
    "print(lex.scores[105661])\n",
    "print(lex.scores[106760])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_lex.pkl', 'wb') as f:\n",
    "    pickle.dump(lex, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
