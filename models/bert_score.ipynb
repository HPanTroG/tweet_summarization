{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lsh import LSH\n",
    "from fast_lexrank import Lexrank\n",
    "import time, emoji, string\n",
    "# hide the loading messages\n",
    "import re\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  RT @MENTION : Emergency Rally Against Trump's ...\n",
      "1  824941519857610752  RT @MENTION : Theresa May has not apologized t...\n",
      "2  824941616314122240  RT @MENTION : Trump's Immigration Ban Excludes...\n",
      "3  824942056741167105  RT @MENTION : Trump's immigration order expand...\n",
      "4  824942966875774976  ALERT : Senator John McCain Threatens Action O...\n",
      "(123385, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/ehoang/hnt/data/processed_travel_ban.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet  \\\n",
      "0  824941360449015808  emergency rally against trump's muslim travel ...   \n",
      "1  824941519857610752  theresa may has not apologized to trump for in...   \n",
      "2  824941616314122240  trump's immigration ban excludes countries wit...   \n",
      "3  824942056741167105  trump's immigration order expands the definiti...   \n",
      "4  824942966875774976  alert : senator john mccain threatens action o...   \n",
      "\n",
      "                                              Tweet1  uniWPercent  \n",
      "0  emergency rally trumps muslim travel ban nyc 1...           10  \n",
      "1  theresa may apologized trump insulting fails t...           11  \n",
      "2  trumps immigration ban excludes countries busi...            9  \n",
      "3  trumps immigration order expands definition cr...            6  \n",
      "4  alert senator john mccain threatens action pre...            8  \n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())\n",
    "# remove stopwords, punctuation\n",
    "stopWords = stopwords.words('english')\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stopWords))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '',  string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—|→', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())\n",
    "\n",
    "# remove tweets #unique words less than haft of length\n",
    "data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['uniWPercent']!=0]\n",
    "# # remove tweets with lengths < 3, duplicates\n",
    "while data['uniWPercent'].min() <=2:\n",
    "    data = data[data['uniWPercent'] >2]\n",
    "    data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "# # # remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remained_index = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.iloc[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract tfidf vector\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfData = tfidf.fit_transform(data['Tweet1'])\n",
    "print(tfidfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_tfidf = LSH(tfidfData)\n",
    "lsh_tfidf.train(num_bits = 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = lsh_tfidf.extract_nearby_bins(max_search_radius = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in buckets:\n",
    "    print(len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scorers = []\n",
    "# for i in range(2):\n",
    "#     scorers.append(BERTScorer(lang='en', rescale_with_baseline = True, idf = True, \n",
    "#                               idf_sents = list(data['Tweet']), device = 'cuda:'+str(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer = BERTScorer(lang='en', rescale_with_baseline = True, idf = True, \n",
    "#                               idf_sents = list(data['Tweet']), device = 'cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lex_tfidf = Lexrank(np.array(data['Tweet']), lsh_tfidf)\n",
    "lex_tfidf.build_graph_bert_score(scorer, nJobs = 4, search_radius = 0, sim_thres = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n"
     ]
    }
   ],
   "source": [
    "lex_tfidf.train(lexrank_iter = 100, damping_factor = 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentences....\n",
      "Sent scores: 105175\n",
      "selected one: 2308, 0.00016267717545550078\n",
      "selected one: 2232, 0.00015204489298886547\n",
      "selected one: 6554, 0.0001435023933956082\n",
      "selected one: 9699, 0.00013920528327345002\n",
      "selected one: 9266, 0.0001366544223113021\n",
      "selected one: 1274, 0.00013273544847640603\n",
      "selected one: 11341, 0.00012631667204310456\n",
      "selected one: 10777, 0.00012625004801884056\n",
      "selected one: 214, 0.00012066388655633267\n",
      "selected one: 3896, 0.00012033727094492562\n",
      "selected one: 8132, 0.000119573180849666\n",
      "selected one: 1739, 0.00011814550255408933\n",
      "selected one: 4945, 0.00011669149593890295\n",
      "selected one: 8771, 0.00011033140623668407\n",
      "selected one: 8934, 0.00010782450588396402\n",
      "selected one: 8536, 0.00010773261463037123\n",
      "selected one: 5200, 0.00010726311248413626\n",
      "selected one: 7450, 0.00010631571531539408\n",
      "selected one: 9432, 0.00010585763972435693\n",
      "selected one: 20323, 0.00010549149471261904\n"
     ]
    }
   ],
   "source": [
    "sentIds = lex_tfidf.extract_summary(n_sents = 20, cosine_thres=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id #adjacentEdges lexrank\n",
      "0 2308 243 0.00016267717545550078\n",
      "1 2232 238 0.00015204489298886547\n",
      "2 6554 344 0.0001435023933956082\n",
      "3 9699 169 0.00013920528327345002\n",
      "4 9266 193 0.0001366544223113021\n",
      "5 1274 201 0.00013273544847640603\n",
      "6 11341 278 0.00012631667204310456\n",
      "7 10777 193 0.00012625004801884056\n",
      "8 214 149 0.00012066388655633267\n",
      "9 3896 219 0.00012033727094492562\n",
      "10 8132 190 0.000119573180849666\n",
      "11 1739 279 0.00011814550255408933\n",
      "12 4945 342 0.00011669149593890295\n",
      "13 8771 186 0.00011033140623668407\n",
      "14 8934 279 0.00010782450588396402\n",
      "15 8536 256 0.00010773261463037123\n",
      "16 5200 146 0.00010726311248413626\n",
      "17 7450 135 0.00010631571531539408\n",
      "18 9432 163 0.00010585763972435693\n",
      "19 20323 226 0.00010549149471261904\n"
     ]
    }
   ],
   "source": [
    "print(\"Id\", \"#adjacentEdges\", \"lexrank\")\n",
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, idx, len(lex_tfidf.graph[idx]), lex_tfidf.scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 president trump's \" extreme vetting \" plans is causing anxiety for u.s. muslims\n",
      "1 trump's \" extreme vetting \" plans causing anxiety for u.s. muslims\n",
      "2 breaking : refugees being detained at u.s. airports\n",
      "3 growing fallout from trump's new immigration crackdown :\n",
      "4 calls to ban muslims from entering the u.s. are offensive and unconstitutional .\n",
      "5 obama’s open borders policy undone : trump reverses course\n",
      "6 iran retaliates against trump order by banning u.s. visitors .\n",
      "7 please oppose the immigrant ban .\n",
      "8 link : trump's radical immigration plan : enforce the law .\n",
      "9 as trump issues his order on refugees and immigration , pro-lifers march in d.c. will they speak out on this issue ?\n",
      "10 muslim ban : refugees detained at u.s. airports under trump's immigration order ..\n",
      "11 trump’s immigration actions reverse obama’s open borders policy\n",
      "12 trump’s immigration ban is illegal\n",
      "13 . signs executive order banning syrian refugees from entering the u.s.\n",
      "14 protesters rally against trump's muslim immigration ban .\n",
      "15 hey america ! the last six presidents have blocked some immigrants using executive orders !\n",
      "16 fun fact : 5 of the 7 countries on the #muslimban list are currently being bombed by the u.s.a.\n",
      "17 blog : protecting america from ill intended refugees\n",
      "18 veterans voice concern about trump’s exec order on immigration\n",
      "19 u.s. tech leaders sound alarm over trump immigration ban\n"
     ]
    }
   ],
   "source": [
    "# with idf\n",
    "for i, idx in enumerate(sentIds):\n",
    "    print(i, data.iloc[idx]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lex_tfidf.graph[373]) # last selected tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lex_tfidf.graph[74531]) # the one selected by lex_tfidf, but not bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lex_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('bert_score_lsh_tfidf.pkl', 'rb') as f:\n",
    "    lex_tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_card_idx = data[data['Tweet'].str.contains('green card')].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  1216,   2323,   2419,   2609,   2755,   2995,   3099,   3202,\n",
       "              3259,   3385,\n",
       "            ...\n",
       "            102906, 102993, 103099, 103206, 103512, 103920, 104019, 104760,\n",
       "            104876, 104916],\n",
       "           dtype='int64', length=1446)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_card_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1446"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(green_card_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_card_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                                            825064765261176832\n",
       "Tweet          company sent out a notice about trump's muslim...\n",
       "Tweet1         company sent notice trumps muslim ban green ca...\n",
       "uniWPercent                                                   10\n",
       "Name: 1216, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[green_card_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"green_card_bertScore.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bert(start_idx, batch, device, thres):\n",
    "   \n",
    "    if start_idx + batch > len(green_card_idx):\n",
    "        end_idx = len(green_card_idx)\n",
    "    else:\n",
    "        end_idx = start_idx + batch\n",
    "    scorer = BERTScorer(lang='en', rescale_with_baseline = True, idf = False, \n",
    "                               device = 'cuda:'+str(device))\n",
    "    print(\"device: {}..running {}-{}\".format(device, start_idx, end_idx))\n",
    "    \n",
    "    for idx in range(start_idx, end_idx):\n",
    "        # compute bert score\n",
    "        count = 0\n",
    "        count_green_card = 0\n",
    "        time_start = time.time()\n",
    "        batch_size = 1000\n",
    "        for i in range(0, data.shape[0], batch_size):\n",
    "            rightBound = i+batch_size\n",
    "            if i + batch_size > data.shape[0]:\n",
    "                rightBound = data.shape[0]\n",
    "            can1 = scorer.score([str(data.iloc[green_card_idx[idx]]['Tweet'])]*(rightBound -i), list(data.iloc[i:rightBound]['Tweet']))[0]\n",
    "            count += sum(x>thres for x in can1)\n",
    "        can2 = scorer.score([str(data.iloc[green_card_idx[idx]]['Tweet'])]*len(green_card_idx), list(data.iloc[green_card_idx]['Tweet']))[0]\n",
    "        count_green_card = int(sum(x>thres for x in can2))\n",
    "        print(time.time() - time_start)\n",
    "        with open(file, 'a') as f:\n",
    "            f.write(\"{}, {}, {}, {}\\n\".format(green_card_idx[idx], int(count), count_green_card, round(count_green_card/int(count), 2)))\n",
    "#         print(\"idx: {}, #neighbors by bert: {}, cuda: {}, time: {}\".format(green_card_idx[idx], count, device, time.time()-time_start))\n",
    "#         print(\"...............................................\")\n",
    "        \n",
    "#     return idx_neighbors_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(0, len(green_card_idx), 145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,  145,  290,  435,  580,  725,  870, 1015, 1160, 1305])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 145\n",
    "results_xx = Parallel(n_jobs = 10)(delayed(compute_bert)(start, batch, i%10+1, 0.15) for i, start in enumerate(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
