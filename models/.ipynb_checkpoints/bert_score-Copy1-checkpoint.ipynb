{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nlp_path = os.path.abspath('../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def regex_or(*items):\n",
    "    return '(?:' + '|'.join(items) + ')'\n",
    "\n",
    "\n",
    "punctChars = r\"['\\\"“”‘’.?!…,:;]\"\n",
    "#punctSeq   = punctChars+\"+\"\t#'anthem'. => ' anthem '.\n",
    "punctSeq   = r\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\t#'anthem'. => ' anthem ' .\n",
    "entity     = r\"&(?:amp|lt|gt|quot);\"\n",
    "#  URLs\n",
    "urlStart1  = r\"(?:https?://|\\bwww\\.)\"\n",
    "commonTLDs = r\"(?:com|org|edu|gov|net|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|pro|tel|travel|xxx)\"\n",
    "ccTLDs\t = r\"(?:ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|\" + \\\n",
    "r\"bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|\" + \\\n",
    "r\"er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|\" + \\\n",
    "r\"hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|\" + \\\n",
    "r\"lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|\" + \\\n",
    "r\"nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|sk|\" + \\\n",
    "r\"sl|sm|sn|so|sr|ss|st|su|sv|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|\" + \\\n",
    "r\"va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|za|zm|zw)\"\t#TODO: remove obscure country domains?\n",
    "urlStart2  = r\"\\b(?:[A-Za-z\\d-])+(?:\\.[A-Za-z0-9]+){0,3}\\.\" + regex_or(commonTLDs, ccTLDs) + r\"(?:\\.\"+ccTLDs+r\")?(?=\\W|$)\"\n",
    "urlBody    = r\"(?:[^\\.\\s<>][^\\s<>]*?)?\"\n",
    "urlExtraCrapBeforeEnd = regex_or(punctChars, entity) + \"+?\"\n",
    "urlEnd     = r\"(?:\\.\\.+|[<>]|\\s|$)\"\n",
    "url        = regex_or(urlStart1, urlStart2) + urlBody + \"(?=(?:\"+urlExtraCrapBeforeEnd+\")?\"+urlEnd+\")\"\n",
    "\n",
    "\n",
    "Bound = r\"(?:\\W|^|$)\"\n",
    "Email = regex_or(\"(?<=(?:\\W))\", \"(?<=(?:^))\") + r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}(?=\" +Bound+\")\"\n",
    "AtMention = \"[@＠][a-zA-Z0-9_]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw = \"@huyen I am goinnt to school https://duo.com.vn\"\n",
    "s = re.sub(regex_or(url,Email), '', tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@huyen I am goinnt to school \n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lsh import LSH\n",
    "from fast_lexrank import Lexrank\n",
    "from numpy.linalg import norm\n",
    "import time, emoji, string\n",
    "from joblib import Parallel, delayed\n",
    "# hide the loading messages\n",
    "from utils import tokenizeRawTweetText\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                              Tweet\n",
      "0  824941360449015808  RT @nancykric: Emergency Rally Against Trump's...\n",
      "1  824941360449015808  RT @nancykric: Emergency Rally Against Trump's...\n",
      "2  824941360449015808  RT @nancykric: Emergency Rally Against Trump's...\n",
      "3  824941519857610752  RT @tponews: Theresa May has not apologized to...\n",
      "4  824941616314122240  RT @TheDemCoalition: Trump's Immigration Ban E...\n",
      "(123387, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "# data = pd.read_csv('/home/ehoang/hnt/data/processed_travel_ban_withURL.csv')\n",
    "# data = pd.read_csv('/home/ehoang/git/python/tweet_classification/data/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv')\n",
    "data = pd.read_csv('/home/ehoang/hnt/data/travel_ban.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['TweetId', 'Tweet', 'label']\n",
    "data = data[data['label'] == 'on-topic']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @nancykric: Emergency Rally Against Trump's Muslim Travel Ban in NYC, 1/25 at 5 p.m. https://t.co/BNC3Gz7NVb\n",
      "\n",
      "RT @nancykric: Emergency Rally Against Trump's Muslim Travel Ban in NYC, 1/25 at 5 p.m. https://t.co/BNC3Gz7NVb\n",
      "\n",
      "RT @nancykric: Emergency Rally Against Trump's Muslim Travel Ban in NYC, 1/25 at 5 p.m. https://t.co/BNC3Gz7NVb\n",
      "\n",
      "RT @tponews: Theresa May has not apologized to Trump for insulting him. If she fails to do that today, Trump should just send her back to B…\n",
      "\n",
      "RT @TheDemCoalition: Trump's Immigration Ban Excludes Countries with Business Ties https://t.co/8dtBQve4op via @bbgvisualdata #DemocracyFor…\n",
      "\n",
      "RT @nytimes: Trump's immigration order expands the definition of \"criminal\" https://t.co/fNpFTyQTwh https://t.co/AB8gSJdbyV\n",
      "\n",
      "ALERT: Senator John McCain Threatens Action On President Trump If He Does This https://t.co/5zhD1FjisZ\n",
      "\n",
      "@TrumpMyPres @realDonaldTrump @LeahR77 @Lrihendry @TruthFeedNews  Kiva still distracted while Trump gets on with people's business.\n",
      "\n",
      "RT @BillyBaldwin: TY @mcgregor_ewan for bailing on GMB &amp; @piersmorgan today.  Piers Morgan drank the Trump Kool Aid &amp; is a vocal opponent o…\n",
      "\n",
      "RT @LVNancy: ✍🏻#Trump to sign EO temporary ban suspending visas for Syria &amp; six other ME, African countries #BuildTheWall👍🏼… \n",
      "\n",
      "RT @MauriceMichael: Did we have a moral obligation to stop Hitler? If so we have a moral obligation to stop Trump.\n",
      "\n",
      "Are these people just now getting radicalized by Trump or did they always hate our freedom? https://t.co/nBKSDuzy6y\n",
      "\n",
      "Blacks are divided by Religion Donald Trump will help unite Us by \" PUSHING REAL GOOD \"  Min @LouisFarrakhan  https://t.co/NAe9UBnhU5\n",
      "\n",
      "Blacks are divided by Religion Donald Trump will help unite Us by \" PUSHING REAL GOOD \"  Min @LouisFarrakhan  https://t.co/K7D4BCCGIx\n",
      "\n",
      "Blacks are divided by Religion Donald Trump will help unite Us by \" PUSHING REAL GOOD \"  Min @LouisFarrakhan  https://t.co/XbAjNKx1zz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in list(data.iloc[0:15]['Tweet']):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda x: ' '.join(tokenizeRawTweetText(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT TWEETMENTION : Emergency Rally Against Trump's Muslim Travel Ban in NYC , 1/25 at 5 p.m. HTTPURL\n",
      "RT TWEETMENTION : Emergency Rally Against Trump's Muslim Travel Ban in NYC , 1/25 at 5 p.m. HTTPURL\n",
      "RT TWEETMENTION : Emergency Rally Against Trump's Muslim Travel Ban in NYC , 1/25 at 5 p.m. HTTPURL\n",
      "RT TWEETMENTION : Theresa May has not apologized to Trump for insulting him . If she fails to do that today , Trump should just send her back to B …\n",
      "RT TWEETMENTION : Trump's Immigration Ban Excludes Countries with Business Ties HTTPURL via TWEETMENTION #DemocracyFor …\n",
      "RT TWEETMENTION : Trump's immigration order expands the definition of \" criminal \" HTTPURL HTTPURL\n",
      "ALERT : Senator John McCain Threatens Action On President Trump If He Does This HTTPURL\n",
      "TWEETMENTION TWEETMENTION TWEETMENTION TWEETMENTION TWEETMENTION Kiva still distracted while Trump gets on with people's business .\n",
      "RT TWEETMENTION : TY TWEETMENTION for bailing on GMB & TWEETMENTION today . Piers Morgan drank the Trump Kool Aid & is a vocal opponent o …\n",
      "RT TWEETMENTION : ✍🏻 #Trump to sign EO temporary ban suspending visas for Syria & six other ME , African countries #BuildTheWall 👍🏼 …\n",
      "RT TWEETMENTION : Did we have a moral obligation to stop Hitler ? If so we have a moral obligation to stop Trump .\n",
      "Are these people just now getting radicalized by Trump or did they always hate our freedom ? HTTPURL\n",
      "Blacks are divided by Religion Donald Trump will help unite Us by \" PUSHING REAL GOOD \" Min TWEETMENTION HTTPURL\n",
      "Blacks are divided by Religion Donald Trump will help unite Us by \" PUSHING REAL GOOD \" Min TWEETMENTION HTTPURL\n",
      "Blacks are divided by Religion Donald Trump will help unite Us by \" PUSHING REAL GOOD \" Min TWEETMENTION HTTPURL\n"
     ]
    }
   ],
   "source": [
    "for tweet in list(data.iloc[0:15]['Tweet']):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('TWEETMENTION', \"\").\n",
    "                                    replace(\"EMAILADDRESS\", \"\").replace('HTTPURL', ''))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.lower().strip())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m.\n",
      "emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m.\n",
      "emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m.\n",
      "theresa may has not apologized to trump for insulting him . if she fails to do that today , trump should just send her back to b …\n",
      "trump's immigration ban excludes countries with business ties via #democracyfor …\n",
      "trump's immigration order expands the definition of \" criminal \"\n",
      "alert : senator john mccain threatens action on president trump if he does this\n",
      "kiva still distracted while trump gets on with people's business .\n",
      "ty for bailing on gmb & today . piers morgan drank the trump kool aid & is a vocal opponent o …\n",
      "#trump to sign eo temporary ban suspending visas for syria & six other me , african countries #buildthewall  …\n",
      "did we have a moral obligation to stop hitler ? if so we have a moral obligation to stop trump .\n",
      "are these people just now getting radicalized by trump or did they always hate our freedom ?\n",
      "blacks are divided by religion donald trump will help unite us by \" pushing real good \" min\n",
      "blacks are divided by religion donald trump will help unite us by \" pushing real good \" min\n",
      "blacks are divided by religion donald trump will help unite us by \" pushing real good \" min\n"
     ]
    }
   ],
   "source": [
    "for tweet in list(data.iloc[0:15]['Tweet']):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.iloc[0:5200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   id                                              Tweet  \\\n",
      "0  824941360449015808  emergency rally against trump's muslim travel ...   \n",
      "3  824941519857610752  theresa may has not apologized to trump for in...   \n",
      "4  824941616314122240  trump's immigration ban excludes countries wit...   \n",
      "5  824942056741167105  trump's immigration order expands the definiti...   \n",
      "6  824942966875774976  alert : senator john mccain threatens action o...   \n",
      "\n",
      "                                              Tweet1  uniWPercent  \n",
      "0  emergency rally trumps muslim travel ban nyc 1...           10  \n",
      "3  theresa may apologized trump insulting fails t...           11  \n",
      "4  trumps immigration ban excludes countries busi...            9  \n",
      "5  trumps immigration order expands definition cr...            6  \n",
      "6  alert senator john mccain threatens action pre...            8  \n"
     ]
    }
   ],
   "source": [
    "# remove stopwords, punctuation\n",
    "stopWords = stopwords.words('english')\n",
    "# data['Tweet1'] = data['Tweet'].apply(lambda x: re.sub(Email, '', x))\n",
    "# data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(url, '', x))\n",
    "# data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(AtMention, '', x))\n",
    "# data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "# data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "# data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stopWords))\n",
    "\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '',  string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—|→', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())\n",
    "\n",
    "# remove tweets #unique words less than haft of length\n",
    "data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['uniWPercent']!=0]\n",
    "# # remove tweets with lengths < 3, duplicates\n",
    "while data['uniWPercent'].min() <=2:\n",
    "    data = data[data['uniWPercent'] >2]\n",
    "    data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "# # # remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remained_index = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     0,      3,      4,      5,      6,      7,      8,      9,\n",
       "                10,     11,\n",
       "            ...\n",
       "            123376, 123377, 123378, 123379, 123380, 123381, 123382, 123383,\n",
       "            123385, 123386],\n",
       "           dtype='int64', length=105175)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remained_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105175, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.iloc[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Tweet'] = data['Tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(data.iloc[1]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexrank:\n",
    "    \"\"\"\n",
    "    lexrank model combined with lsh & cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.graph = {}\n",
    "        self.matrix = None\n",
    "        self.scores = None\n",
    "        \n",
    "        \n",
    "    def build_graph_bertscore_file(self, input_file, sim_thres=0.3):        \n",
    "        count = 0\n",
    "        with open(input_file, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                \n",
    "                content = line.split(\",\")\n",
    "                idx = int(content[0])\n",
    "                if idx not in remained_index:\n",
    "                    continue\n",
    "                if content[1][1:-1]==\"\":\n",
    "                    continue\n",
    "                print(content[1][1:-1])\n",
    "                print(content[2][1:-2])\n",
    "                neighbors = [int(x) for x in content[1][1:-1].split(',')]\n",
    "            \n",
    "                sim_scores = [float(y) for y in content[2][1:-2].split(\" \") if y!='']\n",
    "                \n",
    "                for j, neighbor_idx in enumerate(neighbors):\n",
    "                    if idx + neighbor_idx+1 not in remained_index or sim_scores[j] <sim_thres:\n",
    "                        continue\n",
    "\n",
    "                    if idx not in self.graph:\n",
    "                        self.graph[idx] = {}\n",
    "                    self.graph[idx][idx + neighbor_idx+1] = sim_scores[j]\n",
    "                    if idx+neighbor_idx+1 not in self.graph:\n",
    "                        self.graph[idx + neighbor_idx+1] = {}\n",
    "                    self.graph[idx + neighbor_idx+1][idx] = sim_scores[j]\n",
    "#                     print(idx, idx+j+1, score)\n",
    "                if i%200 == 0:\n",
    "                    print(\"Line: \", i, idx)\n",
    "        print(\"Done, \", i, count)\n",
    "           \n",
    "\n",
    "    def build_graph_bertscore(self, input_file, sim_thres=0.3):        \n",
    "        count = 0\n",
    "        with open(input_file, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                \n",
    "                \n",
    "                content = line[0:line.index('[')].split(',')\n",
    "                idx = int(content[0])\n",
    "#                 if idx not in remained_index:\n",
    "#                     continue\n",
    "                if (line[line.index('[')+1:len(line)-2]==\"\"):\n",
    "                    print(\"..................................\")\n",
    "                    continue\n",
    "                try:\n",
    "                    sim_scores = [float(x) for x in line[line.index('[')+1:len(line)-2].split(',')]\n",
    "                except Exception:\n",
    "                    print(line)\n",
    "                \n",
    "                for j, score in enumerate(sim_scores):\n",
    "                    if score <=sim_thres:\n",
    "                        continue\n",
    "#                     if (idx+j+1 not in remained_index):\n",
    "#                         continue\n",
    "                    if idx not in self.graph:\n",
    "                        self.graph[idx] = {}\n",
    "                    self.graph[idx][idx+j+1] = score\n",
    "                    if idx+j+1 not in self.graph:\n",
    "                        self.graph[idx+j+1] = {}\n",
    "                    self.graph[idx+j+1][idx] = score\n",
    "#                     print(idx, idx+j+1, score)\n",
    "                if i%200 == 0:\n",
    "                    print(\"Line: \", i, idx)\n",
    "        print(\"Done, \", i, count)\n",
    "\n",
    "    def build_graph_cosine1(self, sim_thres = 0.3, batch_size = 1000):\n",
    "        for i in range(0, data.shape[0], 1):\n",
    "            cosine = cosine_similarity(self.data[i], self.data)[0]\n",
    "            idx = np.where(cosine>sim_thres)[0]\n",
    "            for id in idx:\n",
    "                if id==i:\n",
    "                    continue\n",
    "                if i not in self.graph:\n",
    "                    self.graph[i] = {}\n",
    "                self.graph[i][id] = cosine[id]\n",
    "                \n",
    "            if i%1000 == 0:\n",
    "                print(\"Line: \", i)\n",
    "                    \n",
    "    def build_graph_cosine(self, sim_thres = 0.3, batch_size = 1000):\n",
    "        for i in range(0, data.shape[0], batch_size):\n",
    "            j = i\n",
    "            if i + batch_size>data.shape[0]:\n",
    "                sents = self.data[i:data.shape[0]]\n",
    "\n",
    "            else:\n",
    "                sents = self.data[i:i+batch_size]\n",
    "            \n",
    "            for j in range(i, data.shape[0], batch_size):\n",
    "#                 if j == 0:\n",
    "#                     continue\n",
    "                rightBound = j+batch_size\n",
    "                if rightBound > data.shape[0]:\n",
    "                    rightBound = data.shape[0]\n",
    "\n",
    "                sents1 = self.data[j: rightBound]  # get list of sentVecs\n",
    "\n",
    "                num = np.dot(sents, sents1.T)\n",
    "                if scipy.sparse.issparse(sents):\n",
    "                    magnitude = norm(sents.toarray(), axis=1)\n",
    "                    magnitude1 = norm(sents1.toarray(), axis=1)\n",
    "                else:\n",
    "                    magnitude = norm(sents, axis=1)\n",
    "                    magnitude1 = norm(sents1, axis=1)\n",
    "                den = np.dot(magnitude.reshape(-1, 1), magnitude1.T.reshape(1, -1))\n",
    "\n",
    "                cosine_matrix = np.array(num / den)\n",
    "                indices = np.where(cosine_matrix > sim_thres)\n",
    "#                 print(\"matrix:\", cosine_matrix.shape)\n",
    "                if len(indices[0]) == 0:\n",
    "                    continue\n",
    "                try:\n",
    "                    for row, col in zip(indices[0], indices[1]):\n",
    "                        if i+row != j+col:  # ignore self-links\n",
    "        #                     matrix_indices.append([b[row], b[col]])\n",
    "        #                     weights.append(cosine_matrix[row][col])\n",
    "        #                     weights.append(1)\n",
    "\n",
    "                            if i+row not in self.graph:\n",
    "                                self.graph[i+row] = {}\n",
    "                            if j+col not in self.graph:\n",
    "                                self.graph[j+col] = {}\n",
    "\n",
    "                            self.graph[i+row][j+col] = cosine_matrix[row][col]\n",
    "                            self.graph[j+col][i+row] = cosine_matrix[row][col]\n",
    "    #                         print(i, j, i+row, j+col, cosine_matrix[row][col])\n",
    "                except Exception:\n",
    "                    print(\"Error:\", i, j, row, col, cosine_matrix.shape)\n",
    "\n",
    "    # using pagerank pagekage\n",
    "    def page_rank(self, damping_factor=0.85):\n",
    "        pr = pagerank(self.matrix, p=damping_factor)\n",
    "        self.scores = pr\n",
    "\n",
    "    def train(self, lexrank_iter=100, damping_factor=0.85):\n",
    "        n = self.data.shape[0]\n",
    "\n",
    "        # for each node: compute sum of weights of adjacent nodes\n",
    "        sum_weights = {}\n",
    "        for sent, adj in self.graph.items():\n",
    "            sum_weights[sent] = sum(adj.values())\n",
    "\n",
    "        self.scores = [1 / n] * n  # initialize pagerank scores\n",
    "        try:\n",
    "\n",
    "            for iter in range(lexrank_iter):\n",
    "                if iter % 10 == 0:\n",
    "                    print(\"Iteration: {}\".format(iter))\n",
    "                for sent, adjs in self.graph.items():\n",
    "                    score = 0\n",
    "                    for adj, value in adjs.items():\n",
    "                        score += self.scores[adj] * value / sum_weights[adj]\n",
    "                    self.scores[sent] = (1 - damping_factor)/n +damping_factor * score\n",
    "        except Exception:\n",
    "            print(sent)\n",
    "            print(adj)\n",
    "\n",
    "    def extract_summary(self, n_sents=10, cosine_thres=0.5, max_sent=100):\n",
    "\n",
    "        sentIds = []\n",
    "        sentScores = np.array(self.scores.copy())\n",
    "\n",
    "        print(\"Extracting sentences....\")\n",
    "        # get #max_sent maximal scores along with its indices\n",
    "        print(\"Sent scores: {}\".format(len(sentScores)))\n",
    "\n",
    "        indices = np.argpartition(sentScores, -max_sent)[-max_sent:]\n",
    "        values = sentScores[indices]\n",
    "        max_index_value = {key: value for key, value in zip(indices, values)}\n",
    "        max_index_value = sorted(max_index_value.items(), key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        i = 0\n",
    "        while i < n_sents:\n",
    "            index, value = max_index_value.pop()\n",
    "            if index not in self.graph:\n",
    "                print(\"Sent {} not in graph\".format(index))\n",
    "                continue\n",
    "            assign = 1\n",
    "            # iterate selected sentences\n",
    "            for idx in sentIds:\n",
    "                # if new index is not an ajdacent node of the selected one\n",
    "                if idx not in self.graph[index]:\n",
    "                    continue\n",
    "                similarity = self.graph[index][idx]\n",
    "                if similarity > cosine_thres:\n",
    "                    print(\"Sent {} is similar to a {}: {}\".format(index, idx, similarity))\n",
    "                    assign = 0\n",
    "                    break\n",
    "            if assign == 1:\n",
    "#                 print(i, \", \", 'TweetId: ', self.data.iloc[index]['Id'], \": \", self.data.iloc[index]['Tweet'])\n",
    "                print(\"selected one: {}, {}\".format(index, value))\n",
    "                sentIds.append(index)\n",
    "                i += 1\n",
    "        return sentIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = Lexrank(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.build_graph_bertscore(input_file = \"/home/ehoang/hnt/data/files/bertscore_sandy_noURLs_tfidf.txt\", sim_thres=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lex.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex.train(lexrank_iter=100, damping_factor=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_dict = {}\n",
    "for key, value in lex_tfidf.graph.items():\n",
    "    bertscore_dict[key] = len(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_dict = {k: v for k, v in sorted(bertscore_dict.items(), key=lambda item: item[1], reverse = True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "selected = []\n",
    "for key, value in bertscore_dict.items():\n",
    "    \n",
    "    if count>0:\n",
    "        added = True\n",
    "        for k in selected:\n",
    "            if k in lex_tfidf.graph[key]:\n",
    "                if lex_tfidf.graph[key][k]>0.88:\n",
    "                    added = False\n",
    "                    break\n",
    "        \n",
    "        if added==True:\n",
    "            selected.append(key)\n",
    "            count+=1\n",
    "            print(count, \".\", key, str(data.iloc[key]['Tweet']))\n",
    "    else:\n",
    "        selected.append(key)\n",
    "        count+=1\n",
    "        print(key, str(data.iloc[key]['Tweet']))\n",
    "            \n",
    "    if count > 20: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentIds = lex.extract_summary(n_sents=20, cosine_thres=0.18, max_sent=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, idx in enumerate(sentIds):\n",
    "    print(i+1,\". \", idx, len(lex.graph[idx]), data.iloc[idx]['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5350, 8255)\n"
     ]
    }
   ],
   "source": [
    "#extract tfidf vector\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidfData = tfidf.fit_transform(data['Tweet1'])\n",
    "print(tfidfData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf = Lexrank(tfidfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf.build_graph_cosine(sim_thres = 0.1, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n"
     ]
    }
   ],
   "source": [
    "lex_tfidf.train(lexrank_iter=100, damping_factor=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sentences....\n",
      "Sent scores: 5350\n",
      "selected one: 614, 0.0008448568930018358\n",
      "selected one: 324, 0.0008299505957066723\n",
      "selected one: 1792, 0.0008257212484580751\n",
      "Sent 1560 is similar to a 1792: 0.5047185108189062\n",
      "Sent 333 is similar to a 1792: 0.4226943089601616\n",
      "Sent 4422 is similar to a 1792: 0.3056238495180111\n",
      "Sent 2021 is similar to a 1792: 0.7796535378958315\n",
      "selected one: 2647, 0.0007641701652448602\n",
      "Sent 5029 is similar to a 1792: 0.3305511279545086\n",
      "selected one: 2510, 0.0007476430929619427\n",
      "Sent 3274 is similar to a 1792: 0.4887843152610539\n",
      "selected one: 4027, 0.0007293385148713963\n",
      "Sent 4485 is similar to a 1792: 0.3887482238384022\n",
      "Sent 2741 is similar to a 4027: 0.8139025672217256\n",
      "Sent 4307 is similar to a 1792: 0.3106724637447997\n",
      "Sent 3621 is similar to a 2510: 0.40531228106523576\n",
      "Sent 3509 is similar to a 1792: 0.3640542503204733\n",
      "Sent 3668 is similar to a 1792: 0.533825730045829\n",
      "Sent 1205 is similar to a 324: 0.4630971608420549\n",
      "Sent 2462 is similar to a 1792: 0.4690637173113854\n",
      "selected one: 4875, 0.0006883122357572027\n",
      "Sent 4275 is similar to a 1792: 0.4280377858702633\n",
      "Sent 60 is similar to a 614: 0.3312190594260512\n",
      "Sent 5048 is similar to a 1792: 0.49208091184939734\n",
      "selected one: 30, 0.0006764174267043355\n",
      "Sent 179 is similar to a 4027: 0.30605047198407226\n",
      "Sent 3755 is similar to a 324: 0.8254596281092088\n",
      "Sent 678 is similar to a 324: 0.5378267636915992\n",
      "Sent 2797 is similar to a 324: 0.44985990741562576\n",
      "Sent 257 is similar to a 614: 0.39720142906022254\n",
      "Sent 4370 is similar to a 1792: 0.42847463196122937\n",
      "Sent 5324 is similar to a 1792: 0.564147559607847\n",
      "Sent 4265 is similar to a 1792: 0.3249016250207438\n",
      "Sent 1767 is similar to a 324: 0.3660481663741603\n",
      "Sent 3731 is similar to a 1792: 0.3544002869505913\n",
      "Sent 3729 is similar to a 1792: 0.4920408987232274\n",
      "Sent 3778 is similar to a 4027: 0.5184228692073167\n",
      "Sent 3715 is similar to a 614: 0.5589252447042332\n",
      "selected one: 2403, 0.0006215787742853855\n",
      "Sent 4758 is similar to a 1792: 0.3442527589876233\n",
      "Sent 1424 is similar to a 1792: 0.34502321426501587\n",
      "Sent 3087 is similar to a 614: 0.35049097600950574\n",
      "Sent 2381 is similar to a 4027: 0.46683332898186614\n",
      "Sent 2035 is similar to a 1792: 0.3264713556651169\n",
      "Sent 1313 is similar to a 4027: 0.4333019943348503\n",
      "selected one: 4125, 0.0005915252863763653\n",
      "Sent 4349 is similar to a 2647: 0.3451078640868388\n",
      "Sent 1583 is similar to a 1792: 0.6521128089487054\n",
      "Sent 2997 is similar to a 614: 0.33090693327091675\n",
      "selected one: 3693, 0.0005812946363035525\n",
      "Sent 4732 is similar to a 30: 0.3136258659547219\n",
      "Sent 2884 is similar to a 30: 0.47515280538531907\n",
      "Sent 2768 is similar to a 1792: 0.43785576837605855\n",
      "Sent 1820 is similar to a 4027: 0.504376290960203\n",
      "selected one: 4567, 0.0005694104135463131\n",
      "Sent 3236 is similar to a 1792: 0.3042217609369895\n",
      "Sent 4774 is similar to a 4027: 0.3602246639989658\n",
      "Sent 1812 is similar to a 2510: 0.4113500430106119\n",
      "selected one: 2564, 0.0005664809055517697\n",
      "Sent 2748 is similar to a 324: 0.31901848794277554\n",
      "selected one: 5303, 0.0005625361246114058\n",
      "Sent 2026 is similar to a 4027: 0.7079804892754067\n",
      "Sent 4509 is similar to a 2510: 0.47652183646909985\n",
      "Sent 3867 is similar to a 1792: 0.5956188897715204\n",
      "Sent 1460 is similar to a 1792: 0.39670637931892333\n",
      "Sent 1446 is similar to a 4027: 0.6342144186240015\n",
      "Sent 2749 is similar to a 2647: 0.5592606076962777\n",
      "selected one: 4619, 0.0005522859971538703\n",
      "selected one: 894, 0.0005515234029815739\n",
      "Sent 2848 is similar to a 30: 0.7359964683187484\n",
      "Sent 1474 is similar to a 1792: 0.3054859820131863\n",
      "Sent 4063 is similar to a 1792: 0.4117603135080525\n",
      "Sent 3489 is similar to a 324: 0.4059253638357955\n",
      "selected one: 4608, 0.0005473690880035068\n",
      "selected one: 1052, 0.0005466756756556634\n",
      "Sent 380 is similar to a 1792: 0.4802152017142739\n",
      "Sent 3448 is similar to a 2510: 0.4986245055688358\n",
      "Sent 1835 is similar to a 614: 0.3769289367483304\n",
      "Sent 2563 is similar to a 324: 0.42670566866023785\n",
      "selected one: 3036, 0.0005411367606042481\n",
      "selected one: 2602, 0.0005384698643171274\n"
     ]
    }
   ],
   "source": [
    "sentIds_cosine = lex_tfidf.extract_summary(n_sents=20, cosine_thres=0.3, max_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "614 472 0.0008448568930018358\n",
      "324 436 0.0008299505957066723\n",
      "1792 478 0.0008257212484580751\n",
      "2647 488 0.0007641701652448602\n",
      "2510 495 0.0007476430929619427\n",
      "4027 420 0.0007293385148713963\n",
      "4875 431 0.0006883122357572027\n",
      "30 464 0.0006764174267043355\n",
      "2403 338 0.0006215787742853855\n",
      "4125 362 0.0005915252863763653\n",
      "3693 360 0.0005812946363035525\n",
      "4567 340 0.0005694104135463131\n",
      "2564 364 0.0005664809055517697\n",
      "5303 354 0.0005625361246114058\n",
      "4619 361 0.0005522859971538703\n",
      "894 360 0.0005515234029815739\n",
      "4608 343 0.0005473690880035068\n",
      "1052 338 0.0005466756756556634\n",
      "3036 329 0.0005411367606042481\n",
      "2602 353 0.0005384698643171274\n"
     ]
    }
   ],
   "source": [
    "for idx in sentIds_cosine:\n",
    "    print(idx, len(lex_tfidf.graph[idx]), lex_tfidf.scores[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  614 i'm in nyc & sandy is here !!!!\n",
      "2 .  324 what people in new york do during a hurricane\n",
      "3 .  1792 for everyone on the east coast in the path of hurricane sandy , please be safe !\n",
      "4 .  2647 watch hurricane sandy like not even hit us ...\n",
      "5 .  2510 i hope i don't have school for this hurricane but i still want power lol\n",
      "6 .  4027 thoughts and prayers go out to all those affected by hurricane sandy . \\hope that everyone is safe .. #hurricanesandy\n",
      "7 .  4875 this hurricane was all over the news like come on you know a hurricane was coming\n",
      "8 .  30 guys !!!! it's getting really bad out there now !!! hope everyone @ frankenstorm apocalypse - hurricane sandy\n",
      "9 .  2403 i didn't even know there was a hurricane lol\n",
      "10 .  4125 gonna see him tomorrow if this hurricane isn't so bad :)\n",
      "11 .  3693 lmao , people would make a twitter for hurricane sandy\n",
      "12 .  4567 there's really no need for a hurricane right now .\n",
      "13 .  2564 power is back ! take that #frankenstorm\n",
      "14 .  5303 #pray for all the people in the east coast . its going down hard out there with the hurricane .\n",
      "15 .  4619 what hurricane ? like everyone is taking about a hurricane ?\n",
      "16 .  894 i would live to go with you big there hurricane coming at me\n",
      "17 .  4608 i hope all my friends and family in ny are safe from hurricane sandy\n",
      "18 .  1052 i'm getting hit by a hurricane on monday !!!\n",
      "19 .  3036 hurricane sandy is gonna make us all her bitch this week\n",
      "20 .  2602 people making a twitter for hurricane sandy , that shit ain't funny a bunch of people have died from that shit\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(sentIds_cosine):\n",
    "    print(i+1, \". \", idx, data.iloc[idx]['Tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert sentence + cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting bertsen embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "modelSent = SentenceTransformer('bert-base-nli-mean-tokens', device='cuda:3')\n",
    "embeddings = np.empty((0, 768))\n",
    "\n",
    "leftBound = 0\n",
    "with torch.no_grad():\n",
    "    while  leftBound < data.shape[0]:\n",
    "        rightBound = leftBound+batch_size\n",
    "        if rightBound > data.shape[0]:\n",
    "            rightBound = data.shape[0]\n",
    "        embeddings = np.concatenate((embeddings, modelSent.encode(list(data.iloc[leftBound:rightBound]['Tweet']))), axis=0)\n",
    "        leftBound += batch_size\n",
    "        print(\"Len: \", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf = Lexrank(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf.build_graph_cosine(sim_thres = 0.78, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_tfidf.train(lexrank_iter=100, damping_factor=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentIds_cosine = lex_tfidf.extract_summary(n_sents=15, cosine_thres=0.88, max_sent=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 5000 tweets travel ban with url\n",
    "for i, idx in enumerate(sentIds_cosine):\n",
    "    print(i,\".\",idx, data.iloc[idx]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5000 tweets travel ban without url\n",
    "for idx in sentIds_cosine:\n",
    "    print(idx, data.iloc[idx]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_dict = {}\n",
    "for key, value in lex_tfidf.graph.items():\n",
    "    bertscore_dict[key] = len(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_dict = {k: v for k, v in sorted(bertscore_dict.items(), key=lambda item: item[1], reverse = True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "selected = []\n",
    "for key, value in bertscore_dict.items():\n",
    "    \n",
    "    if count>0:\n",
    "        added = True\n",
    "        for k in selected:\n",
    "            if k in lex.graph[key]:\n",
    "                if lex.graph[key][k]>0.18:\n",
    "                    added = False\n",
    "                    break\n",
    "        \n",
    "        if added==True:\n",
    "            selected.append(key)\n",
    "            count+=1\n",
    "            print(count, \".\", key, str(data.iloc[key]['Tweet']))\n",
    "    else:\n",
    "        selected.append(key)\n",
    "        count+=1\n",
    "        print(key, str(data.iloc[key]['Tweet']))\n",
    "            \n",
    "-    if count > 15: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracting bertscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "with open(\"/home/ehoang/hnt/data/files/travel_ban_all.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        idx = line.split(',')[0]\n",
    "        indices.append(int(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = sorted(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconsidered = [i for i in range(data.shape[0]) if i not in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86305 86305\n"
     ]
    }
   ],
   "source": [
    "print(len(unconsidered), data.shape[0] - len(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unconsidered = np.arange(0, data.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batches = []\n",
    "size = 2158\n",
    "for i in range(0, len(unconsidered), size):\n",
    "    if i+size > len(unconsidered):\n",
    "        rightB = len(unconsidered)\n",
    "    else:\n",
    "        rightB = i+size\n",
    "    new_batches.append([j for j in unconsidered[i:rightB]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86305 86305 40\n"
     ]
    }
   ],
   "source": [
    "x = sum(len(y) for y in new_batches)\n",
    "print(x, len(unconsidered), len(new_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/home/ehoang/hnt/data/files/travel_ban_all.txt\"\n",
    "def compute_bert(batch, device, thres):\n",
    "   \n",
    "    scorer = BERTScorer(lang='en', rescale_with_baseline = True, idf = True, idf_sents = list(data['Tweet']), \n",
    "                               device = 'cuda:'+str(device))\n",
    "    print(\"device: {}..running {}\".format(device, len(batch)))\n",
    "    \n",
    "    for idx in batch:\n",
    "        # compute bert score\n",
    "        batch_size = 1000\n",
    "        sim_score = []\n",
    "        sim_idx = []\n",
    "        for i in range(idx+1, data.shape[0], batch_size):\n",
    "            rightBound = i+batch_size\n",
    "            if i + batch_size > data.shape[0]:\n",
    "                rightBound = data.shape[0]\n",
    "            sim = scorer.score([str(data.iloc[idx]['Tweet'])]*(rightBound -i), list(data.iloc[i:rightBound]['Tweet']))[0]\n",
    "            sim_score+=list(sim.numpy())\n",
    "#         print(\"len: \", len(sim_score))\n",
    "        sim_score = np.array(sim_score)\n",
    "        sim_idx = np.where(sim_score>thres)[0]\n",
    "        sim_score = sim_score[sim_idx]\n",
    "#         if len(sim_idx) == 0:\n",
    "#             continue\n",
    "        if (len(sim_idx) !=0):\n",
    "            sim_idx = sim_idx+(idx+1)\n",
    "        with open(file, 'a') as f:\n",
    "            f.write(\"{},{},{}\\n\".format(idx, str(list(sim_idx)), str(list(sim_score))))\n",
    "#         print(\"{},{},{}\\n\".format(idx, str(list(sim_idx)), str(list(sim_score))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_jobs = 40\n",
    "results_xx = Parallel(n_jobs = n_jobs)(delayed(compute_bert)(batch, i%3+2, 0.0) for i, batch in enumerate(new_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
