{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization:\n",
    "1. K mean with sentence embeddings returned by SentenceTransformer\n",
    "2. Biterm\n",
    "3. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n",
      "[nltk_data] Downloading package wordnet to /home/nguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import string\n",
    "import emoji\n",
    "import nltk, time\n",
    "nltk.download('wordnet')\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "from biterm.cbtm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  RT @MENTION : Emergency Rally Against Trump's ...\n",
      "1  824941519857610752  RT @MENTION : Theresa May has not apologized t...\n",
      "2  824941616314122240  RT @MENTION : Trump's Immigration Ban Excludes...\n",
      "3  824942056741167105  RT @MENTION : Trump's immigration order expand...\n",
      "4  824942966875774976  ALERT : Senator John McCain Threatens Action O...\n",
      "(123385, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/nguyen/data/processed_travel_ban.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"RT @MENTION : Emergency Rally Against Trump's Muslim Travel Ban in NYC , 1/25 at 5 p.m. @URL\",\n",
       " 'RT @MENTION : Theresa May has not apologized to Trump for insulting him . If she fails to do that today , Trump should just send her back to B ‚Ä¶',\n",
       " \"RT @MENTION : Trump's Immigration Ban Excludes Countries with Business Ties @URL via @MENTION #DemocracyFor ‚Ä¶\",\n",
       " 'RT @MENTION : Trump\\'s immigration order expands the definition of \" criminal \" @URL @URL',\n",
       " 'ALERT : Senator John McCain Threatens Action On President Trump If He Does This @URL',\n",
       " \"@MENTION @MENTION @MENTION @MENTION @MENTION Kiva still distracted while Trump gets on with people's business .\",\n",
       " 'RT @MENTION : TY @MENTION for bailing on GMB & @MENTION today . Piers Morgan drank the Trump Kool Aid & is a vocal opponent o ‚Ä¶',\n",
       " 'RT @MENTION : ‚úçüèª #Trump to sign EO temporary ban suspending visas for Syria & six other ME , African countries #BuildTheWall üëçüèº ‚Ä¶',\n",
       " 'RT @MENTION : Did we have a moral obligation to stop Hitler ? If so we have a moral obligation to stop Trump .',\n",
       " 'Are these people just now getting radicalized by Trump or did they always hate our freedom ? @URL']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[0:10]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  emergency rally against trump's muslim travel ...\n",
      "1  824941519857610752  theresa may has not apologized to trump for in...\n",
      "2  824941616314122240  trump's immigration ban excludes countries wit...\n",
      "3  824942056741167105  trump's immigration order expands the definiti...\n",
      "4  824942966875774976  alert : senator john mccain threatens action o...\n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m. \",\n",
       " 'theresa may has not apologized to trump for insulting him . if she fails to do that today , trump should just send her back to b ‚Ä¶',\n",
       " \"trump's immigration ban excludes countries with business ties via #democracyfor ‚Ä¶\",\n",
       " 'trump\\'s immigration order expands the definition of \" criminal \" ',\n",
       " 'alert : senator john mccain threatens action on president trump if he does this ',\n",
       " \" kiva still distracted while trump gets on with people's business .\",\n",
       " 'ty for bailing on gmb & today . piers morgan drank the trump kool aid & is a vocal opponent o ‚Ä¶',\n",
       " ' #trump to sign eo temporary ban suspending visas for syria & six other me , african countries #buildthewall  ‚Ä¶',\n",
       " 'did we have a moral obligation to stop hitler ? if so we have a moral obligation to stop trump .',\n",
       " 'are these people just now getting radicalized by trump or did they always hate our freedom ? ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[0:10]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMean(data, sentenceEmbs, n_clusters = 20, random_states = 10):\n",
    "    print(\"data Shape: \", data.shape)\n",
    "    print(\"Embedding shape: \", sentenceEmbs.shape)\n",
    "    sentences = []\n",
    "    for x in sentenceEmbs:\n",
    "        sentences.append(x.ravel())\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(sentences)\n",
    "    labels = kmeans.labels_.tolist()\n",
    "    \n",
    "    centers = np.array(kmeans.cluster_centers_)\n",
    "    # compute cluster size:\n",
    "    cluster_size = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_size[i] = labels.count(i)\n",
    "        \n",
    "    # find elements closest to the cluster centers\n",
    "    closest_data = []\n",
    "    for i in range(n_clusters):\n",
    "        center_vec = centers[i].reshape(1, -1)\n",
    "        data_idx_within_i_cluster = [ idx for idx, clu_num in enumerate(labels) if clu_num == i ]\n",
    "\n",
    "        one_cluster_tf_matrix = np.zeros( (len(data_idx_within_i_cluster) , centers.shape[1] ) )\n",
    "        for row_num, data_idx in enumerate(data_idx_within_i_cluster):\n",
    "            one_row = sentences[data_idx]\n",
    "            one_cluster_tf_matrix[row_num] = one_row\n",
    "\n",
    "\n",
    "        closest, _ = pairwise_distances_argmin_min(center_vec, one_cluster_tf_matrix)\n",
    "        closest_idx_in_one_cluster_tf_matrix = closest[0]\n",
    "        closest_data_row_num = data_idx_within_i_cluster[closest_idx_in_one_cluster_tf_matrix]\n",
    "    #     data_id = all_data[closest_data_row_num]\n",
    "\n",
    "        closest_data.append(closest_data_row_num)\n",
    "\n",
    "    closest_data = list(set(closest_data))\n",
    "    \n",
    "    return labels, closest_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kmean: first token bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nguyen/data/travel_ban_first_token_embeddings.pkl', 'rb') as f:\n",
    "    sentenceEmbs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Shape:  (123385, 2)\n",
      "Embedding shape:  (123385, 768)\n"
     ]
    }
   ],
   "source": [
    "cluster_size, closest_data = kMean(data, sentenceEmbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in closest_data:\n",
    "    print(i, str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kmean: sentence transformers embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nguyen/data/travel_ban_sentence_transformers_embeddings.pkl', 'rb') as f:\n",
    "    sentenceEmbs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size, closest_data = kMean(data, sentenceEmbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in closest_data:\n",
    "    print(i, str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
