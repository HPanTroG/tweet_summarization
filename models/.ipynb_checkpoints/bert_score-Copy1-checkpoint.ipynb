{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from lsh import LSH\n",
    "from fast_lexrank import Lexrank\n",
    "import time, emoji, string\n",
    "from joblib import Parallel, delayed\n",
    "# hide the loading messages\n",
    "import re\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweet id                                              tweet  \\\n",
      "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
      "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
      "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
      "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
      "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
      "\n",
      "       label  \n",
      "0  off-topic  \n",
      "1   on-topic  \n",
      "2  off-topic  \n",
      "3  off-topic  \n",
      "4  off-topic  \n",
      "(10008, 3)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/ehoang/git/python/tweet_classification/data/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['TweetId', 'Tweet', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6138, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['label'] == 'on-topic']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 TweetId                                              Tweet  \\\n",
      "1   '263044104500420609'  sandy be soooo mad that she be shattering our ...   \n",
      "5   '263101347421888513'  neighborly duties. @cory_kennedy arrives to th...   \n",
      "7   '263298821189156865'  i don't know how i'm getting back to jersey si...   \n",
      "10  '262914476989358080'  already flooded so much #sandy @ hoboken http:...   \n",
      "12  '262991999911743490'  on that note, i pray that everyone stays safe,...   \n",
      "\n",
      "       label                                             Tweet1  uniWPercent  \n",
      "1   on-topic  sandy soooo mad shattering doors shiet hurrica...            7  \n",
      "5   on-topic  neighborly duties corykennedy arrives rescue s...           12  \n",
      "7   on-topic  know im getting back jersey since trains subwa...            9  \n",
      "10  on-topic  already flooded much sandy hoboken httptcomphf...            6  \n",
      "12  on-topic  note pray everyone stays safe keeps positive a...            9  \n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())\n",
    "# remove stopwords, punctuation\n",
    "stopWords = stopwords.words('english')\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stopWords))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '',  string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—|→', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())\n",
    "\n",
    "# remove tweets #unique words less than haft of length\n",
    "data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['uniWPercent']!=0]\n",
    "# # remove tweets with lengths < 3, duplicates\n",
    "while data['uniWPercent'].min() <=2:\n",
    "    data = data[data['uniWPercent'] >2]\n",
    "    data['uniWPercent'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "# # # remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5563, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remained_index = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5563, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lexrank:\n",
    "    \"\"\"\n",
    "    lexrank model combined with lsh & cosine similarity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, lsh):\n",
    "        self.data = data\n",
    "        self.lsh = lsh\n",
    "        self.graph = {}\n",
    "        self.matrix = None\n",
    "        self.scores = None\n",
    "           \n",
    "\n",
    "    def build_graph(self, input_file, sim_thres=0.3):        \n",
    "        \n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                print(line.split(',')[0])\n",
    "            break\n",
    "            \n",
    "        \n",
    "           \n",
    "\n",
    "    # using pagerank pagekage\n",
    "    def page_rank(self, damping_factor=0.85):\n",
    "        pr = pagerank(self.matrix, p=damping_factor)\n",
    "        self.scores = pr\n",
    "\n",
    "    def train(self, lexrank_iter=100, damping_factor=0.85):\n",
    "        n = self.data.shape[0]\n",
    "\n",
    "        # for each node: compute sum of weights of adjacent nodes\n",
    "        sum_weights = {}\n",
    "        for sent, adj in self.graph.items():\n",
    "            sum_weights[sent] = sum(adj.values())\n",
    "\n",
    "        self.scores = [1 / n] * n  # initialize pagerank scores\n",
    "\n",
    "        for iter in range(lexrank_iter):\n",
    "            if iter % 10 == 0:\n",
    "                print(\"Iteration: {}\".format(iter))\n",
    "            for sent, adjs in self.graph.items():\n",
    "                score = 0\n",
    "                for adj, value in adjs.items():\n",
    "                    score += self.scores[adj] * value / sum_weights[adj]\n",
    "                self.scores[sent] = (1 - damping_factor)/n +damping_factor * score\n",
    "\n",
    "    def extract_summary(self, n_sents=10, cosine_thres=0.5, max_sent=100):\n",
    "\n",
    "        sentIds = []\n",
    "        sentScores = np.array(self.scores.copy())\n",
    "\n",
    "        print(\"Extracting sentences....\")\n",
    "        # get #max_sent maximal scores along with its indices\n",
    "        print(\"Sent scores: {}\".format(len(sentScores)))\n",
    "\n",
    "        indices = np.argpartition(sentScores, -max_sent)[-max_sent:]\n",
    "        values = sentScores[indices]\n",
    "        max_index_value = {key: value for key, value in zip(indices, values)}\n",
    "        max_index_value = sorted(max_index_value.items(), key=lambda x: (x[1], x[0]))\n",
    "\n",
    "        i = 0\n",
    "        while i < n_sents:\n",
    "            index, value = max_index_value.pop()\n",
    "            if index not in self.graph:\n",
    "                print(\"Sent {} not in graph\".format(index))\n",
    "                continue\n",
    "            assign = 1\n",
    "            # iterate selected sentences\n",
    "            for idx in sentIds:\n",
    "                # if new index is not an ajdacent node of the selected one\n",
    "                if idx not in self.graph[index]:\n",
    "                    continue\n",
    "                similarity = self.graph[index][idx]\n",
    "                if similarity > cosine_thres:\n",
    "                    print(\"Sent {} is similar to a {}: {}\".format(index, idx, similarity))\n",
    "                    assign = 0\n",
    "                    break\n",
    "            if assign == 1:\n",
    "#                 print(i, \", \", 'TweetId: ', self.data.iloc[index]['Id'], \": \", self.data.iloc[index]['Tweet'])\n",
    "                print(\"selected one: {}, {}\".format(index, value))\n",
    "                sentIds.append(index)\n",
    "                i += 1\n",
    "        return sentIds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
