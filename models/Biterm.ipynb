{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, emoji, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, time\n",
    "from biterm.cbtm import oBTM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  RT @MENTION : Emergency Rally Against Trump's ...\n",
      "1  824941519857610752  RT @MENTION : Theresa May has not apologized t...\n",
      "2  824941616314122240  RT @MENTION : Trump's Immigration Ban Excludes...\n",
      "3  824942056741167105  RT @MENTION : Trump's immigration order expand...\n",
      "4  824942966875774976  ALERT : Senator John McCain Threatens Action O...\n",
      "(123385, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/nguyen/data/processed_travel_ban.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  emergency rally against trump's muslim travel ...\n",
      "1  824941519857610752  theresa may has not apologized to trump for in...\n",
      "2  824941616314122240  trump's immigration ban excludes countries wit...\n",
      "3  824942056741167105  trump's immigration order expands the definiti...\n",
      "4  824942966875774976  alert : senator john mccain threatens action o...\n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "# data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(lemmatize_stemming(y) for y in x.split(\" \") if y.strip()!= \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['like', 'https', 'htt', 'get', 'would', 'im', 'know', 'says',\n",
    "                   'want', 'see', 'make', 'need', 'think', 'going', 'please', 'let', 'w',\n",
    "                   '–', 'much', 'many', 'feel', 'go', 'take', 'like', 'hate', 'news', 'rt'])\n",
    "for item in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    stop_words.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, punctuation\n",
    "\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stop_words))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    emergency rally trumps muslim travel ban nyc 1...\n",
       "1    theresa may apologized trump insulting fails t...\n",
       "2    trumps immigration ban excludes countries busi...\n",
       "3    trumps immigration order expands definition cr...\n",
       "4    alert senator john mccain threatens action pre...\n",
       "Name: Tweet1, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48879 48879\n",
      "[('trump', 36613), ('ban', 28016), ('trumps', 20850), ('muslim', 13374), ('people', 13119), ('order', 12963), ('us', 12287), ('refugees', 12088), ('immigration', 11977), ('muslimban', 10531), ('travel', 7473), ('countries', 7232), ('president', 7037), ('executive', 6266), ('donald', 6231), ('america', 5920), ('via', 5788), ('refugee', 5480), ('white', 5016), ('muslims', 4255), ('obama', 3883), ('world', 3851), ('protest', 3677), ('state', 3436), ('new', 3111), ('country', 3004), ('one', 2633), ('visit', 2627), ('immigrants', 2405), ('support', 2394), ('banned', 2354), ('may', 2314), ('house', 2269), ('quebec', 2247), ('americans', 2242), ('acting', 2229), ('uk', 2223), ('mosque', 2138), ('right', 2136), ('american', 2123), ('general', 2117), ('terrorist', 2108), ('iran', 2071), ('judge', 2070), ('breaking', 2056), ('attorney', 1998), ('airport', 1954), ('petition', 1921), ('yates', 1898), ('orders', 1883), ('nobannowall', 1867), ('media', 1803), ('say', 1779), ('im', 1753), ('green', 1739), ('terrorists', 1724), ('syrian', 1694), ('today', 1687), ('stop', 1662), ('first', 1643), ('back', 1634), ('protests', 1621), ('united', 1621), ('detained', 1620), ('stand', 1620), ('ceo', 1617), ('business', 1601), ('vetting', 1529), ('card', 1526), ('christian', 1520), ('federal', 1519), ('sally', 1506), ('saudi', 1505), ('citizens', 1493), ('cant', 1474), ('list', 1437), ('jfk', 1429), ('legal', 1429), ('killed', 1411), ('said', 1407), ('attack', 1404), ('visa', 1404), ('court', 1398), ('holders', 1383), ('policy', 1338), ('canada', 1331), ('uber', 1325), ('religion', 1324), ('good', 1309), ('usa', 1306), ('starbucks', 1304), ('defend', 1302), ('still', 1302), ('time', 1298), ('law', 1282), ('airports', 1258), ('call', 1258), ('isis', 1236), ('also', 1227), ('safe', 1206)]\n"
     ]
    }
   ],
   "source": [
    "# view most frequent words\n",
    "cv = CountVectorizer()  \n",
    "cv_fit = cv.fit_transform(list(data['Tweet1']))\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)   \n",
    "wCount = dict(zip(word_list,count_list))\n",
    "textCount =  sorted(wCount.items(), key=lambda k: -k[1])\n",
    "print(len(word_list), len(textCount))\n",
    "print(textCount[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['len'] = data['Tweet1'].apply(lambda x: len(x.split(\" \")))\n",
    "data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123341, 5)\n"
     ]
    }
   ],
   "source": [
    "# remove tweets #unique words less than haft of length\n",
    "# data['len'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['#uniWord']/data['len']>0.5]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104687, 5)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:  7979\n",
      "data.shape:  (101133, 5)\n",
      "Len:  7976\n",
      "data.shape:  (101104, 5)\n",
      "Len:  7976\n",
      "data.shape:  (101104, 5)\n"
     ]
    }
   ],
   "source": [
    "# remove tweets with lengths < 3\n",
    "cv = CountVectorizer(stop_words='english', min_df = 10, max_df = 0.035) \n",
    "cv_fit = cv.fit(list(data['Tweet1']))\n",
    "vocab = set(cv.get_feature_names())\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y in vocab))\n",
    "while True:\n",
    "    data['len'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "    data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))\n",
    "    data = data[data['len'] >2]\n",
    "    cv = CountVectorizer(stop_words='english', min_df = 10)  \n",
    "    cv.fit(list(data['Tweet1']))\n",
    "    newVocab = set(cv.get_feature_names())\n",
    "    \n",
    "    print(\"Len: \", len(newVocab))\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    if len(vocab) == len(newVocab):\n",
    "        break\n",
    "    data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y in newVocab))\n",
    "    vocab = newVocab.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7976 7976\n",
      "[('muslims', 3843), ('obama', 3238), ('protest', 3180), ('world', 3179), ('country', 2793), ('new', 2515), ('state', 2475), ('support', 2064), ('immigrants', 2051), ('right', 2022), ('americans', 1996), ('banned', 1987), ('american', 1941), ('uk', 1925), ('quebec', 1902), ('terrorist', 1879), ('visit', 1836), ('house', 1812), ('acting', 1793), ('nobannowall', 1782), ('iran', 1729), ('mosque', 1704), ('airport', 1701), ('yates', 1673), ('general', 1653), ('today', 1624), ('say', 1620), ('im', 1618), ('attorney', 1590), ('terrorists', 1589), ('breaking', 1583), ('judge', 1540), ('green', 1535), ('media', 1487), ('orders', 1463), ('stop', 1463), ('stand', 1419), ('saudi', 1386), ('syrian', 1386), ('detained', 1343), ('vetting', 1325), ('citizens', 1321), ('sally', 1315), ('card', 1304), ('killed', 1291), ('jfk', 1270), ('protests', 1270), ('usa', 1270), ('list', 1263), ('petition', 1251), ('business', 1250), ('said', 1242), ('time', 1220), ('attack', 1212), ('ceo', 1204), ('holders', 1186), ('religion', 1184), ('court', 1177), ('federal', 1172), ('law', 1164), ('christian', 1161), ('canada', 1152), ('good', 1133), ('legal', 1130), ('policy', 1113), ('visa', 1112), ('uber', 1086), ('safe', 1063), ('isis', 1055), ('defend', 1047), ('banning', 1039), ('united', 1031), ('great', 1025), ('come', 1024), ('theresa', 1022), ('rights', 1018), ('states', 1003), ('starbucks', 992), ('iraq', 985), ('terrorism', 982), ('help', 975), ('stay', 965), ('christians', 954), ('love', 948), ('arabia', 944), ('sign', 943), ('resist', 936), ('read', 930), ('city', 926), ('fired', 920), ('thats', 918), ('ag', 916), ('airports', 908), ('statement', 906), ('hes', 899), ('terror', 892), ('million', 887), ('visas', 884), ('eo', 883), ('day', 875)]\n"
     ]
    }
   ],
   "source": [
    "# most frequent words after removing short tweets, highly/low frequent words\n",
    "cv = CountVectorizer() \n",
    "cv_fit = cv.fit_transform(list(data['Tweet1']))\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)   \n",
    "wCount = dict(zip(word_list,count_list))\n",
    "textCount =  sorted(wCount.items(), key=lambda k: -k[1])\n",
    "print(len(word_list), len(textCount))\n",
    "print(textCount[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 7976\n",
      "Len(biterms): 101104\n",
      "\n",
      "\n",
      " Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [14:17<00:00, 17.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:0, 857.9769999980927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [10:41<02:59, 16.33s/it]"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    texts = list(data['Tweet1'])\n",
    "\n",
    "    # vectorize texts\n",
    "    vec = CountVectorizer()\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "    print(\"Vocab: {}\".format(len(vocab)))\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(X)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=num_topics, V=vocab)\n",
    "    print(\"Len(biterms):\", len(biterms))\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    start = time.time()\n",
    "    for i in range(0, len(biterms), 2000): # prozess chunk of 200 texts\n",
    "        \n",
    "        biterms_chunk = biterms[i:i + 2000]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "        \n",
    "        if i%2000 ==0:\n",
    "            print(\"....Line:{}, {}\".format(i, (time.time()-start)))\n",
    "            start = time.time()\n",
    "    topics = btm.transform(biterms)\n",
    "\n",
    "#     print(\"\\n\\n Visualize Topics ..\")\n",
    "#     vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))\n",
    "#     pyLDAvis.save_html(vis, 'online_btm.html')\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, X, vocab, 10)\n",
    "\n",
    "#     print(\"\\n\\n Texts & Topics ..\")\n",
    "#     for i in range(len(texts)):\n",
    "#         print(\"{}. {} (topic: {})\".format(i, texts[i], topics[i].argmax()))\n",
    "#     print(topics.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 | Coherence=-194.48 | Top words= quebec judge mosque detained federal court new today stay airport\n",
      "Topic 1 | Coherence=-187.88 | Top words= obama immigrants list banned muslims illegal americans right maga committed\n",
      "Topic 2 | Coherence=-193.51 | Top words= country world american law stop religious nobannowall im proud today\n",
      "Topic 3 | Coherence=-178.03 | Top words= ceo starbucks uber hire google tech 10000 advisory apple council\n",
      "Topic 4 | Coherence=-173.88 | Top words= green saudi card holders iran arabia iraq 911 syria banned\n",
      "Topic 5 | Coherence=-183.16 | Top words= world stand country im americans immigrants good american say media\n",
      "Topic 6 | Coherence=-180.90 | Top words= world support maga american muslims iran time media right obama\n",
      "Topic 7 | Coherence=-181.13 | Top words= state acting yates visit general attorney sally petition uk defend\n",
      "Topic 8 | Coherence=-137.39 | Top words= uk theresa minister pm british prime house press conference visit\n",
      "Topic 9 | Coherence=-196.60 | Top words= poll obama new tears schumer approval media temporary americans rating\n",
      "Topic 10 | Coherence=-185.51 | Top words= obama christians muslims country christian banned 2011 syrian religious american\n",
      "Topic 11 | Coherence=-157.31 | Top words= protest airport nobannowall jfk protests protesters thousands house outside rally\n",
      "Topic 12 | Coherence=-162.50 | Top words= muslims terrorist americans terrorists killed right quebec attack obama american\n",
      "Topic 13 | Coherence=-169.27 | Top words= new vetting orders extreme breaking signs terrorists islamic muslims iran\n",
      "Topic 14 | Coherence=-201.28 | Top words= free house world party tonight penn live syrian 11 state\n",
      "Topic 15 | Coherence=-189.97 | Top words= muslims immigrants support mosque religion war world help country syrian\n",
      "Topic 16 | Coherence=-214.78 | Top words= uber protest support iranian world ceo awards muslims right director\n",
      "Topic 17 | Coherence=-209.27 | Top words= muslims rights states isis united world mccain iran human constitution\n",
      "Topic 18 | Coherence=-165.69 | Top words= muslims obama banned terrorists terrorism terrorist country religion americans killed\n",
      "Topic 19 | Coherence=-230.15 | Top words= quebec mosque wind judge shooting city humidity rain federal mm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence': [-194.47972203810775,\n",
       "  -187.87835510311297,\n",
       "  -193.50702655434074,\n",
       "  -178.02936579862066,\n",
       "  -173.88418514240198,\n",
       "  -183.1572053868775,\n",
       "  -180.90024534308355,\n",
       "  -181.1284999771516,\n",
       "  -137.3860702297489,\n",
       "  -196.5957772744971,\n",
       "  -185.51397900970758,\n",
       "  -157.3085725728535,\n",
       "  -162.50106103283855,\n",
       "  -169.2734176893231,\n",
       "  -201.2784702591572,\n",
       "  -189.97232865891166,\n",
       "  -214.78068470269673,\n",
       "  -209.2743847980392,\n",
       "  -165.69335127384565,\n",
       "  -230.15177437752845],\n",
       " 'top_words': [array(['quebec', 'judge', 'mosque', 'detained', 'federal', 'court', 'new',\n",
       "         'today', 'stay', 'airport'], dtype='<U28'),\n",
       "  array(['obama', 'immigrants', 'list', 'banned', 'muslims', 'illegal',\n",
       "         'americans', 'right', 'maga', 'committed'], dtype='<U28'),\n",
       "  array(['country', 'world', 'american', 'law', 'stop', 'religious',\n",
       "         'nobannowall', 'im', 'proud', 'today'], dtype='<U28'),\n",
       "  array(['ceo', 'starbucks', 'uber', 'hire', 'google', 'tech', '10000',\n",
       "         'advisory', 'apple', 'council'], dtype='<U28'),\n",
       "  array(['green', 'saudi', 'card', 'holders', 'iran', 'arabia', 'iraq',\n",
       "         '911', 'syria', 'banned'], dtype='<U28'),\n",
       "  array(['world', 'stand', 'country', 'im', 'americans', 'immigrants',\n",
       "         'good', 'american', 'say', 'media'], dtype='<U28'),\n",
       "  array(['world', 'support', 'maga', 'american', 'muslims', 'iran', 'time',\n",
       "         'media', 'right', 'obama'], dtype='<U28'),\n",
       "  array(['state', 'acting', 'yates', 'visit', 'general', 'attorney',\n",
       "         'sally', 'petition', 'uk', 'defend'], dtype='<U28'),\n",
       "  array(['uk', 'theresa', 'minister', 'pm', 'british', 'prime', 'house',\n",
       "         'press', 'conference', 'visit'], dtype='<U28'),\n",
       "  array(['poll', 'obama', 'new', 'tears', 'schumer', 'approval', 'media',\n",
       "         'temporary', 'americans', 'rating'], dtype='<U28'),\n",
       "  array(['obama', 'christians', 'muslims', 'country', 'christian', 'banned',\n",
       "         '2011', 'syrian', 'religious', 'american'], dtype='<U28'),\n",
       "  array(['protest', 'airport', 'nobannowall', 'jfk', 'protests',\n",
       "         'protesters', 'thousands', 'house', 'outside', 'rally'],\n",
       "        dtype='<U28'),\n",
       "  array(['muslims', 'terrorist', 'americans', 'terrorists', 'killed',\n",
       "         'right', 'quebec', 'attack', 'obama', 'american'], dtype='<U28'),\n",
       "  array(['new', 'vetting', 'orders', 'extreme', 'breaking', 'signs',\n",
       "         'terrorists', 'islamic', 'muslims', 'iran'], dtype='<U28'),\n",
       "  array(['free', 'house', 'world', 'party', 'tonight', 'penn', 'live',\n",
       "         'syrian', '11', 'state'], dtype='<U28'),\n",
       "  array(['muslims', 'immigrants', 'support', 'mosque', 'religion', 'war',\n",
       "         'world', 'help', 'country', 'syrian'], dtype='<U28'),\n",
       "  array(['uber', 'protest', 'support', 'iranian', 'world', 'ceo', 'awards',\n",
       "         'muslims', 'right', 'director'], dtype='<U28'),\n",
       "  array(['muslims', 'rights', 'states', 'isis', 'united', 'world', 'mccain',\n",
       "         'iran', 'human', 'constitution'], dtype='<U28'),\n",
       "  array(['muslims', 'obama', 'banned', 'terrorists', 'terrorism',\n",
       "         'terrorist', 'country', 'religion', 'americans', 'killed'],\n",
       "        dtype='<U28'),\n",
       "  array(['quebec', 'mosque', 'wind', 'judge', 'shooting', 'city',\n",
       "         'humidity', 'rain', 'federal', 'mm'], dtype='<U28')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summuary(btm.phi_wz.T, X, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7976, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " btm.phi_wz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop= True)\n",
    "data['len'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))\n",
    "data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract instances/tweets that have highest topic-document prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101104, 20)\n",
      "#0. 87155\t 0.9998515193421754 filed restraining filed\n",
      "#1. 26277\t 0.9917291554326706 results prescription online online pharmacy online\n",
      "#2. 3365\t 0.9939585193668687 heartbroken malala yousafzai\n",
      "#3. 41468\t 0.9987054701533128 apple tim cook\n",
      "#4. 43147\t 0.9934031065985757 uae saudia arabia\n",
      "#5. 90325\t 0.9886550666162748 elliptical bike cross trainer exercise fitness machine\n",
      "#6. 5934\t 0.9999027156281527 ausopen venus williams serena williams\n",
      "#7. 71895\t 0.9976800923081895 dana boente yates\n",
      "#8. 8164\t 0.9677912070854435 shinzo abe discusses importance japan alliance\n",
      "#9. 55852\t 0.9969359003546336 approval rating unpopular gallup\n",
      "#10. 1424\t 0.9780894955283705 persecuted christians given priority\n",
      "#11. 25590\t 0.9964889451041298 filling arrivals hall\n",
      "#12. 6976\t 0.9357109170151493 shoot theaters theater shooters coincidence woke\n",
      "#13. 47120\t 0.9997776447959211 malevolence tempered incompetence\n",
      "#14. 56208\t 0.9994641209076541 kal penn insulted\n",
      "#15. 66787\t 0.910216629282378 villains ashamed millions\n",
      "#16. 10161\t 0.999993168239402 asghar farhadi attending\n",
      "#17. 18132\t 0.9999299548339307 bitfinex btce gdax bitstamp bitcoin\n",
      "#18. 2316\t 0.9758256040539806 pardon sailor photos\n",
      "#19. 100037\t 0.8697375099625142 gmt temperature 32 wind mph ave mph gust humidity 87 rain hourly 00 mm pressure 986 hpa rising\n"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(topics)\n",
    "print(x.shape)\n",
    "x.shape\n",
    "for i in range(num_topics):\n",
    "    print(\"#{}. {}\\t {} {}\".format(i, x[i].idxmax(),  x[i].max(), data.iloc[x[i].idxmax()]['Tweet1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([     0,      1,      4,      6,      7,     10,     12,     15,\n",
      "                16,     17,\n",
      "            ...\n",
      "            101093, 101094, 101095, 101096, 101097, 101099, 101100, 101101,\n",
      "            101102, 101103],\n",
      "           dtype='int64', length=83624)\n",
      "Int64Index([     0,      1,      4,      6,      7,     10,     12,     15,\n",
      "                16,     17,\n",
      "            ...\n",
      "            101093, 101094, 101095, 101096, 101097, 101099, 101100, 101101,\n",
      "            101102, 101103],\n",
      "           dtype='int64', length=83624)\n"
     ]
    }
   ],
   "source": [
    "# extract only instance with > 5 uniWords\n",
    "data1 = data[data['#uniWord']>4].copy()\n",
    "print(data1.index)\n",
    "x = pd.DataFrame(topics)\n",
    "x = x.iloc[data1.index]\n",
    "print(x.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.reset_index(drop=True)\n",
    "x = x.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0. 25579\t 0.9248268232756537 motion darweesh judge donnellys tro\n",
      "#1. 1481\t 0.9507302032856352 publishing crimes committed illegals democrats converting\n",
      "#2. 1973\t 0.9726844119312122 nobel peace winner malala heartbroken\n",
      "#3. 12864\t 0.9974272535683989 apple ceo cook apple exist recode\n",
      "#4. 61653\t 0.9820632675108047 sudan somalia libya banned traveling\n",
      "#5. 74555\t 0.9886550666162748 elliptical bike cross trainer exercise fitness machine\n",
      "#6. 4361\t 0.9947723003797263 venus serena williams tennis world\n",
      "#7. 63226\t 0.9941030929454294 fires acting attorney general betrayed\n",
      "#8. 6708\t 0.9677912070854435 shinzo abe discusses importance japan alliance\n",
      "#9. 83241\t 0.9864087866838271 poll job approval rating lags\n",
      "#10. 40694\t 0.9312212606196182 nowplaying listeners web media np\n",
      "#11. 31536\t 0.9876325257478071 thousands gathered bostons copley square protest\n",
      "#12. 5743\t 0.9357109170151493 shoot theaters theater shooters coincidence woke\n",
      "#13. 24212\t 0.938962215869479 memorandum boost military readiness rt\n",
      "#14. 59805\t 0.9873292186490573 actor kal penn feels insulted\n",
      "#15. 1563\t 0.8443714254727558 rep seth moulton iraq war veteran pulls ashamed\n",
      "#16. 5724\t 0.9988539635966059 asghar farhadi attend oscars film\n",
      "#17. 14904\t 0.9999299548339307 bitfinex btce gdax bitstamp bitcoin\n",
      "#18. 143\t 0.927059063020219 days false claims inaccurate statements exaggerations wapo\n",
      "#19. 82728\t 0.8697375099625142 gmt temperature 32 wind mph ave mph gust humidity 87 rain hourly 00 mm pressure 986 hpa rising\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_topics):\n",
    "    print(\"#{}. {}\\t {} {}\".format(i, x[i].idxmax(),  x[i].max(), data1.iloc[x[i].idxmax()]['Tweet1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract instances/tweets that maximize P(d|z)\n",
    "   *   $P(d|z) = \\ln{(len(d))}*\\prod_{b_i \\in d}P(b_i|z)$ # give more weight to long sentences\n",
    "   \n",
    "   *   $d = argmax_dP(d|z) = argmax_d \\ln{(P(d|z))}$\n",
    "   *   $d = argmax_d\\ln{(\\ln{(len(d)))}} + \\sum_{b_i}\\ln{P(b_i|z)}$\n",
    "   *   $d = argmax_d\\ln{(\\ln{(len(d)))}} + \\sum_{b_i, w_{i0, i1} \\in b_i}\\ln{(P(w_{i0}|z)*P(w_{i1}|z)))}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data[data['#uniWord'] >4].copy()\n",
    "remained_index = data1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 =data1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     0,      1,      4,      6,      7,     10,     12,     15,\n",
       "                16,     17,\n",
       "            ...\n",
       "            101093, 101094, 101095, 101096, 101097, 101099, 101100, 101101,\n",
       "            101102, 101103],\n",
       "           dtype='int64', length=83624)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remained_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "biterms1 = np.array(biterms).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "biterms1 = biterms1[remained_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83624, 20)\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "P_dz = np.zeros([len(biterms1), num_topics])\n",
    "# iterate documents\n",
    "for i, d in enumerate(biterms1):\n",
    "    n_biterms = len(d)\n",
    "    P_bz = np.zeros([len(d), num_topics])\n",
    "    for j, b in enumerate(d):\n",
    "        P_bz[j] = np.log(btm.phi_wz[b[0], :] * btm.phi_wz[b[1], :])\n",
    "    P_dz[i] = P_bz.sum(axis = 0)\n",
    "\n",
    "print(P_dz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract most representative sentences for each topic\n",
    "indices = P_dz.argmax(axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 39638\tseveral people killed in shooting at quebec city mosque\n",
      "1. 65407\tobama did enforce the list and he deported more immigrants than any president so …\n",
      "2. 5221\t#ireland just became the world's first country to stop investing in #fossilfuels\n",
      "3. 77954\tuber ceo quits trump advisory council\n",
      "4. 8563\tthese are us citizens . not green card holders not people on work/student/travel visas . us citizens . …\n",
      "5. 50980\teverything is not always about americans . i love my country but there is a whole big world out there .\n",
      "6. 25972\tworld : we want you to win this #ausopen federer : roger that .\n",
      "7. 54419\tacting attorney general sally yates to trump's #muslimban\n",
      "8. 640\twatch : president donald trump and prime minister of uk theresa may insid ... via\n",
      "9. 80696\tnew poll : majority of americans disapprove of trump's immigration ban\n",
      "10. 17391\tobama banned all iraqi refugees for 6 months in 2011 .\n",
      "11. 17760\tthousands at #newyork’s jfk airport protest visa and refugee suspensions\n",
      "12. 27232\tdoes trump know that being muslim is a religion , not a race ? they're are americans that are muslims , you going to ban them too ?\n",
      "13. 1492\t: trump says new vetting will keep ‘ radical islamic terrorists ’ out of u.s.\n",
      "14. 39114\tairbnb offers free housing to immigrants and refugees after trump's ban\n",
      "15. 8570\ti am also ashamed of this once great country and will never support this ban on immigrants .\n",
      "16. 11049\tiranian director can't attend oscars due to trump's visa ban\n",
      "17. 8822\tunited states has always been about freedom and human rights . don't let that be trumped . #muslimban\n",
      "18. 62805\twhat terrorists from any of the countries on list have killed americans ?? none . trumps business pals have\n",
      "19. 39638\tseveral people killed in shooting at quebec city mosque\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(indices):\n",
    "    print(\"{}. {}\\t{}\".format(i, idx, data1.iloc[idx]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet1</th>\n",
       "      <th>len</th>\n",
       "      <th>#uniWord</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>824941360449015808</td>\n",
       "      <td>emergency rally against trump's muslim travel ...</td>\n",
       "      <td>emergency rally nyc 125 pm</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>824941519857610752</td>\n",
       "      <td>theresa may has not apologized to trump for in...</td>\n",
       "      <td>theresa insulting fails today send</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>824941616314122240</td>\n",
       "      <td>trump's immigration ban excludes countries wit...</td>\n",
       "      <td>excludes business ties</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>824942056741167105</td>\n",
       "      <td>trump's immigration order expands the definiti...</td>\n",
       "      <td>expands definition criminal</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>824942966875774976</td>\n",
       "      <td>alert : senator john mccain threatens action o...</td>\n",
       "      <td>alert senator john mccain threatens action</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101099</th>\n",
       "      <td>827675291698880512</td>\n",
       "      <td>well done washington attorney general . federa...</td>\n",
       "      <td>washington attorney general federal judge seat...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101100</th>\n",
       "      <td>827675363006414848</td>\n",
       "      <td>yesterday , i rted a clip about syrian refugee...</td>\n",
       "      <td>yesterday clip syrian working german hospitals...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101101</th>\n",
       "      <td>827675396577521666</td>\n",
       "      <td>judge in seattle blocks trump travel ban plan ...</td>\n",
       "      <td>judge seattle blocks plan resist</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101102</th>\n",
       "      <td>827675522385612800</td>\n",
       "      <td>trump responds to louvre attack in paris , urg...</td>\n",
       "      <td>responds louvre attack paris urging smart new ...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101103</th>\n",
       "      <td>827675534951849984</td>\n",
       "      <td>muslim says killing gays &amp; abusing women are n...</td>\n",
       "      <td>killing gays abusing women normal islamic beli...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101104 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Id                                              Tweet  \\\n",
       "0       824941360449015808  emergency rally against trump's muslim travel ...   \n",
       "1       824941519857610752  theresa may has not apologized to trump for in...   \n",
       "2       824941616314122240  trump's immigration ban excludes countries wit...   \n",
       "3       824942056741167105  trump's immigration order expands the definiti...   \n",
       "4       824942966875774976  alert : senator john mccain threatens action o...   \n",
       "...                    ...                                                ...   \n",
       "101099  827675291698880512  well done washington attorney general . federa...   \n",
       "101100  827675363006414848  yesterday , i rted a clip about syrian refugee...   \n",
       "101101  827675396577521666  judge in seattle blocks trump travel ban plan ...   \n",
       "101102  827675522385612800  trump responds to louvre attack in paris , urg...   \n",
       "101103  827675534951849984  muslim says killing gays & abusing women are n...   \n",
       "\n",
       "                                                   Tweet1  len  #uniWord  \n",
       "0                              emergency rally nyc 125 pm    5         5  \n",
       "1                      theresa insulting fails today send    5         5  \n",
       "2                                  excludes business ties    3         3  \n",
       "3                             expands definition criminal    3         3  \n",
       "4              alert senator john mccain threatens action    6         6  \n",
       "...                                                   ...  ...       ...  \n",
       "101099  washington attorney general federal judge seat...    9         9  \n",
       "101100  yesterday clip syrian working german hospitals...    8         8  \n",
       "101101                   judge seattle blocks plan resist    5         5  \n",
       "101102  responds louvre attack paris urging smart new ...    9         9  \n",
       "101103  killing gays abusing women normal islamic beli...    8         8  \n",
       "\n",
       "[101104 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Tweet'].str.contains('')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('biterm.pkl', 'wb') as f:\n",
    "    pickle.dump(btm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
