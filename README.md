# Tweet summarization
1. Biterm model
2. LDA
3. Kmean with BertEmbeddings and BertSentenceEmbeddings
4. LexRank + LSH with tfidf, BertSentenceEmbedidngs
5. LexRank + BertScore



# Related work
1. Attention Is All You Need, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin, NIPS, 2017
2. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL, 2019
3. Text Summarization with Pretrained Encoders (BERTSUM), Yang Liu, Mirella Lapata, EMNLP-IJCNLP, 2019

___________________
4. Unified Language Model Pre-training for Natural Language Understanding and Generation, Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, NIPS, 2019
5. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers, Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, ICML, 2020
___________________
