{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, emoji, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, time\n",
    "from biterm.cbtm import oBTM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  RT @MENTION : Emergency Rally Against Trump's ...\n",
      "1  824941519857610752  RT @MENTION : Theresa May has not apologized t...\n",
      "2  824941616314122240  RT @MENTION : Trump's Immigration Ban Excludes...\n",
      "3  824942056741167105  RT @MENTION : Trump's immigration order expand...\n",
      "4  824942966875774976  ALERT : Senator John McCain Threatens Action O...\n",
      "(123385, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/nguyen/data/processed_travel_ban.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Id                                              Tweet\n",
      "0  824941360449015808  emergency rally against trump's muslim travel ...\n",
      "1  824941519857610752  theresa may has not apologized to trump for in...\n",
      "2  824941616314122240  trump's immigration ban excludes countries wit...\n",
      "3  824942056741167105  trump's immigration order expands the definiti...\n",
      "4  824942966875774976  alert : senator john mccain threatens action o...\n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return WordNetLemmatizer().lemmatize(text, pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(lemmatize_stemming(y) for y in x.split(\" \") if y.strip()!= \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['like', 'https', 'htt', 'get', 'would', 'im', 'know', 'says',\n",
    "                   'want', 'see', 'make', 'need', 'think', 'going', 'please', 'let', 'w',\n",
    "                   '–', 'much', 'many', 'feel', 'go', 'take', 'like', 'hate', 'news', 'rt'])\n",
    "for item in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    stop_words.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, punctuation\n",
    "\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stop_words))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48886 48886\n",
      "[('trump', 36613), ('ban', 28016), ('trumps', 20850), ('muslim', 13374), ('people', 13119), ('order', 12963), ('us', 12287), ('refugees', 12088), ('immigration', 11977), ('muslimban', 10531), ('travel', 7473), ('countries', 7232), ('president', 7037), ('executive', 6266), ('donald', 6231), ('america', 5920), ('via', 5788), ('refugee', 5480), ('white', 5016), ('muslims', 4255), ('obama', 3883), ('world', 3851), ('see', 3819), ('protest', 3677), ('news', 3554), ('says', 3471), ('state', 3436), ('new', 3111), ('country', 3004), ('like', 2913), ('one', 2633), ('visit', 2627), ('immigrants', 2405), ('support', 2394), ('banned', 2354), ('may', 2314), ('house', 2269), ('quebec', 2247), ('americans', 2242), ('acting', 2229), ('uk', 2223), ('https', 2187), ('get', 2178), ('mosque', 2138), ('right', 2136), ('would', 2131), ('american', 2123), ('general', 2117), ('terrorist', 2108), ('iran', 2071), ('judge', 2070), ('breaking', 2056), ('attorney', 1998), ('airport', 1954), ('petition', 1921), ('im', 1919), ('yates', 1898), ('orders', 1883), ('nobannowall', 1867), ('media', 1803), ('say', 1779), ('make', 1771), ('want', 1766), ('know', 1754), ('green', 1739), ('terrorists', 1724), ('syrian', 1694), ('today', 1687), ('many', 1676), ('stop', 1662), ('first', 1643), ('back', 1634), ('protests', 1621), ('united', 1621), ('detained', 1620), ('stand', 1620), ('ceo', 1617), ('business', 1601), ('need', 1545), ('vetting', 1529), ('card', 1526), ('christian', 1520), ('federal', 1519), ('sally', 1506), ('saudi', 1505), ('citizens', 1493), ('think', 1480), ('cant', 1474), ('go', 1459), ('take', 1448), ('list', 1437), ('jfk', 1429), ('legal', 1429), ('killed', 1411), ('said', 1407), ('attack', 1404), ('visa', 1404), ('court', 1398), ('holders', 1383), ('going', 1341)]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()  \n",
    "cv_fit = cv.fit_transform(list(data['Tweet1']))\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)   \n",
    "wCount = dict(zip(word_list,count_list))\n",
    "textCount =  sorted(wCount.items(), key=lambda k: -k[1])\n",
    "print(len(word_list), len(textCount))\n",
    "print(textCount[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['len'] = data['Tweet1'].apply(lambda x: len(x.split(\" \")))\n",
    "data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 5)\n"
     ]
    }
   ],
   "source": [
    "# remove tweets #unique words less than haft of length\n",
    "# data['len'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['#uniWord']/data['len']>0.5]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104690, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:  7976\n",
      "data.shape:  (101104, 4)\n",
      "Len:  7973\n",
      "data.shape:  (101076, 4)\n",
      "Len:  7973\n",
      "data.shape:  (101076, 4)\n"
     ]
    }
   ],
   "source": [
    "# remove tweets with lengths < 3\n",
    "cv = CountVectorizer(stop_words='english', min_df = 10, max_df = 0.035) \n",
    "cv_fit = cv.fit(list(data['Tweet1']))\n",
    "vocab = set(cv.get_feature_names())\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y in vocab))\n",
    "while True:\n",
    "    data['len'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "    data = data[data['len'] >2]\n",
    "    cv = CountVectorizer(stop_words='english', min_df = 10)  \n",
    "    cv.fit(list(data['Tweet1']))\n",
    "    newVocab = set(cv.get_feature_names())\n",
    "    \n",
    "    print(\"Len: \", len(newVocab))\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    if len(vocab) == len(newVocab):\n",
    "        break\n",
    "    data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y in newVocab))\n",
    "    vocab = newVocab.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7973 7973\n",
      "[('muslims', 3849), ('obama', 3239), ('protest', 3180), ('world', 3180), ('country', 2794), ('new', 2510), ('state', 2479), ('support', 2064), ('immigrants', 2049), ('right', 2025), ('americans', 2000), ('banned', 1990), ('american', 1943), ('uk', 1924), ('quebec', 1903), ('terrorist', 1879), ('visit', 1838), ('house', 1811), ('acting', 1795), ('nobannowall', 1779), ('iran', 1732), ('mosque', 1707), ('airport', 1697), ('yates', 1674), ('general', 1652), ('today', 1628), ('say', 1620), ('attorney', 1589), ('terrorists', 1589), ('breaking', 1585), ('judge', 1542), ('green', 1532), ('media', 1489), ('stop', 1462), ('orders', 1460), ('stand', 1419), ('saudi', 1386), ('syrian', 1385), ('detained', 1340), ('vetting', 1325), ('citizens', 1320), ('sally', 1315), ('card', 1300), ('killed', 1295), ('usa', 1270), ('protests', 1269), ('jfk', 1267), ('list', 1261), ('petition', 1256), ('business', 1249), ('said', 1241), ('time', 1220), ('attack', 1211), ('ceo', 1201), ('holders', 1184), ('religion', 1183), ('court', 1175), ('federal', 1175), ('law', 1164), ('christian', 1161), ('canada', 1151), ('good', 1135), ('legal', 1128), ('policy', 1115), ('visa', 1113), ('uber', 1082), ('safe', 1064), ('isis', 1056), ('defend', 1049), ('banning', 1037), ('united', 1032), ('come', 1025), ('great', 1025), ('theresa', 1022), ('rights', 1018), ('states', 1005), ('starbucks', 993), ('iraq', 986), ('terrorism', 983), ('help', 975), ('stay', 964), ('christians', 954), ('love', 947), ('arabia', 945), ('sign', 945), ('resist', 935), ('read', 930), ('city', 927), ('fired', 920), ('thats', 918), ('ag', 917), ('airports', 909), ('statement', 905), ('hes', 899), ('terror', 893), ('million', 888), ('eo', 882), ('visas', 881), ('day', 876), ('security', 868)]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer() \n",
    "cv_fit = cv.fit_transform(list(data['Tweet1']))\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)   \n",
    "wCount = dict(zip(word_list,count_list))\n",
    "textCount =  sorted(wCount.items(), key=lambda k: -k[1])\n",
    "print(len(word_list), len(textCount))\n",
    "print(textCount[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 7973\n",
      "Len(biterms): 101076\n",
      "\n",
      "\n",
      " Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:40<00:00, 16.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:0, 161.35531544685364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:36<00:00, 15.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:2000, 157.07841205596924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:43<00:00, 16.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:4000, 164.43683385849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:37<00:00, 15.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:6000, 158.04039192199707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:40<00:00, 16.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:8000, 161.1889991760254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:35<00:00, 15.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:10000, 156.5211992263794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:39<00:00, 15.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:12000, 160.46943521499634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:37<00:00, 15.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:14000, 158.08185601234436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    texts = list(data['Tweet1'])\n",
    "\n",
    "    # vectorize texts\n",
    "    vec = CountVectorizer()\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "    print(\"Vocab: {}\".format(len(vocab)))\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(X)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=num_topics, V=vocab)\n",
    "    print(\"Len(biterms):\", len(biterms))\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    start = time.time()\n",
    "    for i in range(0, len(biterms), 2000): # prozess chunk of 200 texts\n",
    "        \n",
    "        biterms_chunk = biterms[i:i + 2000]\n",
    "        btm.fit(biterms_chunk, iterations=10)\n",
    "        \n",
    "        if i%2000 ==0:\n",
    "            print(\"....Line:{}, {}\".format(i, (time.time()-start)))\n",
    "            start = time.time()\n",
    "    topics = btm.transform(biterms)\n",
    "\n",
    "#     print(\"\\n\\n Visualize Topics ..\")\n",
    "#     vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))\n",
    "#     pyLDAvis.save_html(vis, 'online_btm.html')\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, X, vocab, 10)\n",
    "\n",
    "#     print(\"\\n\\n Texts & Topics ..\")\n",
    "#     for i in range(len(texts)):\n",
    "#         print(\"{}. {} (topic: {})\".format(i, texts[i], topics[i].argmax()))\n",
    "#     print(topics.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 | Coherence=-176.08 | Top words= quebec yates acting general mosque sally attorney judge house breaking\n",
      "Topic 1 | Coherence=-190.19 | Top words= muslims world mosque obama country right americans attack said come\n",
      "Topic 2 | Coherence=-197.96 | Top words= american stand world muslims right obama say business religious freedom\n",
      "Topic 3 | Coherence=-174.08 | Top words= green card holders obama visa visas muslims security legal dhs\n",
      "Topic 4 | Coherence=-193.99 | Top words= ceo tech country apple policy support google americans saudi employees\n",
      "Topic 5 | Coherence=-183.73 | Top words= world free right muslims rights country law media american human\n",
      "Topic 6 | Coherence=-141.32 | Top words= saudi iran arabia list iraq syria 911 banned yemen libya\n",
      "Topic 7 | Coherence=-188.38 | Top words= country world americans obama syrian support canada american terrorist say\n",
      "Topic 8 | Coherence=-204.41 | Top words= world safe chaos support airport rights immigrants ceo muslims war\n",
      "Topic 9 | Coherence=-180.98 | Top words= muslims new country terrorists vetting christians islamic christian radical stop\n",
      "Topic 10 | Coherence=-152.72 | Top words= protest airport jfk nobannowall protests protesters thousands outside today terminal\n",
      "Topic 11 | Coherence=-176.84 | Top words= world muslims nobannowall support americans country man right stand immigrants\n",
      "Topic 12 | Coherence=-196.58 | Top words= new iran breaking obama iraqi green orders detained card poll\n",
      "Topic 13 | Coherence=-202.00 | Top words= iranian maga statement awards director obama visa great tcot muslims\n",
      "Topic 14 | Coherence=-176.49 | Top words= terrorist muslims safe terror attack killed syrian right attacks years\n",
      "Topic 15 | Coherence=-114.95 | Top words= state visit petition uk million sign making prevent signatures queen\n",
      "Topic 16 | Coherence=-233.69 | Top words= starbucks obama ceo hire uber 10000 vetting terrorists advisory 2011\n",
      "Topic 17 | Coherence=-169.28 | Top words= muslims terrorist american americans religion country killed obama banned good\n",
      "Topic 18 | Coherence=-220.97 | Top words= world schumer state tears chuck new stand media support mass\n",
      "Topic 19 | Coherence=-177.03 | Top words= acting attorney general yates quebec sally mosque judge defend fires\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence': [-176.08016471423312,\n",
       "  -190.1937594555519,\n",
       "  -197.9640699729928,\n",
       "  -174.07745424971364,\n",
       "  -193.98989017688382,\n",
       "  -183.72932518254785,\n",
       "  -141.32139861827272,\n",
       "  -188.37986070483106,\n",
       "  -204.41298364504763,\n",
       "  -180.98183711881407,\n",
       "  -152.7192894991019,\n",
       "  -176.8443527583473,\n",
       "  -196.57747297389776,\n",
       "  -201.99922037965052,\n",
       "  -176.49111915891885,\n",
       "  -114.95383430765486,\n",
       "  -233.69011665121653,\n",
       "  -169.28420384175527,\n",
       "  -220.96502459575262,\n",
       "  -177.02743426369048],\n",
       " 'top_words': [array(['quebec', 'yates', 'acting', 'general', 'mosque', 'sally',\n",
       "         'attorney', 'judge', 'house', 'breaking'], dtype='<U28'),\n",
       "  array(['muslims', 'world', 'mosque', 'obama', 'country', 'right',\n",
       "         'americans', 'attack', 'said', 'come'], dtype='<U28'),\n",
       "  array(['american', 'stand', 'world', 'muslims', 'right', 'obama', 'say',\n",
       "         'business', 'religious', 'freedom'], dtype='<U28'),\n",
       "  array(['green', 'card', 'holders', 'obama', 'visa', 'visas', 'muslims',\n",
       "         'security', 'legal', 'dhs'], dtype='<U28'),\n",
       "  array(['ceo', 'tech', 'country', 'apple', 'policy', 'support', 'google',\n",
       "         'americans', 'saudi', 'employees'], dtype='<U28'),\n",
       "  array(['world', 'free', 'right', 'muslims', 'rights', 'country', 'law',\n",
       "         'media', 'american', 'human'], dtype='<U28'),\n",
       "  array(['saudi', 'iran', 'arabia', 'list', 'iraq', 'syria', '911',\n",
       "         'banned', 'yemen', 'libya'], dtype='<U28'),\n",
       "  array(['country', 'world', 'americans', 'obama', 'syrian', 'support',\n",
       "         'canada', 'american', 'terrorist', 'say'], dtype='<U28'),\n",
       "  array(['world', 'safe', 'chaos', 'support', 'airport', 'rights',\n",
       "         'immigrants', 'ceo', 'muslims', 'war'], dtype='<U28'),\n",
       "  array(['muslims', 'new', 'country', 'terrorists', 'vetting', 'christians',\n",
       "         'islamic', 'christian', 'radical', 'stop'], dtype='<U28'),\n",
       "  array(['protest', 'airport', 'jfk', 'nobannowall', 'protests',\n",
       "         'protesters', 'thousands', 'outside', 'today', 'terminal'],\n",
       "        dtype='<U28'),\n",
       "  array(['world', 'muslims', 'nobannowall', 'support', 'americans',\n",
       "         'country', 'man', 'right', 'stand', 'immigrants'], dtype='<U28'),\n",
       "  array(['new', 'iran', 'breaking', 'obama', 'iraqi', 'green', 'orders',\n",
       "         'detained', 'card', 'poll'], dtype='<U28'),\n",
       "  array(['iranian', 'maga', 'statement', 'awards', 'director', 'obama',\n",
       "         'visa', 'great', 'tcot', 'muslims'], dtype='<U28'),\n",
       "  array(['terrorist', 'muslims', 'safe', 'terror', 'attack', 'killed',\n",
       "         'syrian', 'right', 'attacks', 'years'], dtype='<U28'),\n",
       "  array(['state', 'visit', 'petition', 'uk', 'million', 'sign', 'making',\n",
       "         'prevent', 'signatures', 'queen'], dtype='<U28'),\n",
       "  array(['starbucks', 'obama', 'ceo', 'hire', 'uber', '10000', 'vetting',\n",
       "         'terrorists', 'advisory', '2011'], dtype='<U28'),\n",
       "  array(['muslims', 'terrorist', 'american', 'americans', 'religion',\n",
       "         'country', 'killed', 'obama', 'banned', 'good'], dtype='<U28'),\n",
       "  array(['world', 'schumer', 'state', 'tears', 'chuck', 'new', 'stand',\n",
       "         'media', 'support', 'mass'], dtype='<U28'),\n",
       "  array(['acting', 'attorney', 'general', 'yates', 'quebec', 'sally',\n",
       "         'mosque', 'judge', 'defend', 'fires'], dtype='<U28')]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summuary(btm.phi_wz.T, X, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7973, 20)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " btm.phi_wz.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract instances/tweets that have highest topic-document prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101076, 20)\n",
      "#0. 18843\t 0.9979276631701551 101 thx jnj\n",
      "#1. 18122\t 0.9999057484680026 bitfinex btce gdax bitstamp bitcoin\n",
      "#2. 54580\t 0.9842965979863711 explained geneva conventions\n",
      "#3. 9569\t 0.9992765005704639 earned pin pins\n",
      "#4. 18831\t 0.9927730948432226 apple exist tim cook\n",
      "#5. 2990\t 0.9925449103022643 tired poor huddled masses yearning breathe free\n",
      "#6. 48818\t 0.967278832793375 19 saudi arabia saudi arabia\n",
      "#7. 40708\t 0.8959245126108748 kal penn raises counting\n",
      "#8. 7505\t 0.9458090158492217 causes chaos panic anger worldwide\n",
      "#9. 25999\t 0.979730228046799 status saudis status\n",
      "#10. 10747\t 0.9789923712458503 gather terminal4 arrivals jfk nomuslimbanjfk\n",
      "#11. 36510\t 0.99155574191433 fucked niteflirt 1800toflirt ext\n",
      "#12. 100675\t 0.957457306404343 lowest approval rating 44\n",
      "#13. 10153\t 0.9954020869385828 asghar farhadi attending\n",
      "#14. 39875\t 0.9231368907491159 races religions ethnicities\n",
      "#15. 38582\t 0.9878202952438125 queen knight alien\n",
      "#16. 57098\t 0.9791957838194373 starbucks hire 10000 supportstarbucks\n",
      "#17. 90294\t 0.99331625423376 elliptical bike cross trainer exercise fitness machine\n",
      "#18. 3656\t 0.9982008076599239 venus williams grand rivalry\n",
      "#19. 100012\t 0.7785950962987573 gmt temperature 32 wind mph ave mph gust humidity 87 rain hourly 00 mm pressure 986 hpa rising\n"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(topics)\n",
    "print(x.shape)\n",
    "x.shape\n",
    "\n",
    "for i in range(num_topics):\n",
    "    print(\"#{}. {}\\t {} {}\".format(i, x[i].idxmax(),  x[i].max(), data.iloc[x[i].idxmax()]['Tweet1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract instances/tweets that maximize P(d|z)\n",
    "   *   $P(d|z) = \\ln{(len(d))}*\\prod_{b_i \\in d}P(b_i|z)$ # give more weight to long sentences\n",
    "   \n",
    "   *   $d = argmax_dP(d|z) = argmax_d \\ln{(P(d|z))}$\n",
    "   *   $d = argmax_d\\ln{(\\ln{(len(d)))}} + \\sum_{b_i}\\ln{P(b_i|z)}$\n",
    "   *   $d = argmax_d\\ln{(\\ln{(len(d)))}} + \\sum_{b_i, w_{i0, i1} \\in b_i}\\ln{(P(w_{i0}|z)*P(w_{i1}|z)))}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy()\n",
    "remained_index = data1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['uniW'] = data1['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tweet1</th>\n",
       "      <th>len</th>\n",
       "      <th>uniW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>824941360449015808</td>\n",
       "      <td>emergency rally against trump's muslim travel ...</td>\n",
       "      <td>emergency rally nyc 125 pm</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>824941519857610752</td>\n",
       "      <td>theresa may has not apologized to trump for in...</td>\n",
       "      <td>theresa insulting fails today send</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>824942966875774976</td>\n",
       "      <td>alert : senator john mccain threatens action o...</td>\n",
       "      <td>alert senator john mccain threatens action</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>824944363587395584</td>\n",
       "      <td>ty for bailing on gmb &amp; today . piers morgan d...</td>\n",
       "      <td>ty today piers morgan aid vocal</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>824944376182927360</td>\n",
       "      <td>#trump to sign eo temporary ban suspending vi...</td>\n",
       "      <td>sign eo temporary suspending visas syria afric...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Id                                              Tweet  \\\n",
       "0  824941360449015808  emergency rally against trump's muslim travel ...   \n",
       "1  824941519857610752  theresa may has not apologized to trump for in...   \n",
       "4  824942966875774976  alert : senator john mccain threatens action o...   \n",
       "6  824944363587395584  ty for bailing on gmb & today . piers morgan d...   \n",
       "7  824944376182927360   #trump to sign eo temporary ban suspending vi...   \n",
       "\n",
       "                                              Tweet1  len  uniW  \n",
       "0                         emergency rally nyc 125 pm    5     5  \n",
       "1                 theresa insulting fails today send    5     5  \n",
       "4         alert senator john mccain threatens action    6     6  \n",
       "6                    ty today piers morgan aid vocal    6     6  \n",
       "7  sign eo temporary suspending visas syria afric...    8     8  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[data1['uniW']>4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1[data1['uniW'] >4]\n",
    "remained_index = data1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([     0,      1,      4,      6,      7,     10,     12,     15,\n",
       "                16,     17,\n",
       "            ...\n",
       "            101065, 101066, 101067, 101068, 101069, 101071, 101072, 101073,\n",
       "            101074, 101075],\n",
       "           dtype='int64', length=83513)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remained_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "biterms1 = np.array(biterms).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "biterms1 = biterms1[remained_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83513, 20)\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "P_dz = np.zeros([len(biterms1), num_topics])\n",
    "# iterate documents\n",
    "for i, d in enumerate(biterms1):\n",
    "    n_biterms = len(d)\n",
    "    P_bz = np.zeros([len(d), num_topics])\n",
    "    for j, b in enumerate(d):\n",
    "        P_bz[j] = np.log(btm.phi_wz[b[0], :] * btm.phi_wz[b[1], :])\n",
    "    P_dz[i] = P_bz.sum(axis = 0)\n",
    "\n",
    "print(P_dz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract most representative sentences for each topic\n",
    "indices = P_dz.argmax(axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 54342\tacting attorney general sally yates to trump's #muslimban \n",
      "1. 72995\t#canadian_terrorist what you think about the person who has been killing six muslims in a mosque in canada #occidental_terrorist\n",
      "2. 74138\ti stand against the persecution of muslims . this is not only against our values as muslims , but also against the american values . #muslimban\n",
      "3. 15657\ttrump’s immigration order strands green card holders who were outside country via mia …\n",
      "4. 23354\tbreaking tim cook : \" apple does not support trumps ban on refugees ! \"\n",
      "5. 16864\ttrue leader of the free world . #welcometocanada \n",
      "6. 79841\t yet trump hasn't banned saudi arabia , who did 9/11 as he does business with them .\n",
      "7. 79561\tnot one person from 7 countries banned ever came here and killed an american . but from the countries where psychotr … \n",
      "8. 29249\t' refugees are welcome here ' : protesters rally at jfk airport \n",
      "9. 1497\t : trump says new vetting will keep ‘ radical islamic terrorists ’ out of u.s. \n",
      "10. 15508\tprotest a #chicago o'hare airport #muslimban #nobannowall \n",
      "11. 13949\twhat do we do when muslims are under attack ? we stand up ! we fight back ! #resist #muslimban \n",
      "12. 57358\t green card holders and legal citizens do .\n",
      "13. 11030\tiranian director can't attend oscars due to trump's visa ban \n",
      "14. 16921\tnot all muslims are terrorists . islam is a religion about peace  #muslimban \n",
      "15. 43255\t petition to ban trump's state visit to the uk over 1 million \n",
      "16. 77848\tuber ceo quits trump advisory council \n",
      "17. 79561\tnot one person from 7 countries banned ever came here and killed an american . but from the countries where psychotr … \n",
      "18. 45944\ttrump accuses schumer of crying \" fake tears \" \n",
      "19. 54342\tacting attorney general sally yates to trump's #muslimban \n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(indices):\n",
    "    print(\"{}. {}\\t{}\".format(i, idx, data1.iloc[idx]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
