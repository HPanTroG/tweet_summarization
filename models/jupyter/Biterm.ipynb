{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nlp_path = os.path.abspath('../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "from utils import tokenizeRawTweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, emoji, string\n",
    "from nltk.corpus import stopwords\n",
    "import nltk, time\n",
    "from biterm.cbtm import oBTM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweet id                                              tweet  \\\n",
      "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
      "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
      "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
      "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
      "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
      "\n",
      "       label  \n",
      "0  off-topic  \n",
      "1   on-topic  \n",
      "2  off-topic  \n",
      "3  off-topic  \n",
      "4  off-topic  \n",
      "(10008, 3)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "data = pd.read_csv('/home/ehoang/git/python/tweet_classification/data/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = ['TweetId', 'Tweet', 'label']\n",
    "data = data[data['label'] == 'on-topic']\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                TweetId                                              Tweet  \\\n",
      "0  '263044104500420609'  sandy be soooo mad that she be shattering our ...   \n",
      "1  '263101347421888513'  neighborly duties . arrives to the rescue spor...   \n",
      "2  '263298821189156865'  i don't know how i'm getting back to jersey si...   \n",
      "3  '262914476989358080'           already flooded so much #sandy @ hoboken   \n",
      "4  '262991999911743490'  on that note , i pray that everyone stays safe...   \n",
      "\n",
      "      label  \n",
      "0  on-topic  \n",
      "1  on-topic  \n",
      "2  on-topic  \n",
      "3  on-topic  \n",
      "4  on-topic  \n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ' '.join(tokenizeRawTweetText(x)))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('TWEETMENTION', \"\").replace(\"HTTPURL\", \"\").\n",
    "                                    replace(\"EMAILADDRESS\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_stemming(text):\n",
    "#     return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "# data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(lemmatize_stemming(y) for y in x.split(\" \") if y.strip()!= \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['like', 'https', 'htt', 'get', 'would', 'im', 'know', 'says',\n",
    "                   'want', 'see', 'make', 'need', 'think', 'going', 'please', 'let', 'w',\n",
    "                   '–', 'much', 'many', 'feel', 'go', 'take', 'like', 'hate', 'news', 'rt'])\n",
    "for item in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    stop_words.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords, punctuation\n",
    "\n",
    "data['Tweet1'] = data['Tweet'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y not in stop_words))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub('“|…|’|‘|”|—', \"\", x))\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: re.sub(' +', ' ',x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sandy soooo mad shattering doors shiet hurrica...\n",
       "1    neighborly duties arrives rescue sporting spel...\n",
       "2    im getting back jersey since trains subways ru...\n",
       "3                        already flooded sandy hoboken\n",
       "4    note pray everyone stays safe keeps positive a...\n",
       "Name: Tweet1, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet1'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8241 8241\n",
      "[('hurricane', 5119), ('sandy', 3194), ('im', 402), ('new', 299), ('everyone', 287), ('power', 272), ('safe', 269), ('people', 266), ('frankenstorm', 247), ('storm', 229), ('coast', 224), ('east', 217), ('even', 203), ('hope', 201), ('us', 189), ('york', 184), ('school', 177), ('shit', 173), ('nyc', 171), ('name', 170), ('stay', 166), ('cant', 143), ('hit', 142), ('really', 142), ('aint', 139), ('lol', 135), ('time', 127), ('rain', 125), ('still', 124), ('tomorrow', 116), ('affected', 114), ('got', 113), ('bad', 112), ('coming', 110), ('fuck', 107), ('due', 105), ('god', 103), ('gonna', 102), ('way', 102), ('house', 100), ('water', 100), ('bitch', 99), ('good', 99), ('via', 99), ('day', 97), ('jersey', 96), ('one', 95), ('irene', 93), ('praying', 92), ('weather', 92), ('home', 91), ('hurricanesandy', 91), ('prayers', 91), ('right', 88), ('help', 86), ('come', 85), ('getting', 84), ('path', 84), ('obama', 83), ('back', 82), ('call', 82), ('oh', 81), ('romney', 81), ('watch', 80), ('theres', 79), ('tho', 79), ('hurricanes', 76), ('live', 76), ('twitter', 76), ('actually', 75), ('city', 75), ('last', 74), ('ny', 74), ('tweets', 73), ('scary', 72), ('today', 72), ('went', 72), ('bout', 71), ('fucking', 70), ('real', 70), ('every', 69), ('sandys', 68), ('shanaynay', 68), ('love', 67), ('niggas', 66), ('brought', 64), ('victims', 63), ('making', 62), ('away', 61), ('katrina', 61), ('lost', 61), ('apocalypse', 60), ('ass', 60), ('damage', 60), ('could', 59), ('relief', 59), ('well', 59), ('work', 59), ('life', 58), ('north', 58)]\n"
     ]
    }
   ],
   "source": [
    "# view most frequent words\n",
    "cv = CountVectorizer()  \n",
    "cv_fit = cv.fit_transform(list(data['Tweet1']))\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)   \n",
    "wCount = dict(zip(word_list,count_list))\n",
    "textCount =  sorted(wCount.items(), key=lambda k: -k[1])\n",
    "print(len(word_list), len(textCount))\n",
    "print(textCount[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['len'] = data['Tweet1'].apply(lambda x: len(x.split(\" \")))\n",
    "data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6137, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove tweets #unique words less than haft of length\n",
    "# data['len'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "data = data[data['#uniWord']/data['len']>0.5]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5383, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove duplicates\n",
    "data.drop_duplicates(subset=['Tweet1'], keep='first', inplace = True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:  533\n",
      "data.shape:  (3257, 6)\n",
      "Len:  513\n",
      "data.shape:  (3028, 6)\n",
      "Len:  507\n",
      "data.shape:  (2977, 6)\n",
      "Len:  507\n",
      "data.shape:  (2960, 6)\n"
     ]
    }
   ],
   "source": [
    "# remove tweets with lengths < 3\n",
    "cv = CountVectorizer(stop_words='english', min_df = 10, max_df = 0.035) \n",
    "cv_fit = cv.fit(list(data['Tweet1']))\n",
    "vocab = set(cv.get_feature_names())\n",
    "data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y in vocab))\n",
    "while True:\n",
    "    data['len'] = data['Tweet1'].apply(lambda x: 0 if len(set(x.split(\" \")))/len(x.split(\" \")) <= 0.5 else len(x.split(\" \")))\n",
    "    data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))\n",
    "    data = data[data['len'] >2]\n",
    "    cv = CountVectorizer(stop_words='english', min_df = 10)  \n",
    "    cv.fit(list(data['Tweet1']))\n",
    "    newVocab = set(cv.get_feature_names())\n",
    "    \n",
    "    print(\"Len: \", len(newVocab))\n",
    "    print(\"data.shape: \", data.shape)\n",
    "    if len(vocab) == len(newVocab):\n",
    "        break\n",
    "    data['Tweet1'] = data['Tweet1'].apply(lambda x: ' '.join(y for y in x.split(\" \") if y in newVocab))\n",
    "    vocab = newVocab.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507 507\n",
      "[('coast', 157), ('school', 144), ('york', 139), ('hope', 135), ('nyc', 132), ('stay', 122), ('really', 107), ('shit', 106), ('lol', 103), ('tomorrow', 102), ('hit', 98), ('bad', 94), ('good', 85), ('time', 84), ('affected', 83), ('coming', 82), ('gonna', 82), ('got', 79), ('fuck', 75), ('jersey', 75), ('day', 74), ('water', 74), ('house', 72), ('home', 70), ('right', 70), ('way', 69), ('prayers', 68), ('rain', 68), ('getting', 66), ('city', 65), ('hurricanesandy', 65), ('today', 65), ('come', 64), ('live', 63), ('weather', 62), ('praying', 61), ('god', 60), ('ny', 60), ('theres', 60), ('romney', 57), ('love', 56), ('twitter', 54), ('watch', 54), ('bitch', 52), ('obama', 51), ('path', 49), ('wind', 49), ('work', 48), ('relief', 47), ('away', 46), ('damn', 46), ('friends', 46), ('monday', 46), ('sandys', 46), ('thoughts', 46), ('ass', 45), ('thing', 45), ('yall', 45), ('night', 44), ('real', 44), ('thanks', 44), ('closed', 43), ('damage', 42), ('thats', 42), ('winds', 42), ('days', 41), ('fucking', 41), ('hurricanes', 41), ('katrina', 41), ('nj', 41), ('street', 40), ('crazy', 39), ('flooding', 39), ('guys', 39), ('help', 39), ('park', 39), ('aint', 38), ('big', 38), ('little', 38), ('making', 38), ('oh', 38), ('video', 38), ('outside', 37), ('stop', 37), ('watching', 37), ('better', 36), ('family', 36), ('goes', 36), ('world', 36), ('actually', 35), ('apocalypse', 35), ('tonight', 35), ('high', 34), ('irene', 34), ('party', 34), ('thank', 34), ('end', 33), ('funny', 33), ('2012', 32), ('blow', 32)]\n"
     ]
    }
   ],
   "source": [
    "# most frequent words after removing short tweets, highly/low frequent words\n",
    "cv = CountVectorizer() \n",
    "cv_fit = cv.fit_transform(list(data['Tweet1']))\n",
    "word_list = cv.get_feature_names()\n",
    "count_list = cv_fit.toarray().sum(axis=0)   \n",
    "wCount = dict(zip(word_list,count_list))\n",
    "textCount =  sorted(wCount.items(), key=lambda k: -k[1])\n",
    "print(len(word_list), len(textCount))\n",
    "print(textCount[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: 507\n",
      "Len(biterms): 2960\n",
      "\n",
      "\n",
      " Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:37<00:00,  1.33it/s]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:0, 37.4647696018219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Line:2000, 17.65606117248535\n",
      "\n",
      "\n",
      " Topic coherence ..\n",
      "Topic 0 | Coherence=-150.57 | Top words= shit school hope bad nyc really gonna coming good lol\n",
      "Topic 1 | Coherence=-142.00 | Top words= coast obama nj hits hope left warning god school jersey\n",
      "Topic 2 | Coherence=-130.64 | Top words= got food water streets survival major driving trees black powers\n",
      "Topic 3 | Coherence=-111.83 | Top words= katrina david stern said irene bout fuck aint lol niggas\n",
      "Topic 4 | Coherence=-110.19 | Top words= romney relief obama campaign fema google mitt president bus disaster\n",
      "Topic 5 | Coherence=-142.83 | Top words= york ny apocalypse time weather issued tsunami canada warning latest\n",
      "Topic 6 | Coherence=-126.81 | Top words= jersey monday shore coast school tuesday blow closed snooki tomorrow\n",
      "Topic 7 | Coherence=-140.79 | Top words= away hurricanes fuck things come little romney blown mitt house\n",
      "Topic 8 | Coherence=-131.91 | Top words= york city nyc subway times service video approaches shut view\n",
      "Topic 9 | Coherence=-140.29 | Top words= today relief time media million buy social local emergency president\n",
      "Topic 10 | Coherence=-95.21 | Top words= tomb unknown soldier guard national winds center mph amazing continues\n",
      "Topic 11 | Coherence=-140.60 | Top words= school tomorrow work hit home bad days night monday 12\n",
      "Topic 12 | Coherence=-130.72 | Top words= tsunami twitter really lol hawaii coast funny earthquake real tweets\n",
      "Topic 13 | Coherence=-139.12 | Top words= good water way thing streets thats flooded came ny far\n",
      "Topic 14 | Coherence=-141.37 | Top words= today home class hurricanesandy thank stay tree time hits way\n",
      "Topic 15 | Coherence=-72.21 | Top words= style dance brought rain actually giant gangnam massive really weve\n",
      "Topic 16 | Coherence=-130.28 | Top words= coast lives hit apple maps lost rip pray worst breaking\n",
      "Topic 17 | Coherence=-109.83 | Top words= affected stay prayers thoughts coast praying path friends hope god\n",
      "Topic 18 | Coherence=-147.22 | Top words= affected hope prayers goes help nyc praying cross red thank\n",
      "Topic 19 | Coherence=-128.58 | Top words= stop bus subway romney instead came today wont rain goes\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    texts = list(data['Tweet1'])\n",
    "\n",
    "    # vectorize texts\n",
    "    vec = CountVectorizer()\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "    print(\"Vocab: {}\".format(len(vocab)))\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(X)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=num_topics, V=vocab)\n",
    "    print(\"Len(biterms):\", len(biterms))\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    start = time.time()\n",
    "    for i in range(0, len(biterms), 2000): # prozess chunk of 200 texts\n",
    "        \n",
    "        biterms_chunk = biterms[i:i + 2000]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "        \n",
    "        if i%2000 ==0:\n",
    "            print(\"....Line:{}, {}\".format(i, (time.time()-start)))\n",
    "            start = time.time()\n",
    "    topics = btm.transform(biterms)\n",
    "\n",
    "#     print(\"\\n\\n Visualize Topics ..\")\n",
    "#     vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))\n",
    "#     pyLDAvis.save_html(vis, 'online_btm.html')\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, X, vocab, 10)\n",
    "\n",
    "#     print(\"\\n\\n Texts & Topics ..\")\n",
    "#     for i in range(len(texts)):\n",
    "#         print(\"{}. {} (topic: {})\".format(i, texts[i], topics[i].argmax()))\n",
    "#     print(topics.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 | Coherence=-150.57 | Top words= shit school hope bad nyc really gonna coming good lol\n",
      "Topic 1 | Coherence=-142.00 | Top words= coast obama nj hits hope left warning god school jersey\n",
      "Topic 2 | Coherence=-130.64 | Top words= got food water streets survival major driving trees black powers\n",
      "Topic 3 | Coherence=-111.83 | Top words= katrina david stern said irene bout fuck aint lol niggas\n",
      "Topic 4 | Coherence=-110.19 | Top words= romney relief obama campaign fema google mitt president bus disaster\n",
      "Topic 5 | Coherence=-142.83 | Top words= york ny apocalypse time weather issued tsunami canada warning latest\n",
      "Topic 6 | Coherence=-126.81 | Top words= jersey monday shore coast school tuesday blow closed snooki tomorrow\n",
      "Topic 7 | Coherence=-140.79 | Top words= away hurricanes fuck things come little romney blown mitt house\n",
      "Topic 8 | Coherence=-131.91 | Top words= york city nyc subway times service video approaches shut view\n",
      "Topic 9 | Coherence=-140.29 | Top words= today relief time media million buy social local emergency president\n",
      "Topic 10 | Coherence=-95.21 | Top words= tomb unknown soldier guard national winds center mph amazing continues\n",
      "Topic 11 | Coherence=-140.60 | Top words= school tomorrow work hit home bad days night monday 12\n",
      "Topic 12 | Coherence=-130.72 | Top words= tsunami twitter really lol hawaii coast funny earthquake real tweets\n",
      "Topic 13 | Coherence=-139.12 | Top words= good water way thing streets thats flooded came ny far\n",
      "Topic 14 | Coherence=-141.37 | Top words= today home class hurricanesandy thank stay tree time hits way\n",
      "Topic 15 | Coherence=-72.21 | Top words= style dance brought rain actually giant gangnam massive really weve\n",
      "Topic 16 | Coherence=-130.28 | Top words= coast lives hit apple maps lost rip pray worst breaking\n",
      "Topic 17 | Coherence=-109.83 | Top words= affected stay prayers thoughts coast praying path friends hope god\n",
      "Topic 18 | Coherence=-147.22 | Top words= affected hope prayers goes help nyc praying cross red thank\n",
      "Topic 19 | Coherence=-128.58 | Top words= stop bus subway romney instead came today wont rain goes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence': [-150.5675121148826,\n",
       "  -141.99736434095607,\n",
       "  -130.63707085423516,\n",
       "  -111.82654720697106,\n",
       "  -110.18830082802589,\n",
       "  -142.83451239286268,\n",
       "  -126.81212794987185,\n",
       "  -140.78897075064302,\n",
       "  -131.91192497611104,\n",
       "  -140.29405872670708,\n",
       "  -95.21145973414153,\n",
       "  -140.59996714430235,\n",
       "  -130.7235404361064,\n",
       "  -139.1203029459656,\n",
       "  -141.371086568604,\n",
       "  -72.20884531450177,\n",
       "  -130.28437099577383,\n",
       "  -109.83143262724565,\n",
       "  -147.21603447326498,\n",
       "  -128.57804468210708],\n",
       " 'top_words': [array(['shit', 'school', 'hope', 'bad', 'nyc', 'really', 'gonna',\n",
       "         'coming', 'good', 'lol'], dtype='<U14'),\n",
       "  array(['coast', 'obama', 'nj', 'hits', 'hope', 'left', 'warning', 'god',\n",
       "         'school', 'jersey'], dtype='<U14'),\n",
       "  array(['got', 'food', 'water', 'streets', 'survival', 'major', 'driving',\n",
       "         'trees', 'black', 'powers'], dtype='<U14'),\n",
       "  array(['katrina', 'david', 'stern', 'said', 'irene', 'bout', 'fuck',\n",
       "         'aint', 'lol', 'niggas'], dtype='<U14'),\n",
       "  array(['romney', 'relief', 'obama', 'campaign', 'fema', 'google', 'mitt',\n",
       "         'president', 'bus', 'disaster'], dtype='<U14'),\n",
       "  array(['york', 'ny', 'apocalypse', 'time', 'weather', 'issued', 'tsunami',\n",
       "         'canada', 'warning', 'latest'], dtype='<U14'),\n",
       "  array(['jersey', 'monday', 'shore', 'coast', 'school', 'tuesday', 'blow',\n",
       "         'closed', 'snooki', 'tomorrow'], dtype='<U14'),\n",
       "  array(['away', 'hurricanes', 'fuck', 'things', 'come', 'little', 'romney',\n",
       "         'blown', 'mitt', 'house'], dtype='<U14'),\n",
       "  array(['york', 'city', 'nyc', 'subway', 'times', 'service', 'video',\n",
       "         'approaches', 'shut', 'view'], dtype='<U14'),\n",
       "  array(['today', 'relief', 'time', 'media', 'million', 'buy', 'social',\n",
       "         'local', 'emergency', 'president'], dtype='<U14'),\n",
       "  array(['tomb', 'unknown', 'soldier', 'guard', 'national', 'winds',\n",
       "         'center', 'mph', 'amazing', 'continues'], dtype='<U14'),\n",
       "  array(['school', 'tomorrow', 'work', 'hit', 'home', 'bad', 'days',\n",
       "         'night', 'monday', '12'], dtype='<U14'),\n",
       "  array(['tsunami', 'twitter', 'really', 'lol', 'hawaii', 'coast', 'funny',\n",
       "         'earthquake', 'real', 'tweets'], dtype='<U14'),\n",
       "  array(['good', 'water', 'way', 'thing', 'streets', 'thats', 'flooded',\n",
       "         'came', 'ny', 'far'], dtype='<U14'),\n",
       "  array(['today', 'home', 'class', 'hurricanesandy', 'thank', 'stay',\n",
       "         'tree', 'time', 'hits', 'way'], dtype='<U14'),\n",
       "  array(['style', 'dance', 'brought', 'rain', 'actually', 'giant',\n",
       "         'gangnam', 'massive', 'really', 'weve'], dtype='<U14'),\n",
       "  array(['coast', 'lives', 'hit', 'apple', 'maps', 'lost', 'rip', 'pray',\n",
       "         'worst', 'breaking'], dtype='<U14'),\n",
       "  array(['affected', 'stay', 'prayers', 'thoughts', 'coast', 'praying',\n",
       "         'path', 'friends', 'hope', 'god'], dtype='<U14'),\n",
       "  array(['affected', 'hope', 'prayers', 'goes', 'help', 'nyc', 'praying',\n",
       "         'cross', 'red', 'thank'], dtype='<U14'),\n",
       "  array(['stop', 'bus', 'subway', 'romney', 'instead', 'came', 'today',\n",
       "         'wont', 'rain', 'goes'], dtype='<U14')]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summuary(btm.phi_wz.T, X, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(507, 20)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " btm.phi_wz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop= True)\n",
    "data['len'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))\n",
    "data['#uniWord'] = data['Tweet1'].apply(lambda x: len(set(x.split(\" \"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract instances/tweets that have highest topic-document prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2960, 20)\n",
      "#0. 1745\t 0.9994337396495933 live watch watch\n",
      "#1. 1528\t 0.9014600008969619 gets warning job\n",
      "#2. 1936\t 0.8953393075771464 survival water candles black\n",
      "#3. 1668\t 0.999079775815964 katrina david stern\n",
      "#4. 2638\t 0.9865459762056363 campaign bus relief\n",
      "#5. 86\t 0.9732178360935507 apocalypse york ny\n",
      "#6. 846\t 0.9579061675484792 snooki lot theyre heading jersey shore blow\n",
      "#7. 1949\t 0.9714008051512666 warnings warnings little things\n",
      "#8. 2381\t 0.9577537247925334 york city subway\n",
      "#9. 2688\t 0.9973449888132752 social media approaches\n",
      "#10. 2313\t 0.9999937572115121 tomb unknown soldier\n",
      "#11. 1840\t 0.8394613494687058 wont bad pm\n",
      "#12. 876\t 0.9509203485826587 jokes funny dying\n",
      "#13. 1654\t 0.9861082589019909 blog ave ave\n",
      "#14. 298\t 0.7720940769865772 evacuation time evacuate\n",
      "#15. 1616\t 0.9995726635530693 gangnam style rain dance brought\n",
      "#16. 2621\t 0.9754705635626476 apple maps told approaching\n",
      "#17. 2495\t 0.9418282436916979 thoughts affected stay\n",
      "#18. 28\t 0.9088484860487207 town fine driving\n",
      "#19. 271\t 0.3339305138506348 way stop bus\n"
     ]
    }
   ],
   "source": [
    "x = pd.DataFrame(topics)\n",
    "print(x.shape)\n",
    "x.shape\n",
    "for i in range(num_topics):\n",
    "    print(\"#{}. {}\\t {} {}\".format(i, x[i].idxmax(),  x[i].max(), data.iloc[x[i].idxmax()]['Tweet1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   4,    5,    6,    8,   10,   11,   12,   18,   23,   24,\n",
      "            ...\n",
      "            2931, 2932, 2936, 2938, 2940, 2946, 2947, 2948, 2952, 2953],\n",
      "           dtype='int64', length=1000)\n",
      "Int64Index([   4,    5,    6,    8,   10,   11,   12,   18,   23,   24,\n",
      "            ...\n",
      "            2931, 2932, 2936, 2938, 2940, 2946, 2947, 2948, 2952, 2953],\n",
      "           dtype='int64', length=1000)\n"
     ]
    }
   ],
   "source": [
    "# extract only instance with > 5 uniWords\n",
    "data1 = data[data['#uniWord']>4].copy()\n",
    "print(data1.index)\n",
    "x = pd.DataFrame(topics)\n",
    "x = x.iloc[data1.index]\n",
    "print(x.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.reset_index(drop=True)\n",
    "x = x.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0. 940\t 0.9982797060849918 star coming thought worst week\n",
      "#1. 306\t 0.8026268554040461 great parents mt care disaster\n",
      "#2. 127\t 0.725340194253373 thank house telling major streets closed trees powers driving\n",
      "#3. 181\t 0.9917232640501498 nigga david stern said katrina\n",
      "#4. 564\t 0.976738199492876 mitt romney talk fema event\n",
      "#5. 350\t 0.8027260135439039 breaking weve issued tsunami warnings states\n",
      "#6. 285\t 0.9579061675484792 snooki lot theyre heading jersey shore blow\n",
      "#7. 656\t 0.7779909011227468 blow away mean send huge wind\n",
      "#8. 263\t 0.8854107129358217 video york city view times square\n",
      "#9. 284\t 0.88376932954098 social media residents approaches business\n",
      "#10. 540\t 0.9998254588776453 guard tomb unknown soldier usa\n",
      "#11. 735\t 0.6939992015094045 12 pm 12 10 days halloween cancelled\n",
      "#12. 876\t 0.8468227396384019 tsunami canada earthquake hawaii 2012\n",
      "#13. 64\t 0.7551031927107661 streets flooded rain came far\n",
      "#14. 37\t 0.6082512620000926 strong tree hard strong hurricanesandy jersey\n",
      "#15. 538\t 0.9995726635530693 gangnam style rain dance brought\n",
      "#16. 933\t 0.8931241022853917 rip victims lost lives follow\n",
      "#17. 798\t 0.9144937886764148 sandys path thoughts prayers stay god bless\n",
      "#18. 333\t 0.7392520945402843 heart sad destroyed hope prayers\n",
      "#19. 767\t 0.06674775615909678 breaking nyc subway shut tonight stop\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_topics):\n",
    "    print(\"#{}. {}\\t {} {}\".format(i, x[i].idxmax(),  x[i].max(), data1.iloc[x[i].idxmax()]['Tweet1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract instances/tweets that maximize P(d|z)\n",
    "   *   $P(d|z) = \\ln{(len(d))}*\\prod_{b_i \\in d}P(b_i|z)$ # give more weight to long sentences\n",
    "   \n",
    "   *   $d = argmax_dP(d|z) = argmax_d \\ln{(P(d|z))}$\n",
    "   *   $d = argmax_d\\ln{(\\ln{(len(d)))}} + \\sum_{b_i}\\ln{P(b_i|z)}$\n",
    "   *   $d = argmax_d\\ln{(\\ln{(len(d)))}} + \\sum_{b_i, w_{i0, i1} \\in b_i}\\ln{(P(w_{i0}|z)*P(w_{i1}|z)))}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data[data['#uniWord'] >4].copy()\n",
    "remained_index = data1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 =data1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   4,    5,    6,    8,   10,   11,   12,   18,   23,   24,\n",
       "            ...\n",
       "            2931, 2932, 2936, 2938, 2940, 2946, 2947, 2948, 2952, 2953],\n",
       "           dtype='int64', length=1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remained_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "biterms1 = np.array(biterms).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "biterms1 = biterms1[remained_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "num_topics = 20\n",
    "P_dz = np.zeros([len(biterms1), num_topics])\n",
    "# iterate documents\n",
    "for i, d in enumerate(biterms1):\n",
    "    n_biterms = len(d)\n",
    "    P_bz = np.zeros([len(d), num_topics])\n",
    "    for j, b in enumerate(d):\n",
    "        P_bz[j] = np.log(btm.phi_wz[b[0], :] * btm.phi_wz[b[1], :])\n",
    "    P_dz[i] = P_bz.sum(axis = 0)\n",
    "\n",
    "print(P_dz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract most representative sentences for each topic\n",
    "indices = P_dz.argmax(axis = 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 320\twatch this hurricane not do shit and we all end up going to school tomorrow . #weallknowitsgoingtohappen\n",
      "1. 500\tsince 1937 , the tomb guards have never left their post . hurricane sandy will be no exception . god bless our military ht ...\n",
      "2. 750\t#hurricanesandy rt : my hurricane survival kit : bottled water , candles , radar invisible black jetboat .\n",
      "3. 181\tlmfao this nigga david stern said hurricane katrina\n",
      "4. 741\tmitt romney donates campaign bus to hurricane sandy relief |\n",
      "5. 554\tfrankenstorm in ny , earthquake in canada , tsunami in hawaii .... #mayans\n",
      "6. 402\twhat do snooki and hurricane #sandy have in common ?… they will blow the entire east coast to get on tv .\n",
      "7. 83\tshe gon blow yo ass away #hahaha #goodluck\n",
      "8. 528\tnew york city shuts all subway , bus and train services at 19:00 est ( 23:00 gmt ) as hurricane #sandy approaches ...\n",
      "9. 582\t#thanksdonald for choosing to donate the 5 million to hurricane relief . tell your friends about this wonderful deed #s ...\n",
      "10. 540\tsoldiers continuing to guard the tomb of the unknown soldier throughout hurricane sandy #usa\n",
      "11. 101\tno work tomorrow . this feels like a snow day in school . #sandy you carazyyy\n",
      "12. 876\ttsunami going to canada , earthquake in hawaii , hurricane sandy ??? #2012\n",
      "13. 6\twater levels be rising !!! (@ long island city piers ) [ pic ]:\n",
      "14. 37\tbe strong my beautiful tree , i know is hard , but be strong ! o_o #hurricanesandy @ garfield , new jersey\n",
      "15. 538\twhat if gangnam style was just a serious rain dance and we brought this hurricane upon ourselves . ??\n",
      "16. 933\tr.i.p to the 65 victims who lost their lives because of hurricane sandy . rt for respect . follow back\n",
      "17. 490\tour thoughts & prayers are with those who will be affected by hurricane sandy . we hope you stay safe & sound during th ...\n",
      "18. 345\tif you would like to donate to the red cross to help those affected by hurricane sandy , please do so at ...\n",
      "19. 84\tstill no power in #chelsea . rain doesn't stop , wind is strong . #sandy #nyc\n"
     ]
    }
   ],
   "source": [
    "for i, idx in enumerate(indices):\n",
    "    print(\"{}. {}\\t{}\".format(i, idx, data1.iloc[idx]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('biterm.pkl', 'wb') as f:\n",
    "    pickle.dump(btm, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
