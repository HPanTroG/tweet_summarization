{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization:\n",
    "1. K mean with sentence embeddings returned by SentenceTransformer\n",
    "2. Biterm\n",
    "3. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nlp_path = os.path.abspath('../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "from utils import tokenizeRawTweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from gensim import corpora, models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import string\n",
    "import emoji\n",
    "import nltk, time\n",
    "# nltk.download('wordnet')\n",
    "from collections import Counter\n",
    "import pyLDAvis\n",
    "from biterm.cbtm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweet id                                              tweet  \\\n",
      "0  '262596552399396864'  I've got enough candles to supply a Mexican fa...   \n",
      "1  '263044104500420609'  Sandy be soooo mad that she be shattering our ...   \n",
      "2  '263309629973491712'  @ibexgirl thankfully Hurricane Waugh played it...   \n",
      "3  '263422851133079552'  @taos you never got that magnificent case of B...   \n",
      "4  '262404311223504896'  I'm at Mad River Bar &amp; Grille (New York, N...   \n",
      "\n",
      "       label  \n",
      "0  off-topic  \n",
      "1   on-topic  \n",
      "2  off-topic  \n",
      "3  off-topic  \n",
      "4  off-topic  \n",
      "(10008, 3)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "# data = pd.read_csv('/home/nguyen/data/processed_travel_ban.csv')\n",
    "data = pd.read_csv('/home/ehoang/git/python/tweet_classification/data/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6138, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = ['TweetId', 'Tweet', 'label']\n",
    "data = data[data['label'] == 'on-topic']\n",
    "data = data.reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sandy be soooo mad that she be shattering our doors and shiet #HurricaneSandy',\n",
       " 'Neighborly duties. @Cory_Kennedy arrives to the rescue sporting some spelunking equipment #sandy @ 300 Squad http://t.co/QbpGdm3w',\n",
       " \"I don't know how I'm getting back to Jersey since the trains and subways aren't running...\",\n",
       " 'Already flooded so much #SANDY @ Hoboken http://t.co/MPhft4a8',\n",
       " 'On that note, i pray that everyone stays safe, and keeps a positive attitude ! #godisgood',\n",
       " \"@codyfinz my house is creeking... Does that mean she's trying to break in?\",\n",
       " 'debating going home in prep for #sandy',\n",
       " \"By 11am it's going to be 100% chance of rain #HurricaneSandy\",\n",
       " '@newscaster we are 5 blocks from the water. First two blocks were evacuated. Sounds like a train just went by. Stay safe! Thanks.',\n",
       " \"It's crazy out there, not gonna lie I'm kind of scared.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[0:10]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                TweetId                                              Tweet  \\\n",
      "0  '263044104500420609'  sandy be soooo mad that she be shattering our ...   \n",
      "1  '263101347421888513'  neighborly duties . arrives to the rescue spor...   \n",
      "2  '263298821189156865'  i don't know how i'm getting back to jersey si...   \n",
      "3  '262914476989358080'          already flooded so much #sandy @ hoboken    \n",
      "4  '262991999911743490'  on that note , i pray that everyone stays safe...   \n",
      "\n",
      "      label  \n",
      "0  on-topic  \n",
      "1  on-topic  \n",
      "2  on-topic  \n",
      "3  on-topic  \n",
      "4  on-topic  \n"
     ]
    }
   ],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ' '.join(tokenizeRawTweetText(x)))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('TWEETMENTION', \"\").replace(\"HTTPURL\", \"\").\n",
    "                                    replace(\"EMAILADDRESS\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sandy be soooo mad that she be shattering our doors and shiet #hurricanesandy',\n",
       " 'neighborly duties . arrives to the rescue sporting some spelunking equipment #sandy @ 300 squad ',\n",
       " \"i don't know how i'm getting back to jersey since the trains and subways aren't running ...\",\n",
       " 'already flooded so much #sandy @ hoboken ',\n",
       " 'on that note , i pray that everyone stays safe , and keeps a positive attitude ! #godisgood',\n",
       " \" my house is creeking ... does that mean she's trying to break in ?\",\n",
       " 'debating going home in prep for #sandy',\n",
       " \"by 11am it's going to be 100% chance of rain #hurricanesandy\",\n",
       " ' we are 5 blocks from the water . first two blocks were evacuated . sounds like a train just went by . stay safe ! thanks .',\n",
       " \"it's crazy out there , not gonna lie i'm kind of scared .\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[0:10]['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMean(data, sentenceEmbs, n_clusters = 20, random_states = 10):\n",
    "    print(\"data Shape: \", data.shape)\n",
    "    print(\"Embedding shape: \", sentenceEmbs.shape)\n",
    "    sentences = []\n",
    "    for x in sentenceEmbs:\n",
    "        sentences.append(x.ravel())\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_states)\n",
    "    kmeans.fit(sentences)\n",
    "    labels = kmeans.labels_.tolist()\n",
    "    \n",
    "    centers = np.array(kmeans.cluster_centers_)\n",
    "    # compute cluster size:\n",
    "    cluster_size = {}\n",
    "    for i in range(n_clusters):\n",
    "        cluster_size[i] = labels.count(i)\n",
    "        \n",
    "    # find elements closest to the cluster centers\n",
    "    closest_data = []\n",
    "    for i in range(n_clusters):\n",
    "        center_vec = centers[i].reshape(1, -1)\n",
    "        data_idx_within_i_cluster = [ idx for idx, clu_num in enumerate(labels) if clu_num == i ]\n",
    "\n",
    "        one_cluster_tf_matrix = np.zeros( (len(data_idx_within_i_cluster) , centers.shape[1] ) )\n",
    "        for row_num, data_idx in enumerate(data_idx_within_i_cluster):\n",
    "            one_row = sentences[data_idx]\n",
    "            one_cluster_tf_matrix[row_num] = one_row\n",
    "\n",
    "\n",
    "        closest, _ = pairwise_distances_argmin_min(center_vec, one_cluster_tf_matrix)\n",
    "        closest_idx_in_one_cluster_tf_matrix = closest[0]\n",
    "        closest_data_row_num = data_idx_within_i_cluster[closest_idx_in_one_cluster_tf_matrix]\n",
    "    #     data_id = all_data[closest_data_row_num]\n",
    "\n",
    "        closest_data.append(closest_data_row_num)\n",
    "\n",
    "    closest_data = list(set(closest_data))\n",
    "    \n",
    "    return labels, closest_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kmean: first token bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import embedding_extraction as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_len (99% data): 37.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehoang/miniconda3/envs/py37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data: \n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 7525, 2022, 17111, 9541, 5506, 2008, 201...   \n",
      "1  [101, 11429, 2135, 5704, 1012, 8480, 2000, 199...   \n",
      "2  [101, 1045, 2123, 1005, 1056, 2113, 2129, 1045...   \n",
      "3  [101, 2525, 10361, 2061, 2172, 1001, 7525, 103...   \n",
      "4  [101, 2006, 2008, 3602, 1010, 1045, 11839, 200...   \n",
      "\n",
      "                                      token_type_ids  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "---------- 0 1000\n",
      "torch.Size([1000, 768])\n",
      "---------- 1000 2000\n",
      "(2000, 768)\n",
      "---------- 2000 3000\n",
      "(3000, 768)\n",
      "---------- 3000 4000\n",
      "(4000, 768)\n",
      "---------- 4000 5000\n",
      "(5000, 768)\n",
      "---------- 5000 6000\n",
      "(6000, 768)\n",
      "---------- 6000 6138\n",
      "(6138, 768)\n"
     ]
    }
   ],
   "source": [
    "sentenceEmbs = model.get_bert_first_token_embeddings(data, cuda=\"cuda:2\", dataColumn=\"Tweet\", max_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nguyen/data/travel_ban_first_token_embeddings.pkl', 'rb') as f:\n",
    "    sentenceEmbs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Shape:  (6138, 4)\n",
      "Embedding shape:  (6138, 768)\n"
     ]
    }
   ],
   "source": [
    "cluster_size, closest_data = kMean(data, sentenceEmbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3843 the latest full disk image of earth puts hurricane sandy into perspective ... it's massive : #fra ...\n",
      "5257 i can't tweet this enough .... stay safe everyone that will be affected by hurricane sandy . sending prayers .\n",
      "4237 can we have a hurricane sandy day ? too much wind to be driving ...\n",
      "3470 here's a copy of the lawsuit give msm a chance to make all the $ off you they can w/hurricane san ...\n",
      "2201 hope that hurricane is causing toooo much harm ! \\ #missyou\n",
      "1820 real talk all my prayers go out to those dealing with hurricane sandy ! #staysafe\n",
      "541 me & my new #pet turtle read this together ... #frankenstorm : hurricane pet emergency plan \n",
      "2334 nice game : i like that a hurricane ruining all your shit cant ruin your spirit . #positivity #newstuff #sandysa ...\n",
      "2722 “ : amazing picture from ocean grove , new jersey . check these mofo waves ! #sandy #frankenstorm ” \n",
      "3234 hurricane party at the altman resident tomorrow wooooooooh\n",
      "934 with all of the storms & other events that have left people w/o power , i seriously wonder why still hasn't implemented ( 1/2 )\n",
      "1714 i'm not worried about hurricane sandy , i have a shamwow .\n",
      "1462 hurricane sandy ain't even a scary name tho . i need a terrifying name like hurricane shanaynay or sum shit .\n",
      "825 priorities . @ frankenstorm apocalypse - hurricane sandy \n",
      "3267 who ready to tweet threw this hurricane ??\n",
      "5317 ahhh shit , they named a hurricane after you ! i know ! they done fucked up now lol\n",
      "2132 i finally have time to finish my college stuff because of this hurricane . #thankssandy\n",
      "3802 some people are dumb and naive and want the hurricane to get worse . but you won't be saying wow this is awesome ! whe ...\n",
      "5876 maybe hurricane sandy will kill oomf #wishfulthinking\n",
      "254  i'm going to be staying at a friend's! haha . too scared of the hurrilame\n"
     ]
    }
   ],
   "source": [
    "for i in closest_data:\n",
    "    print(i, str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \n",
    "\n",
    "* <b>Sub-events captured:</b>\n",
    "        1. protest at the airport\n",
    "        2. starbuck hires 10K refugees\n",
    "        3. attorney general get fired\n",
    "        4. trump’s deportation orders \n",
    "        5. restricting refugees & all entrants from 7 countries\n",
    "        6. washington state will sue to stop trump's immigration\n",
    "        7. canada will accept the refugees \n",
    "* <b>Lack</b>\n",
    "        1. green card holders\n",
    "        2. trump's ban doesn't include saudi arabia .\n",
    "        3. trump visit uk\n",
    "        4. trump sign a new executive order\n",
    "        5. quebec city mosque shooting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kmean: sentence transformers embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:  (1000, 768)\n",
      "Len:  (2000, 768)\n",
      "Len:  (3000, 768)\n",
      "Len:  (4000, 768)\n",
      "Len:  (5000, 768)\n",
      "Len:  (6000, 768)\n",
      "Len:  (6138, 768)\n"
     ]
    }
   ],
   "source": [
    "sentenceEmbs = model.get_sentence_transformers_embedings(data, cuda='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nguyen/data/travel_ban_sentence_transformers_embeddings.pkl', 'rb') as f:\n",
    "    sentenceEmbs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data Shape:  (6138, 4)\n",
      "Embedding shape:  (6138, 768)\n"
     ]
    }
   ],
   "source": [
    "cluster_size, closest_data = kMean(data, sentenceEmbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 according to this i live in a zone-free area . still got power & everything's working fine ! #lucky #staysafenyc \n",
      "4225 i'm ready for this hurricane . got my toilet paper , flashlight , and doritos . i should be aight ...\n",
      "1670 huge . rt : rt the newest satellite view of hurricane sandy shows how massive this storm is : http ...\n",
      "397 before and after #sandy @ breezy point , ny \n",
      "401 i'm at frankenstorm apocalypse - hurricane sandy ( new york , ny ) w/ 2852 others \n",
      "4894 what if gahgnam styke was actually a giant rain dance and we've brought this hurricane upon ourselves ... #sandy\n",
      "3870 “ : r.i.p to the 65 victims who lost their lives because of hurricane sandy . rt for respect <3 ”\n",
      "4005 i hope this hurricane don't take my power out ??\n",
      "1462 hurricane sandy ain't even a scary name tho . i need a terrifying name like hurricane shanaynay or sum shit .\n",
      "3517 we aren't even being hit with the hurricane and classes are still cancelled #soundsguccimynigga\n",
      "5187 google has set up a “ crisis map ” for those preparing for hurricane sandy : google has a fantas ... #technology #media\n",
      "1348 #signstherainisratchet it comes from hurricane sandy\n",
      "3782 “ : blowin so many niggas they should call me hurricane sandy kardashian . ” #dying\n",
      "5961 the nyse will suspend physical trading and floor operations on monday\\due to #hurricane #sandy . \n",
      "1740 this hurricane is really ruining my plans\n",
      "1377 omg a first down , what next , a hurricane ? #ohwait\n",
      "3171 call n see what's up with dat hurricane ! in fla too ! aaaah they picked a great time ! smh\n",
      "2168 i'll hang out as long as i can , but the batteries are about to go soon . just wanted to get all that out before frankenstorm eats me lol ;)\n",
      "2042 praying for everyone in hurricane sandy's path ! please be safe . xoxo\n",
      "5502 thanks hurricane sandy for coming on the week of quarterly tests lol\n"
     ]
    }
   ],
   "source": [
    "for i in closest_data:\n",
    "    print(i, str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### \n",
    "\n",
    "* <b>Sub-events captured:</b>\n",
    "        1. protest at the airport\n",
    "        2. attorney general get fired\n",
    "        3. quebec city mosque shooting \n",
    "        4. trump visit uk\n",
    "* <b>Lack</b>\n",
    "        1. trump’s deportation orders \n",
    "        2. restricting refugees & all entrants from 7 countries\n",
    "        3. washington state will sue to stop trump's immigration\n",
    "        4. canada will accept the refugees \n",
    "        5. green card holders\n",
    "        6. starbuck hires 10K refugees\n",
    "        7. trump's ban doesn't include saudi arabia .\n",
    "        9. trump sign a new executive order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
