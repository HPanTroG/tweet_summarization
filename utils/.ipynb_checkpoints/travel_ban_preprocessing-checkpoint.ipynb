{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tweet data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "nlp_path = os.path.abspath('../')\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dstore/home/nguyen/tweet_summarization'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenizeRawTweetText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "WORK_DIR = \"/home/nguyen/\"\n",
    "RAW_DATA = \"data/travel_ban.txt\"\n",
    "PROCESSED_DATA = \"data/processed_travel_ban.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw dataset, process and print to file\n",
    "def read_and_print_data():\n",
    "    file = open(WORK_DIR+RAW_DATA, \"r\")\n",
    "    lines = file.readlines()\n",
    "    data = []\n",
    "    i=0\n",
    "    for line in lines:\n",
    "       \n",
    "        s = line.split(\"\\t\")\n",
    "        tweetId = s[0]\n",
    "        text = s[4]\n",
    "        text = ' '.join(tokenizeRawTweetText(text))\n",
    "#         print(tweetId, text)\n",
    "#         break\n",
    "        data.append([tweetId, text])\n",
    "        i+=1\n",
    "        if i %1000 ==0:\n",
    "            print(\"Line {}: {}, label: {}\\n\".format(i, tweetId, text))\n",
    "            break\n",
    "    output = pd.DataFrame(data, columns=['Id', 'Tweet'])\n",
    "    \n",
    "    output.to_csv(WORK_DIR+PROCESSED_DATA, index=False)\n",
    "    file.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1000: 825046054500433925, label: RT @MENTION : Theresa May speaking about several issues in her press conference with Donald Trump saying no tangible thing about issues .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "read_and_print_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract first token Bert embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import pickle\n",
    "import embedding_extraction as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/home/nguyen/data/processed_travel_ban.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @MENTION : Emergency Rally Against Trump's Muslim Travel Ban in NYC , 1/25 at 5 p.m. @URL\n",
      "RT @MENTION : Theresa May has not apologized to Trump for insulting him . If she fails to do that today , Trump should just send her back to B ‚Ä¶\n",
      "RT @MENTION : Trump's Immigration Ban Excludes Countries with Business Ties @URL via @MENTION #DemocracyFor ‚Ä¶\n",
      "RT @MENTION : Trump's immigration order expands the definition of \" criminal \" @URL @URL\n",
      "ALERT : Senator John McCain Threatens Action On President Trump If He Does This @URL\n",
      "@MENTION @MENTION @MENTION @MENTION @MENTION Kiva still distracted while Trump gets on with people's business .\n",
      "RT @MENTION : TY @MENTION for bailing on GMB & @MENTION today . Piers Morgan drank the Trump Kool Aid & is a vocal opponent o ‚Ä¶\n",
      "RT @MENTION : ‚úçüèª #Trump to sign EO temporary ban suspending visas for Syria & six other ME , African countries #BuildTheWall üëçüèº ‚Ä¶\n",
      "RT @MENTION : Did we have a moral obligation to stop Hitler ? If so we have a moral obligation to stop Trump .\n",
      "Are these people just now getting radicalized by Trump or did they always hate our freedom ? @URL\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m.\n",
      "theresa may has not apologized to trump for insulting him . if she fails to do that today , trump should just send her back to b ‚Ä¶\n",
      "trump's immigration ban excludes countries with business ties via #democracyfor ‚Ä¶\n",
      "trump's immigration order expands the definition of \" criminal \"\n",
      "alert : senator john mccain threatens action on president trump if he does this\n",
      "kiva still distracted while trump gets on with people's business .\n",
      "ty for bailing on gmb & today . piers morgan drank the trump kool aid & is a vocal opponent o ‚Ä¶\n",
      "#trump to sign eo temporary ban suspending visas for syria & six other me , african countries #buildthewall  ‚Ä¶\n",
      "did we have a moral obligation to stop hitler ? if so we have a moral obligation to stop trump .\n",
      "are these people just now getting radicalized by trump or did they always hate our freedom ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_len (99% data): 37.0\n",
      "Encoded data: \n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 5057, 8320, 2114, 8398, 1005, 1055, 5152...   \n",
      "1  [101, 14781, 2089, 2038, 2025, 17806, 2000, 83...   \n",
      "2  [101, 8398, 1005, 1055, 7521, 7221, 23329, 201...   \n",
      "3  [101, 8398, 1005, 1055, 7521, 2344, 24545, 199...   \n",
      "4  [101, 9499, 1024, 5205, 2198, 19186, 17016, 28...   \n",
      "\n",
      "                                      token_type_ids  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "---------- 0 1000\n",
      "torch.Size([1000, 768])\n",
      "---------- 1000 2000\n",
      "(2000, 768)\n",
      "---------- 2000 3000\n",
      "(3000, 768)\n",
      "---------- 3000 4000\n",
      "(4000, 768)\n",
      "---------- 4000 5000\n",
      "(5000, 768)\n",
      "---------- 5000 6000\n",
      "(6000, 768)\n",
      "---------- 6000 7000\n",
      "(7000, 768)\n",
      "---------- 7000 8000\n",
      "(8000, 768)\n",
      "---------- 8000 9000\n",
      "(9000, 768)\n",
      "---------- 9000 10000\n",
      "(10000, 768)\n",
      "---------- 10000 11000\n",
      "(11000, 768)\n",
      "---------- 11000 12000\n",
      "(12000, 768)\n",
      "---------- 12000 13000\n",
      "(13000, 768)\n",
      "---------- 13000 14000\n",
      "(14000, 768)\n",
      "---------- 14000 15000\n",
      "(15000, 768)\n",
      "---------- 15000 16000\n",
      "(16000, 768)\n",
      "---------- 16000 17000\n",
      "(17000, 768)\n",
      "---------- 17000 18000\n",
      "(18000, 768)\n",
      "---------- 18000 19000\n",
      "(19000, 768)\n",
      "---------- 19000 20000\n",
      "(20000, 768)\n",
      "---------- 20000 21000\n",
      "(21000, 768)\n",
      "---------- 21000 22000\n",
      "(22000, 768)\n",
      "---------- 22000 23000\n",
      "(23000, 768)\n",
      "---------- 23000 24000\n",
      "(24000, 768)\n",
      "---------- 24000 25000\n",
      "(25000, 768)\n",
      "---------- 25000 26000\n",
      "(26000, 768)\n",
      "---------- 26000 27000\n",
      "(27000, 768)\n",
      "---------- 27000 28000\n",
      "(28000, 768)\n",
      "---------- 28000 29000\n",
      "(29000, 768)\n",
      "---------- 29000 30000\n",
      "(30000, 768)\n",
      "---------- 30000 31000\n",
      "(31000, 768)\n",
      "---------- 31000 32000\n",
      "(32000, 768)\n",
      "---------- 32000 33000\n",
      "(33000, 768)\n",
      "---------- 33000 34000\n",
      "(34000, 768)\n",
      "---------- 34000 35000\n",
      "(35000, 768)\n",
      "---------- 35000 36000\n",
      "(36000, 768)\n",
      "---------- 36000 37000\n",
      "(37000, 768)\n",
      "---------- 37000 38000\n",
      "(38000, 768)\n",
      "---------- 38000 39000\n",
      "(39000, 768)\n",
      "---------- 39000 40000\n",
      "(40000, 768)\n",
      "---------- 40000 41000\n",
      "(41000, 768)\n",
      "---------- 41000 42000\n",
      "(42000, 768)\n",
      "---------- 42000 43000\n",
      "(43000, 768)\n",
      "---------- 43000 44000\n",
      "(44000, 768)\n",
      "---------- 44000 45000\n",
      "(45000, 768)\n",
      "---------- 45000 46000\n",
      "(46000, 768)\n",
      "---------- 46000 47000\n",
      "(47000, 768)\n",
      "---------- 47000 48000\n",
      "(48000, 768)\n",
      "---------- 48000 49000\n",
      "(49000, 768)\n",
      "---------- 49000 50000\n",
      "(50000, 768)\n",
      "---------- 50000 51000\n",
      "(51000, 768)\n",
      "---------- 51000 52000\n",
      "(52000, 768)\n",
      "---------- 52000 53000\n",
      "(53000, 768)\n",
      "---------- 53000 54000\n",
      "(54000, 768)\n",
      "---------- 54000 55000\n",
      "(55000, 768)\n",
      "---------- 55000 56000\n",
      "(56000, 768)\n",
      "---------- 56000 57000\n",
      "(57000, 768)\n",
      "---------- 57000 58000\n",
      "(58000, 768)\n",
      "---------- 58000 59000\n",
      "(59000, 768)\n",
      "---------- 59000 60000\n",
      "(60000, 768)\n",
      "---------- 60000 61000\n",
      "(61000, 768)\n",
      "---------- 61000 62000\n",
      "(62000, 768)\n",
      "---------- 62000 63000\n",
      "(63000, 768)\n",
      "---------- 63000 64000\n",
      "(64000, 768)\n",
      "---------- 64000 65000\n",
      "(65000, 768)\n",
      "---------- 65000 66000\n",
      "(66000, 768)\n",
      "---------- 66000 67000\n",
      "(67000, 768)\n",
      "---------- 67000 68000\n",
      "(68000, 768)\n",
      "---------- 68000 69000\n",
      "(69000, 768)\n",
      "---------- 69000 70000\n",
      "(70000, 768)\n",
      "---------- 70000 71000\n",
      "(71000, 768)\n",
      "---------- 71000 72000\n",
      "(72000, 768)\n",
      "---------- 72000 73000\n",
      "(73000, 768)\n",
      "---------- 73000 74000\n",
      "(74000, 768)\n",
      "---------- 74000 75000\n",
      "(75000, 768)\n",
      "---------- 75000 76000\n",
      "(76000, 768)\n",
      "---------- 76000 77000\n",
      "(77000, 768)\n",
      "---------- 77000 78000\n",
      "(78000, 768)\n",
      "---------- 78000 79000\n",
      "(79000, 768)\n",
      "---------- 79000 80000\n",
      "(80000, 768)\n",
      "---------- 80000 81000\n",
      "(81000, 768)\n",
      "---------- 81000 82000\n",
      "(82000, 768)\n",
      "---------- 82000 83000\n",
      "(83000, 768)\n",
      "---------- 83000 84000\n",
      "(84000, 768)\n",
      "---------- 84000 85000\n",
      "(85000, 768)\n",
      "---------- 85000 86000\n",
      "(86000, 768)\n",
      "---------- 86000 87000\n",
      "(87000, 768)\n",
      "---------- 87000 88000\n",
      "(88000, 768)\n",
      "---------- 88000 89000\n",
      "(89000, 768)\n",
      "---------- 89000 90000\n",
      "(90000, 768)\n",
      "---------- 90000 91000\n",
      "(91000, 768)\n",
      "---------- 91000 92000\n",
      "(92000, 768)\n",
      "---------- 92000 93000\n",
      "(93000, 768)\n",
      "---------- 93000 94000\n",
      "(94000, 768)\n",
      "---------- 94000 95000\n",
      "(95000, 768)\n",
      "---------- 95000 96000\n",
      "(96000, 768)\n",
      "---------- 96000 97000\n",
      "(97000, 768)\n",
      "---------- 97000 98000\n",
      "(98000, 768)\n",
      "---------- 98000 99000\n",
      "(99000, 768)\n",
      "---------- 99000 100000\n",
      "(100000, 768)\n",
      "---------- 100000 101000\n",
      "(101000, 768)\n",
      "---------- 101000 102000\n",
      "(102000, 768)\n",
      "---------- 102000 103000\n",
      "(103000, 768)\n",
      "---------- 103000 104000\n",
      "(104000, 768)\n",
      "---------- 104000 105000\n",
      "(105000, 768)\n",
      "---------- 105000 106000\n",
      "(106000, 768)\n",
      "---------- 106000 107000\n",
      "(107000, 768)\n",
      "---------- 107000 108000\n",
      "(108000, 768)\n",
      "---------- 108000 109000\n",
      "(109000, 768)\n",
      "---------- 109000 110000\n",
      "(110000, 768)\n",
      "---------- 110000 111000\n",
      "(111000, 768)\n",
      "---------- 111000 112000\n",
      "(112000, 768)\n",
      "---------- 112000 113000\n",
      "(113000, 768)\n",
      "---------- 113000 114000\n",
      "(114000, 768)\n",
      "---------- 114000 115000\n",
      "(115000, 768)\n",
      "---------- 115000 116000\n",
      "(116000, 768)\n",
      "---------- 116000 117000\n",
      "(117000, 768)\n",
      "---------- 117000 118000\n",
      "(118000, 768)\n",
      "---------- 118000 119000\n",
      "(119000, 768)\n",
      "---------- 119000 120000\n",
      "(120000, 768)\n",
      "---------- 120000 121000\n",
      "(121000, 768)\n",
      "---------- 121000 122000\n",
      "(122000, 768)\n",
      "---------- 122000 123000\n",
      "(123000, 768)\n",
      "---------- 123000 123385\n",
      "(123385, 768)\n"
     ]
    }
   ],
   "source": [
    "first_token_embeddings = model.get_bert_first_token_embeddings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/nguyen/data/travel_ban_first_token_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(first_token_embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract bert all-token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import emoji\n",
    "import embedding_extraction as model\n",
    "from scipy.sparse import csr_matrix, save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/home/nguyen/data/processed_travel_ban.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m.\n",
      "theresa may has not apologized to trump for insulting him . if she fails to do that today , trump should just send her back to b ‚Ä¶\n",
      "trump's immigration ban excludes countries with business ties via #democracyfor ‚Ä¶\n",
      "trump's immigration order expands the definition of \" criminal \"\n",
      "alert : senator john mccain threatens action on president trump if he does this\n",
      "kiva still distracted while trump gets on with people's business .\n",
      "ty for bailing on gmb & today . piers morgan drank the trump kool aid & is a vocal opponent o ‚Ä¶\n",
      "#trump to sign eo temporary ban suspending visas for syria & six other me , african countries #buildthewall  ‚Ä¶\n",
      "did we have a moral obligation to stop hitler ? if so we have a moral obligation to stop trump .\n",
      "are these people just now getting radicalized by trump or did they always hate our freedom ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_len (99% data): 37.0\n",
      "Encoded data: \n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 5057, 8320, 2114, 8398, 1005, 1055, 5152...   \n",
      "1  [101, 14781, 2089, 2038, 2025, 17806, 2000, 83...   \n",
      "2  [101, 8398, 1005, 1055, 7521, 7221, 23329, 201...   \n",
      "3  [101, 8398, 1005, 1055, 7521, 2344, 24545, 199...   \n",
      "4  [101, 9499, 1024, 5205, 2198, 19186, 17016, 28...   \n",
      "\n",
      "                                      token_type_ids  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
      "Matrix: (4000, 26880), #Non-zeros: 67697664\n",
      "File:  /home/nguyen/data/all_tokens/travel_ban_1.0.npz\n",
      "Matrix: (4000, 26880), #Non-zeros: 70619136\n",
      "File:  /home/nguyen/data/all_tokens/travel_ban_2.0.npz\n"
     ]
    }
   ],
   "source": [
    "file = \"/home/nguyen/data/all_tokens/travel_ban_\"\n",
    "model.get_bert_all_token_embeddings(data, file = file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Extract sentence transformer embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import emoji\n",
    "import embedding_extraction as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/home/nguyen/data/processed_travel_ban.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rt, @USER, @URL, emoji\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: x.replace('@MENTION', \"\").replace(\"@URL\", \"\").\n",
    "                                    replace(\"@EMAIL\", \"\").lower())\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"^ ?(rt ?)+\", \"\", x))                              \n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub('^( ?: ?)', '', x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: re.sub(\"  +\", \" \", x))\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: ''.join(c for c in x if c not in emoji.UNICODE_EMOJI).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emergency rally against trump's muslim travel ban in nyc , 1/25 at 5 p.m.\n",
      "theresa may has not apologized to trump for insulting him . if she fails to do that today , trump should just send her back to b ‚Ä¶\n",
      "trump's immigration ban excludes countries with business ties via #democracyfor ‚Ä¶\n",
      "trump's immigration order expands the definition of \" criminal \"\n",
      "alert : senator john mccain threatens action on president trump if he does this\n",
      "kiva still distracted while trump gets on with people's business .\n",
      "ty for bailing on gmb & today . piers morgan drank the trump kool aid & is a vocal opponent o ‚Ä¶\n",
      "#trump to sign eo temporary ban suspending visas for syria & six other me , african countries #buildthewall  ‚Ä¶\n",
      "did we have a moral obligation to stop hitler ? if so we have a moral obligation to stop trump .\n",
      "are these people just now getting radicalized by trump or did they always hate our freedom ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(str(data.iloc[i]['Tweet']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len:  (1000, 768)\n",
      "Len:  (2000, 768)\n",
      "Len:  (3000, 768)\n",
      "Len:  (4000, 768)\n",
      "Len:  (5000, 768)\n",
      "Len:  (6000, 768)\n",
      "Len:  (7000, 768)\n",
      "Len:  (8000, 768)\n",
      "Len:  (9000, 768)\n",
      "Len:  (10000, 768)\n",
      "Len:  (11000, 768)\n",
      "Len:  (12000, 768)\n",
      "Len:  (13000, 768)\n",
      "Len:  (14000, 768)\n",
      "Len:  (15000, 768)\n",
      "Len:  (16000, 768)\n",
      "Len:  (17000, 768)\n",
      "Len:  (18000, 768)\n",
      "Len:  (19000, 768)\n",
      "Len:  (20000, 768)\n",
      "Len:  (21000, 768)\n",
      "Len:  (22000, 768)\n",
      "Len:  (23000, 768)\n",
      "Len:  (24000, 768)\n",
      "Len:  (25000, 768)\n",
      "Len:  (26000, 768)\n",
      "Len:  (27000, 768)\n",
      "Len:  (28000, 768)\n",
      "Len:  (29000, 768)\n",
      "Len:  (30000, 768)\n",
      "Len:  (31000, 768)\n",
      "Len:  (32000, 768)\n",
      "Len:  (33000, 768)\n",
      "Len:  (34000, 768)\n",
      "Len:  (35000, 768)\n",
      "Len:  (36000, 768)\n",
      "Len:  (37000, 768)\n",
      "Len:  (38000, 768)\n",
      "Len:  (39000, 768)\n",
      "Len:  (40000, 768)\n",
      "Len:  (41000, 768)\n",
      "Len:  (42000, 768)\n",
      "Len:  (43000, 768)\n",
      "Len:  (44000, 768)\n",
      "Len:  (45000, 768)\n",
      "Len:  (46000, 768)\n",
      "Len:  (47000, 768)\n",
      "Len:  (48000, 768)\n",
      "Len:  (49000, 768)\n",
      "Len:  (50000, 768)\n",
      "Len:  (51000, 768)\n",
      "Len:  (52000, 768)\n",
      "Len:  (53000, 768)\n",
      "Len:  (54000, 768)\n",
      "Len:  (55000, 768)\n",
      "Len:  (56000, 768)\n",
      "Len:  (57000, 768)\n",
      "Len:  (58000, 768)\n",
      "Len:  (59000, 768)\n",
      "Len:  (60000, 768)\n",
      "Len:  (61000, 768)\n",
      "Len:  (62000, 768)\n",
      "Len:  (63000, 768)\n",
      "Len:  (64000, 768)\n",
      "Len:  (65000, 768)\n",
      "Len:  (66000, 768)\n",
      "Len:  (67000, 768)\n",
      "Len:  (68000, 768)\n",
      "Len:  (69000, 768)\n",
      "Len:  (70000, 768)\n",
      "Len:  (71000, 768)\n",
      "Len:  (72000, 768)\n",
      "Len:  (73000, 768)\n",
      "Len:  (74000, 768)\n",
      "Len:  (75000, 768)\n",
      "Len:  (76000, 768)\n",
      "Len:  (77000, 768)\n",
      "Len:  (78000, 768)\n",
      "Len:  (79000, 768)\n",
      "Len:  (80000, 768)\n",
      "Len:  (81000, 768)\n",
      "Len:  (82000, 768)\n",
      "Len:  (83000, 768)\n",
      "Len:  (84000, 768)\n",
      "Len:  (85000, 768)\n",
      "Len:  (86000, 768)\n",
      "Len:  (87000, 768)\n",
      "Len:  (88000, 768)\n",
      "Len:  (89000, 768)\n",
      "Len:  (90000, 768)\n",
      "Len:  (91000, 768)\n",
      "Len:  (92000, 768)\n",
      "Len:  (93000, 768)\n",
      "Len:  (94000, 768)\n",
      "Len:  (95000, 768)\n",
      "Len:  (96000, 768)\n",
      "Len:  (97000, 768)\n",
      "Len:  (98000, 768)\n",
      "Len:  (99000, 768)\n",
      "Len:  (100000, 768)\n",
      "Len:  (101000, 768)\n",
      "Len:  (102000, 768)\n",
      "Len:  (103000, 768)\n",
      "Len:  (104000, 768)\n",
      "Len:  (105000, 768)\n",
      "Len:  (106000, 768)\n",
      "Len:  (107000, 768)\n",
      "Len:  (108000, 768)\n",
      "Len:  (109000, 768)\n",
      "Len:  (110000, 768)\n",
      "Len:  (111000, 768)\n",
      "Len:  (112000, 768)\n",
      "Len:  (113000, 768)\n",
      "Len:  (114000, 768)\n",
      "Len:  (115000, 768)\n",
      "Len:  (116000, 768)\n",
      "Len:  (117000, 768)\n",
      "Len:  (118000, 768)\n",
      "Len:  (119000, 768)\n",
      "Len:  (120000, 768)\n",
      "Len:  (121000, 768)\n",
      "Len:  (122000, 768)\n",
      "Len:  (123000, 768)\n",
      "Len:  (123385, 768)\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = model.get_sentence_transformers_embedings(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123385, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nguyen/data/travel_ban_sentence_transformers_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sentence_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
